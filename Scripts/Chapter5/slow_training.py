import os
import sys
import time
import argparse
import numpy as np
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
import threading

# ç¤ºä¾‹ä»£ç 
'''
ä»é›¶å¼€å§‹
nohup python Scripts/Chapter5/slow_training.py \
--num-epochs 1000 \
--load-model-path /home/siyu/Master_Code/nets/Chap5/slow_training/0105_113601/slow_training_model_best.pth \
> logs/0105/0105_1.log 2>&1 &

ä»joint_netå¼€å§‹
nohup python Scripts/Chapter5/slow_training.py \
--num-epochs 5 \
--from-joint-net /home/siyu/Master_Code/nets/Chap4/Joint_Net/1223/2 \
--num-epochs 1000 \
> logs/0103/0103_2.log 2>&1 &


'''

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

import torch
import torch.nn as nn
import torch.optim as optim

# å¯¼å…¥å…¬å…±ç»„ä»¶
from Scripts.Chapter5.Meta_RL_Engine import (
    MetaRLPolicy,
    ResultSaver,
    create_output_dir,
    get_project_root
)
from Scripts.Chapter3.MARL_Engine import device, Net
from Scripts.Chapter4.Joint_Net import MultiTaskRNN, JointNet
from Scripts.Chapter5.Env_Ultra import EnvUltra

# ----------------------------------------------------
# å·¥å…·å‡½æ•°ï¼šä»JointNetåŠ è½½å‚æ•°åˆ°æ…¢å­¦ä¹ ç½‘ç»œ
# ----------------------------------------------------
def load_params_from_joint_net(joint_net_dir, policy):
    """
    ä»JointNetæ¨¡å‹ç›®å½•åŠ è½½å‚æ•°å¹¶è¿ç§»åˆ°MetaRLPolicyç½‘ç»œ
    
    Args:
        joint_net_dir: JointNetæ¨¡å‹ç›®å½•è·¯å¾„
        policy: è¦åŠ è½½å‚æ•°çš„MetaRLPolicyç½‘ç»œ
    """
    print(f"ğŸ“Œ å¼€å§‹ä»JointNetåŠ è½½å‚æ•°: {joint_net_dir}")
    
    # 1. åŠ è½½JointNetçš„ä¸‰ä¸ªæ™ºèƒ½ä½“æ¨¡å‹
    agent_names = ["FC", "BAT", "SC"]
    joint_agents = {}
    
    for name in agent_names:
        # å°è¯•åŠ è½½æ¨¡å‹æ–‡ä»¶
        model_path = os.path.join(joint_net_dir, f"Joint_Model_{name}.pth")
        if not os.path.exists(model_path):
            # å°è¯•ä½¿ç”¨å…¶ä»–æ–‡ä»¶åæ ¼å¼
            model_path = os.path.join(joint_net_dir, f"slow_training_model_best_{name}.pth")
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"JointNetæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # åŠ è½½JointNetæ¨¡å‹
        try:
            # åˆ›å»ºä¸´æ—¶çš„JointNetç»“æ„æ¥åŠ è½½å‚æ•°
            temp_rnn = MultiTaskRNN()
            temp_marl = Net(N_STATES=65, N_ACTIONS=32 if name == "FC" else 40 if name == "BAT" else 2)
            temp_joint_net = JointNet(temp_rnn, temp_marl)
            
            temp_joint_net.load_state_dict(torch.load(model_path, map_location=device))
            joint_agents[name] = temp_joint_net
            print(f"âœ… æˆåŠŸåŠ è½½{name}æ™ºèƒ½ä½“æ¨¡å‹: {model_path}")
        except Exception as e:
            print(f"âŒ åŠ è½½{name}æ™ºèƒ½ä½“æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    # 2. è·å–æ…¢å­¦ä¹ ç½‘ç»œçš„å½“å‰å‚æ•°
    slow_state_dict = policy.state_dict()
    
    # 3. ä»JointNetæ¨¡å‹ä¸­æå–MARLå¤´éƒ¨å‚æ•°å¹¶è¿ç§»åˆ°æ…¢å­¦ä¹ ç½‘ç»œ
    print("\nğŸ”„ å¼€å§‹è¿ç§»MARLå¤´éƒ¨å‚æ•°...")
    
    # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“è¿ç§»è¾“å‡ºå±‚å‚æ•°
    for name in agent_names:
        joint_marl_state_dict = joint_agents[name].marl_part.state_dict()
        
        # æ˜ å°„åˆ°æ…¢å­¦ä¹ ç½‘ç»œçš„å¯¹åº”è¾“å‡ºå±‚
        if name == "FC":
            slow_output_prefix = "fc_fc"
        elif name == "BAT":
            slow_output_prefix = "fc_bat"
        else:  # SC
            slow_output_prefix = "fc_sc"
        
        # è¿ç§»outputå‚æ•°åˆ°å¯¹åº”çš„è¾“å‡ºå±‚
        if "output.weight" in joint_marl_state_dict and f"{slow_output_prefix}.weight" in slow_state_dict:
            # è·å–JointNetçš„outputå±‚å‚æ•°
            joint_output_weight = joint_marl_state_dict["output.weight"]  # shape: (action_dim, 64)
            
            # è¿ç§»åˆ°æ…¢å­¦ä¹ ç½‘ç»œçš„è¾“å‡ºå±‚
            # æ…¢å­¦ä¹ ç½‘ç»œçš„è¾“å‡ºå±‚è¾“å…¥æ˜¯32ç»´ï¼ˆfc_feature3çš„è¾“å‡ºï¼‰
            # æˆ‘ä»¬åªä½¿ç”¨JointNet outputå±‚çš„å‰32ä¸ªè¾“å…¥é€šé“
            slow_state_dict[f"{slow_output_prefix}.weight"][:, :32] = joint_output_weight[:, :32]
            
            # è¿ç§»åç½®é¡¹
            if "output.bias" in joint_marl_state_dict and f"{slow_output_prefix}.bias" in slow_state_dict:
                slow_state_dict[f"{slow_output_prefix}.bias"] = joint_marl_state_dict["output.bias"]
            
            print(f"   âœ… è¿ç§»{name}æ™ºèƒ½ä½“çš„outputå‚æ•°åˆ°{slow_output_prefix}")
    
    # åªè¿ç§»FCæ™ºèƒ½ä½“çš„ä¸­é—´å±‚å‚æ•°åˆ°æ…¢å­¦ä¹ ç½‘ç»œçš„ç‰¹å¾æå–å±‚
    print("\nğŸ”„ å¼€å§‹è¿ç§»ä¸­é—´å±‚å‚æ•°...")
    fc_marl_state_dict = joint_agents["FC"].marl_part.state_dict()
    
    # è¿ç§»lay1å‚æ•°åˆ°fc_feature3
    if "lay1.weight" in fc_marl_state_dict and "fc_feature3.weight" in slow_state_dict:
        # è·å–JointNetçš„lay1å±‚å‚æ•°
        joint_lay1_weight = fc_marl_state_dict["lay1.weight"]  # shape: (64, 64)
        
        # æ…¢å­¦ä¹ ç½‘ç»œçš„fc_feature3è¾“å…¥æ˜¯64ç»´ï¼Œè¾“å‡ºæ˜¯32ç»´
        # æˆ‘ä»¬åªä½¿ç”¨JointNet lay1å±‚çš„å‰32ä¸ªè¾“å‡ºé€šé“å’Œå‰32ä¸ªè¾“å…¥é€šé“
        slow_state_dict["fc_feature3.weight"][:, :32] = joint_lay1_weight[:32, :32]
        
        # è¿ç§»åç½®é¡¹
        if "lay1.bias" in fc_marl_state_dict and "fc_feature3.bias" in slow_state_dict:
            slow_state_dict["fc_feature3.bias"][:32] = fc_marl_state_dict["lay1.bias"][:32]
        
        print(f"   âœ… è¿ç§»FCæ™ºèƒ½ä½“çš„lay1å‚æ•°åˆ°fc_feature3")
    
    # è¿ç§»inputå±‚å‚æ•°åˆ°fc_feature2
    if "input.weight" in fc_marl_state_dict and "fc_feature2.weight" in slow_state_dict:
        # è·å–JointNetçš„inputå±‚å‚æ•°
        joint_input_weight = fc_marl_state_dict["input.weight"]  # shape: (64, 65)
        
        # æ…¢å­¦ä¹ ç½‘ç»œçš„fc_feature2è¾“å…¥æ˜¯128ç»´ï¼Œè¾“å‡ºæ˜¯64ç»´
        # æˆ‘ä»¬åªä½¿ç”¨JointNet inputå±‚çš„å‰64ä¸ªè¾“å‡ºé€šé“å’Œå‰64ä¸ªè¾“å…¥é€šé“
        # æ³¨æ„ï¼šJointNetçš„inputå±‚è¾“å…¥æ˜¯65ç»´ï¼ˆ64+1ï¼‰ï¼Œæˆ‘ä»¬è·³è¿‡reg_outéƒ¨åˆ†ï¼Œåªä½¿ç”¨64ç»´ç‰¹å¾
        slow_state_dict["fc_feature2.weight"][:64, :64] = joint_input_weight[:, 1:65]  # è·³è¿‡JointNetçš„reg_outéƒ¨åˆ†
        
        # è¿ç§»åç½®é¡¹
        if "input.bias" in fc_marl_state_dict and "fc_feature2.bias" in slow_state_dict:
            slow_state_dict["fc_feature2.bias"][:64] = fc_marl_state_dict["input.bias"]
        
        print(f"   âœ… è¿ç§»FCæ™ºèƒ½ä½“çš„inputå‚æ•°åˆ°fc_feature2")
    
    # 4. æ›´æ–°æ…¢å­¦ä¹ ç½‘ç»œçš„æ‰€æœ‰å‚æ•°
    policy.load_state_dict(slow_state_dict)
    
    print("\nâœ… æ‰€æœ‰JointNetå‚æ•°è¿ç§»å®Œæˆï¼")
    return policy

# ----------------------------------------------------
# æ…¢è®­ç»ƒç®—æ³•ç±»
# ----------------------------------------------------
class SlowTrainer:
    """
    æ…¢è®­ç»ƒç®—æ³•ç±»ï¼Œä¸“æ³¨äºåœ¨å¤šç§æ¨¡æ€ä¸Šè¿›è¡Œæ‰å®çš„æ…¢è®­ç»ƒ
    ä½¿ç”¨ä¼ ç»ŸDQNè®­ç»ƒé€»è¾‘ï¼šåŒç½‘ç»œç»“æ„ã€ç»éªŒå›æ”¾ã€Bellmanæ–¹ç¨‹
    """
    def __init__(self, policy, lr=5e-4, gamma=0.99, hidden_dim=256, num_workers=9, epsilon=0.1, pool_size=100):
        self.policy = policy
        # åˆ›å»ºç›®æ ‡ç½‘ç»œ
        self.target_policy = MetaRLPolicy(hidden_dim=hidden_dim).to(device)
        self.target_policy.load_state_dict(self.policy.state_dict())
        self.target_policy.eval()  # ç›®æ ‡ç½‘ç»œè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        # ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œå¸¦æœ‰æƒé‡è¡°å‡
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr, weight_decay=1e-5)
        # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå½“å¥–åŠ±è¿ç»­100è½®ä¸æå‡æ—¶ï¼Œå­¦ä¹ ç‡ä¹˜ä»¥0.5
        self.lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', patience=1000, factor=0.8
        )
        # è·Ÿè¸ªå½“å‰å­¦ä¹ ç‡ï¼Œç”¨äºæ—¥å¿—æç¤º
        self.current_lr = lr
        self.gamma = gamma
        # æ¢ç´¢ç‡å‚æ•°
        self.epsilon = epsilon
        
        # DQNè®­ç»ƒå‚æ•°
        self.target_replace_iter = 10  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
        self.learn_step_counter = 0  # å­¦ä¹ æ­¥æ•°è®¡æ•°å™¨
        self.batch_size = 32  # æ‰¹æ¬¡å¤§å°
        self.pool_size = pool_size  # æ± å¤§å°å‚æ•°ï¼Œç”¨äºè®¡ç®—ç»éªŒæ± å®¹é‡
        
        # ç»éªŒå›æ”¾æ±  - å›ºå®šå¤§å°ï¼Œä¸éšç¯å¢ƒåŠ¨æ€è°ƒæ•´
        self.memory = []
        # ç»éªŒæ± å¤§å°å›ºå®šä¸ºï¼š1800s/åœºæ™¯ * 9ä¸ªåœºæ™¯ * pool_sizeå‚æ•°
        self.memory_capacity = 1000 * 9 * self.pool_size
        # ç»éªŒæ± å¡«æ»¡æ ‡å¿—
        self.memory_full_notified = False
        
        # 9ç§åœºæ™¯çš„ä»»åŠ¡é›†åˆ
        self.scenarios = [
            'air', 'surface', 'underwater',  # 3ç§åŸºç¡€åœºæ™¯
            'air_to_surface', 'surface_to_air',  # åˆ‡æ¢åœºæ™¯1-2
            'air_to_underwater', 'underwater_to_air',  # åˆ‡æ¢åœºæ™¯3-4
            'surface_to_underwater', 'underwater_to_surface'  # åˆ‡æ¢åœºæ™¯5-6
        ]
        
        # è®¾ç½®çº¿ç¨‹æ± å’Œçº¿ç¨‹é”
        self.num_workers = num_workers
        self.model_lock = threading.Lock()
    
    def generate_experiences(self, scenario, max_steps=1000):
        """
        åœ¨å•ä¸ªåœºæ™¯ä¸Šç”Ÿæˆå®Œæ•´çš„ç»éªŒæ•°æ®ï¼ˆçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€çŠ¶æ€ï¼‰ï¼Œç”¨äºåç»­è®­ç»ƒ
        """
        # åˆ›å»ºç¯å¢ƒ
        env = EnvUltra(scenario_type=scenario)
        state = env.reset()
        
        # ç»éªŒæ± å¤§å°å·²å›ºå®šï¼Œæ— éœ€åŠ¨æ€è°ƒæ•´
        # æ¯ä¸ªåœºæ™¯æœ€å¤§ä¸º1800sï¼Œå…±9ä¸ªåœºæ™¯ï¼Œä¹˜ä»¥pool_sizeå‚æ•°
        
        total_reward = 0.0
        steps = 0
        
        # æ”¶é›†å®Œæ•´çš„ç»éªŒæ•°æ®
        experiences = []
        
        while steps < max_steps:
            # æ¯æ¬¡è¿­ä»£é‡æ–°åˆå§‹åŒ–éšè—çŠ¶æ€
            hidden = None
            
            # é€‰æ‹©åŠ¨ä½œ
            state_tensor = torch.FloatTensor(state).unsqueeze(0).unsqueeze(1).to(device)
            fc_action_out, bat_action_out, sc_action_out, _ = self.policy(state_tensor, hidden)
            
            # ä½¿ç”¨epsilon-greedyç­–ç•¥é€‰æ‹©åŠ¨ä½œ
            # ç‡ƒæ–™ç”µæ± æ™ºèƒ½ä½“
            if np.random.random() < self.epsilon:
                fc_action = np.random.randint(0, fc_action_out.shape[1])
            else:
                fc_action = torch.argmax(fc_action_out, dim=1).item()
            
            # ç”µæ± æ™ºèƒ½ä½“
            if np.random.random() < self.epsilon:
                bat_action = np.random.randint(0, bat_action_out.shape[1])
            else:
                bat_action = torch.argmax(bat_action_out, dim=1).item()
            
            # è¶…çº§ç”µå®¹æ™ºèƒ½ä½“
            if np.random.random() < self.epsilon:
                sc_action = np.random.randint(0, sc_action_out.shape[1])
            else:
                sc_action = torch.argmax(sc_action_out, dim=1).item()
            
            action_list = [fc_action, bat_action, sc_action]
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action_list)
            
            # è®¡ç®—ç›®æ ‡å€¼ï¼Œæ·»åŠ ç‡ƒæ–™ç”µæ± è·Ÿè¸ªè´Ÿè½½çš„å¥–åŠ±é¡¹
            P_load = info['P_load']
            P_fc = info['P_fc']
            # tracking_reward = -abs(P_load - P_fc) * 0.01  # é¼“åŠ±FCæ¥è¿‘è´Ÿè½½
            
            # ç»„åˆå¥–åŠ±
            # adjusted_reward = reward + tracking_reward
            
            # ä¿å­˜å®Œæ•´çš„ç»éªŒæ•°æ®
            experiences.append({
                'state': state,
                'action': action_list,
                'reward': reward,
                'next_state': next_state,
                'done': done
            })
            
            total_reward += reward
            state = next_state
            steps += 1
            
            if done:
                break
        
        return total_reward, experiences
    
    def update_from_experiences(self, all_experiences):
        """
        ä»æ”¶é›†çš„æ‰€æœ‰ç»éªŒæ•°æ®ä¸­æ›´æ–°æ¨¡å‹ï¼šä½¿ç”¨ä¼ ç»ŸDQNè®­ç»ƒé€»è¾‘
        """
        if not all_experiences:
            return
        
        # 1. å°†ç”Ÿæˆçš„ç»éªŒæ•°æ®å­˜å‚¨åˆ°ç»éªŒå›æ”¾æ± ä¸­
        for experiences in all_experiences:
            for exp in experiences:
                # å­˜å‚¨ç»éªŒåˆ°å›æ”¾æ± 
                self.memory.append(exp)
                # å¦‚æœå›æ”¾æ± è¶…è¿‡å®¹é‡ï¼Œåˆ é™¤æœ€æ—§çš„ç»éªŒ
                if len(self.memory) > self.memory_capacity:
                    self.memory.pop(0)
                
                # æ£€æŸ¥ç»éªŒæ± æ˜¯å¦å¡«æ»¡ï¼Œå¹¶æ‰“å°é€šçŸ¥ï¼ˆåªé€šçŸ¥ä¸€æ¬¡ï¼‰
                if len(self.memory) >= self.memory_capacity and not self.memory_full_notified:
                    print(f"[INFO] ç»éªŒæ± å·²å¡«æ»¡ï¼å½“å‰å®¹é‡: {len(self.memory)}/{self.memory_capacity}")
                    print(f"[INFO] ç»éªŒæ± å¤§å°é…ç½®: 1800s/åœºæ™¯ * 9åœºæ™¯ * pool_size({self.pool_size}) = {1800 * 9 * self.pool_size}")
                    self.memory_full_notified = True
        
        # 2. å½“ç»éªŒæ± è¶³å¤Ÿå¤§æ—¶ï¼Œè¿›è¡Œè®­ç»ƒ
        if len(self.memory) < self.batch_size:
            return
        
        # 3. éšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ
        sample_indices = np.random.choice(len(self.memory), self.batch_size, replace=False)
        batch_experiences = [self.memory[i] for i in sample_indices]
        
        # 4. å‡†å¤‡è®­ç»ƒæ•°æ®
        states = []
        actions = []
        rewards = []
        next_states = []
        dones = []
        
        for exp in batch_experiences:
            states.append(exp['state'])
            actions.append(exp['action'])
            rewards.append(exp['reward'])
            next_states.append(exp['next_state'])
            dones.append(exp['done'])
        
        # è½¬æ¢ä¸ºå¼ é‡
        states_tensor = torch.FloatTensor(np.array(states)).unsqueeze(1).to(device)
        next_states_tensor = torch.FloatTensor(np.array(next_states)).unsqueeze(1).to(device)
        rewards_tensor = torch.FloatTensor(np.array(rewards)).to(device)
        dones_tensor = torch.BoolTensor(np.array(dones)).to(device)
        
        # 5. ä½¿ç”¨å½“å‰ç½‘ç»œè®¡ç®—Qå€¼ï¼ˆq_evalï¼‰
        fc_q_eval, bat_q_eval, sc_q_eval, _ = self.policy(states_tensor, None)
        
        # 6. ä½¿ç”¨ç›®æ ‡ç½‘ç»œè®¡ç®—ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼ï¼ˆq_nextï¼‰
        with torch.no_grad():
            fc_q_next, bat_q_next, sc_q_next, _ = self.target_policy(next_states_tensor, None)
            fc_q_next_max = fc_q_next.max(dim=1)[0]
            bat_q_next_max = bat_q_next.max(dim=1)[0]
            sc_q_next_max = sc_q_next.max(dim=1)[0]
        
        # 7. è®¡ç®—ç›®æ ‡Qå€¼ï¼ˆq_target = r + gamma * q_nextï¼‰
        fc_q_target = rewards_tensor + self.gamma * fc_q_next_max * (~dones_tensor)
        bat_q_target = rewards_tensor + self.gamma * bat_q_next_max * (~dones_tensor)
        sc_q_target = rewards_tensor + self.gamma * sc_q_next_max * (~dones_tensor)
        
        # 8. æå–å®é™…åŠ¨ä½œå¯¹åº”çš„Qå€¼
        actions = np.array(actions)
        fc_actions = actions[:, 0].tolist()
        bat_actions = actions[:, 1].tolist()
        sc_actions = actions[:, 2].tolist()
        
        # è½¬æ¢ä¸ºå¼ é‡
        fc_actions_tensor = torch.LongTensor(fc_actions).unsqueeze(1).to(device)
        bat_actions_tensor = torch.LongTensor(bat_actions).unsqueeze(1).to(device)
        sc_actions_tensor = torch.LongTensor(sc_actions).unsqueeze(1).to(device)
        
        # æå–å¯¹åº”åŠ¨ä½œçš„Qå€¼
        fc_q_eval_selected = fc_q_eval.gather(1, fc_actions_tensor).squeeze(1)
        bat_q_eval_selected = bat_q_eval.gather(1, bat_actions_tensor).squeeze(1)
        sc_q_eval_selected = sc_q_eval.gather(1, sc_actions_tensor).squeeze(1)
        
        # 9. è®¡ç®—æŸå¤±
        loss_func = nn.MSELoss()
        fc_loss = loss_func(fc_q_eval_selected, fc_q_target)
        bat_loss = loss_func(bat_q_eval_selected, bat_q_target)
        sc_loss = loss_func(sc_q_eval_selected, sc_q_target)
        
        # æ€»æŸå¤±ï¼ˆä¸‰ä¸ªæ™ºèƒ½ä½“çš„æŸå¤±ä¹‹å’Œï¼‰
        total_loss = fc_loss + bat_loss + sc_loss
        
        # 10. åå‘ä¼ æ’­æ›´æ–°å½“å‰ç½‘ç»œ
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()
        
        # 11. å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.learn_step_counter += 1
        if self.learn_step_counter % self.target_replace_iter == 0:
            self.target_policy.load_state_dict(self.policy.state_dict())
            print(f"ğŸ“Œ ç›®æ ‡ç½‘ç»œå·²æ›´æ–°ï¼ˆæ­¥æ•°: {self.learn_step_counter}ï¼‰")
    
    def train(self, num_epochs=1000, eval_interval=100, save_interval=100, result_saver=None, output_dir=None):
        """
        æ…¢è®­ç»ƒä¸»å¾ªç¯
        """
        training_rewards = []
        best_avg_reward = -float('inf')
        
        # ä½¿ç”¨tqdmæ·»åŠ epochè¿›åº¦æ¡
        pbar = tqdm(range(num_epochs), desc="æ…¢è®­ç»ƒè¿›åº¦", unit="epoch")
        for epoch in pbar:
            epoch_rewards = []
            all_experiences = []
            
            # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œç”Ÿæˆæ‰€æœ‰åœºæ™¯çš„ç»éªŒæ•°æ®
            with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                # æäº¤æ‰€æœ‰åœºæ™¯ç»éªŒç”Ÿæˆä»»åŠ¡
                future_to_scenario = {executor.submit(self.generate_experiences, scenario, max_steps=1000): scenario for scenario in self.scenarios}
                
                # ä½¿ç”¨tqdmæ˜¾ç¤ºåœºæ™¯è®­ç»ƒè¿›åº¦
                for future in tqdm(future_to_scenario, desc=f"Epoch {epoch}åœºæ™¯ç»éªŒç”Ÿæˆ", unit="scenario", leave=False):
                    try:
                        reward, experiences = future.result()
                        epoch_rewards.append(reward)
                        all_experiences.append(experiences)
                    except Exception as e:
                        print(f"  âŒ åœºæ™¯ç»éªŒç”Ÿæˆå¤±è´¥: {e}")
                        epoch_rewards.append(0.0)
            
            # åœ¨ä¸»çº¿ç¨‹ä¸­ç»Ÿä¸€æ›´æ–°æ¨¡å‹
            self.update_from_experiences(all_experiences)
            
            avg_reward = np.mean(epoch_rewards)
            training_rewards.append(avg_reward)
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            self.lr_scheduler.step(avg_reward)
            
            # æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦å˜åŒ–å¹¶è¾“å‡ºæ—¥å¿—
            new_lr = self.optimizer.param_groups[0]['lr']
            
            # åœ¨tqdmè¿›åº¦æ¡ä¸Šæ˜¾ç¤ºå½“å‰çš„å¥–åŠ±å€¼å’Œå­¦ä¹ ç‡
            pbar.set_postfix({"å½“å‰å¥–åŠ±": f"{avg_reward:.4f}", "å½“å‰å­¦ä¹ ç‡": f"{new_lr:.6f}"})
            if new_lr != self.current_lr:
                print(f"ğŸ“‰ å­¦ä¹ ç‡å·²æ›´æ–°: {self.current_lr:.6f} â†’ {new_lr:.6f}")
                self.current_lr = new_lr
            
            # æ¯eval_intervalæ¬¡è¿­ä»£è¿›è¡Œä¸€æ¬¡è¯„ä¼°
            if epoch % eval_interval == 0:
                print(f"Epoch {epoch}, Average Reward: {avg_reward:.4f}, Best Avg Reward: {best_avg_reward:.4f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if avg_reward > best_avg_reward:
                    best_avg_reward = avg_reward
                    print(f"  âœ… æœ€ä½³æ¨¡å‹æ›´æ–°ï¼Œå¹³å‡å¥–åŠ±: {best_avg_reward:.4f}")
                    if result_saver:
                        result_saver.save_model(self.policy, "slow_training_model_best")
            
            # æ¯save_intervalæ¬¡è¿­ä»£ä¿å­˜ä¸€æ¬¡æ¨¡å‹
            if result_saver and output_dir and epoch % save_interval == 0 and epoch > 0:
                result_saver.save_model(self.policy, f"slow_training_model_epoch_{epoch}")
        
        return training_rewards, best_avg_reward

# ----------------------------------------------------
# æ…¢è®­ç»ƒä¸»å‡½æ•°
# ----------------------------------------------------
def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='æ…¢è®­ç»ƒè„šæœ¬')
    parser.add_argument('--num-epochs', type=int, default=1000, help='è®­ç»ƒè¿­ä»£æ¬¡æ•°')
    parser.add_argument('--lr', type=float, default=5e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--hidden-dim', type=int, default=512, help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--gamma', type=float, default=0.95, help='æŠ˜æ‰£å› å­')
    parser.add_argument('--epsilon', type=float, default=0.1, help='è´ªå¿ƒç‡/æ¢ç´¢ç‡')
    parser.add_argument('--output-dir', type=str, default='', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--eval-interval', type=int, default=50, help='è¯„ä¼°é—´éš”')
    parser.add_argument('--save-interval', type=int, default=100, help='æ¨¡å‹ä¿å­˜é—´éš”')
    parser.add_argument('--num-workers', type=int, default=9, help='è®­ç»ƒçº¿ç¨‹æ•°')
    parser.add_argument('--pool-size', type=int, default=100, help='æ± å¤§å°ï¼ˆç”¨äºè®¡ç®—ç»éªŒæ± å®¹é‡ï¼‰')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­ï¼Œç”¨äºç¡®ä¿è®­ç»ƒå¯å¤ç°')
    parser.add_argument('--load-model-path', type=str, default='', help='è¦åŠ è½½çš„é¢„è®­ç»ƒæ…¢å­¦ä¹ æ¨¡å‹è·¯å¾„ï¼Œç”¨äºç»§ç»­è®­ç»ƒ')
    parser.add_argument('--from-joint-net', type=str, default='', help='è¦åŠ è½½çš„JointNetæ¨¡å‹ç›®å½•ï¼Œç”¨äºä»JointNetç»§ç»­è®­ç»ƒ')
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed_all(args.seed) if torch.cuda.is_available() else None
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not args.output_dir:
        output_dir = create_output_dir("slow_training")
    else:
        output_dir = args.output_dir
    
    # åˆå§‹åŒ–ç»“æœä¿å­˜å™¨
    result_saver = ResultSaver(output_dir)
    
    # åˆå§‹åŒ–ç­–ç•¥ç½‘ç»œå¹¶ç§»åŠ¨åˆ°è®¾å¤‡ä¸Š
    policy = MetaRLPolicy(hidden_dim=args.hidden_dim).to(device)
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœæä¾›ï¼‰
    if args.load_model_path and args.from_joint_net:
        print("âŒ é”™è¯¯ï¼š--load-model-path å’Œ --from-joint-net ä¸èƒ½åŒæ—¶ä½¿ç”¨")
        raise ValueError("--load-model-path å’Œ --from-joint-net ä¸èƒ½åŒæ—¶ä½¿ç”¨")
    elif args.load_model_path:
        # ä»æ…¢å­¦ä¹ æ¨¡å‹åŠ è½½
        if os.path.exists(args.load_model_path):
            try:
                policy.load_state_dict(torch.load(args.load_model_path, map_location=device))
                print(f"âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ…¢å­¦ä¹ æ¨¡å‹: {args.load_model_path}")
            except Exception as e:
                print(f"âŒ åŠ è½½é¢„è®­ç»ƒæ…¢å­¦ä¹ æ¨¡å‹å¤±è´¥: {e}")
                raise
        else:
            print(f"âŒ é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.load_model_path}")
            raise FileNotFoundError(f"é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.load_model_path}")
    elif args.from_joint_net:
        # ä»JointNetæ¨¡å‹åŠ è½½å‚æ•°
        if os.path.exists(args.from_joint_net):
            try:
                policy = load_params_from_joint_net(args.from_joint_net, policy)
                print(f"âœ… æˆåŠŸä»JointNetåŠ è½½å‚æ•°: {args.from_joint_net}")
            except Exception as e:
                print(f"âŒ ä»JointNetåŠ è½½å‚æ•°å¤±è´¥: {e}")
                raise
        else:
            print(f"âŒ JointNetæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {args.from_joint_net}")
            raise FileNotFoundError(f"JointNetæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {args.from_joint_net}")
    
    # åˆå§‹åŒ–æ…¢è®­ç»ƒå™¨
    trainer = SlowTrainer(policy, lr=args.lr, gamma=args.gamma, hidden_dim=args.hidden_dim, num_workers=args.num_workers, epsilon=args.epsilon, pool_size=args.pool_size)
    
    print("=== å¼€å§‹æ…¢è®­ç»ƒ ===")
    print(f"è®­ç»ƒåœºæ™¯: {trainer.scenarios}")
    print(f"å­¦ä¹ ç‡: {args.lr}, æŠ˜æ‰£å› å­: {args.gamma}, è´ªå¿ƒç‡: {args.epsilon}, éšè—å±‚ç»´åº¦: {args.hidden_dim}, è®­ç»ƒè½®æ¬¡: {args.num_epochs}")
    print(f"è®­ç»ƒçº¿ç¨‹æ•°: {args.num_workers}, ç»éªŒæ± å¤§å°å‚æ•°: {args.pool_size}")
    
    # æ‰§è¡Œæ…¢è®­ç»ƒ
    start_time = time.time()
    training_rewards, best_avg_reward = trainer.train(
        num_epochs=args.num_epochs,
        eval_interval=args.eval_interval,
        save_interval=args.save_interval,
        result_saver=result_saver,
        output_dir=output_dir
    )
    end_time = time.time()
    
    print(f"\n=== æ…¢è®­ç»ƒå®Œæˆ ===")
    print(f"æœ€ä½³å¹³å‡å¥–åŠ±: {best_avg_reward:.4f}")
    print(f"è®­ç»ƒè€—æ—¶: {end_time - start_time:.2f} ç§’")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    result_saver.save_model(policy, "slow_training_model_final")
    
    # ä¿å­˜è®­ç»ƒå¥–åŠ±æ›²çº¿
    rewards_path = os.path.join(output_dir, "training_rewards.npy")
    np.save(rewards_path, training_rewards)
    print(f"âœ… è®­ç»ƒå¥–åŠ±æ›²çº¿å·²ä¿å­˜åˆ°: {rewards_path}")
    
    # å¯è§†åŒ–è®­ç»ƒå¥–åŠ±æ›²çº¿
    try:
        from Scripts.Chapter5.Meta_RL_Engine import setup_matplotlib
        matplotlib, plt = setup_matplotlib()
        
        plt.figure(figsize=(12, 6))
        plt.plot(training_rewards, label='Average Reward per Epoch')
        plt.xlabel('Epoch')
        plt.ylabel('Average Reward')
        plt.title('Slow Training Reward Curve')
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.7)
        
        # ä¿å­˜è®­ç»ƒæ›²çº¿
        curve_path_svg = os.path.join(output_dir, "training_reward_curve.svg")
        curve_path_png = os.path.join(output_dir, "training_reward_curve.png")
        plt.savefig(curve_path_svg, bbox_inches='tight', dpi=1200)
        plt.savefig(curve_path_png, dpi=1200, bbox_inches='tight')
        print(f"âœ… è®­ç»ƒå¥–åŠ±æ›²çº¿å·²ä¿å­˜åˆ°:")
        print(f"   SVG: {curve_path_svg}")
        print(f"   PNG: {curve_path_png}")
        plt.close()
    except Exception as e:
        print(f"âš ï¸  æ— æ³•ç”Ÿæˆè®­ç»ƒå¥–åŠ±æ›²çº¿: {e}")
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config = {
        "num_epochs": args.num_epochs,
        "lr": args.lr,
        "hidden_dim": args.hidden_dim,
        "gamma": args.gamma,
        "epsilon": args.epsilon,
        "eval_interval": args.eval_interval,
        "save_interval": args.save_interval,
        "best_avg_reward": best_avg_reward,
        "training_time": end_time - start_time,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    config_path = result_saver.save_results_json(config, "slow_training_config.json")
    print(f"âœ… è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    # ç”Ÿæˆå¿«å­¦ä¹ è¶…å‚æ•°
    fast_learning_hyperparams = {
        # åŸºç¡€å­¦ä¹ è¶…å‚æ•°
        "lr": args.lr * 0.1,  # å¿«å­¦ä¹ ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
        "gamma": args.gamma,
        "hidden_dim": args.hidden_dim,
        "batch_size": 32,
        "update_steps": 10,  # æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°
        
        # KLæ•£åº¦ç›¸å…³å‚æ•°
        "kl_threshold": 0.3,  # æ›´æ–°è§¦å‘KLæ•£åº¦é˜ˆå€¼
        "window_size": 100,  # æ»‘åŠ¨çª—å£å¤§å°
        "kl_weight_temp": 0.5,  # æ¸©åº¦KLæ•£åº¦æƒé‡
        "kl_weight_power": 0.5,  # åŠŸç‡éœ€æ±‚KLæ•£åº¦æƒé‡
        
        # æ€§èƒ½æŒ‡æ ‡é˜ˆå€¼
        "power_matching_threshold": 0.9,  # åŠŸç‡ä¾›éœ€åŒ¹é…åº¦é˜ˆå€¼
        "hydrogen_growth_threshold": 0.1,  # ç­‰æ•ˆæ°¢è€—å¢é•¿ç‡é˜ˆå€¼
        "soc_fluctuation_threshold": 0.08,  # é”‚ç”µæ± SOCæ³¢åŠ¨å¹…åº¦é˜ˆå€¼
        "performance_check_steps": 50,  # æ€§èƒ½æ£€æŸ¥æ­¥æ•°
        
        # æ›´æ–°æµç¨‹å‚æ•°
        "backup_params": True,  # æ˜¯å¦å¤‡ä»½å‚æ•°
        "optimize_all_params": True,  # æ˜¯å¦ä¼˜åŒ–æ‰€æœ‰å‚æ•°
        "validation_steps": 100,  # éªŒè¯æ­¥æ•°
        "success_reward_iterations": 10,  # è¿ç»­æˆåŠŸè¿­ä»£æ¬¡æ•°
        
        # æ ¸å¯†åº¦ä¼°è®¡å‚æ•°
        "kernel_bandwidth_temp": 2.0,  # æ¸©åº¦å¸¦å®½
        "kernel_bandwidth_power": 50.0,  # åŠŸç‡éœ€æ±‚å¸¦å®½
        "density_estimation_method": "gaussian",  # æ ¸å¯†åº¦ä¼°è®¡æ–¹æ³•
        
        # å…ƒå­¦ä¹ ç›¸å…³å‚æ•°
        "meta_lr": args.lr * 0.01,  # å…ƒå­¦ä¹ ç‡
        "meta_steps": 5,  # å…ƒå­¦ä¹ æ­¥æ•°
        "adaptation_steps": 200,  # é€‚é…æ­¥æ•°
        "performance_recovery_rate": 0.98  # æ€§èƒ½æ¢å¤ç‡
    }
    
    # ä¿å­˜å¿«å­¦ä¹ è¶…å‚æ•°
    fast_hyperparams_path = result_saver.save_results_json(fast_learning_hyperparams, "fast_learning_hyperparams.json")
    print(f"âœ… å¿«å­¦ä¹ è¶…å‚æ•°å·²ä¿å­˜åˆ°: {fast_hyperparams_path}")
    
    print(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    # è‡ªåŠ¨è¿è¡Œæµ‹è¯•è„šæœ¬
    print("\n=== å¼€å§‹è‡ªåŠ¨æµ‹è¯•æ…¢å­¦ä¹ ç»“æœ ===")
    import subprocess
    import sys
    
    # è·å–æœ€ä½³æ¨¡å‹è·¯å¾„
    best_model_path = os.path.join(output_dir, "slow_training_model_best.pth")
    
    if os.path.exists(best_model_path):
        # æ„å»ºæµ‹è¯•å‘½ä»¤
        test_cmd = [
            sys.executable,
            "Scripts/Chapter5/test_slow_training.py",
            "--model-path", best_model_path,
            "--hidden-dim", str(args.hidden_dim)
        ]
        
        print(f"æ‰§è¡Œæµ‹è¯•å‘½ä»¤: {' '.join(test_cmd)}")
        
        # æ‰§è¡Œæµ‹è¯•è„šæœ¬
        result = subprocess.run(test_cmd, cwd=get_project_root(), capture_output=True, text=True)
        
        # è¾“å‡ºæµ‹è¯•ç»“æœ
        print("\n=== æµ‹è¯•è„šæœ¬è¾“å‡º ===")
        print(result.stdout)
        
        if result.stderr:
            print("\n=== æµ‹è¯•è„šæœ¬é”™è¯¯ ===")
            print(result.stderr)
        
        print(f"\n=== è‡ªåŠ¨æµ‹è¯•å®Œæˆ ===")
    else:
        print(f"âŒ æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶: {best_model_path}")
        print("è¯·æ‰‹åŠ¨è¿è¡Œæµ‹è¯•è„šæœ¬:")
        print(f"python Scripts/Chapter5/test_slow_training.py --model-path {best_model_path} --hidden-dim {args.hidden_dim}")

if __name__ == "__main__":
    main()
