import os
import sys
import time
import argparse
import numpy as np

# ç³»ç»Ÿå·²ä¿®å¤libstdc++.so.6é—®é¢˜ï¼Œä¸å†éœ€è¦ç¯å¢ƒå˜é‡è®¾ç½®

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

import torch
import torch.nn as nn
import torch.optim as optim

# å¯¼å…¥å…¬å…±ç»„ä»¶
from Scripts.Chapter5.Meta_RL_Engine import (
    MetaRLEnvironment,
    MetaRLPolicy,
    FastAdapter,
    ResultSaver,
    create_output_dir,
    load_model
)
from Scripts.Chapter5.Env_Ultra import EnvUltra

# ----------------------------------------------------
# å¿«é€‚é…è®­ç»ƒå™¨ç±»
# ----------------------------------------------------
class FastAdaptationTrainer:
    """
    å¿«é€‚é…è®­ç»ƒå™¨ï¼Œç”¨äºåœ¨æ–°ä»»åŠ¡ä¸Šå¿«é€Ÿè°ƒæ•´æ…¢è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹
    """
    def __init__(self, base_policy, adaptation_lr=5e-5, kl_threshold=0.15, adaptation_steps=100):
        """
        åˆå§‹åŒ–å¿«é€‚é…è®­ç»ƒå™¨
        
        Args:
            base_policy: æ…¢è®­ç»ƒå¾—åˆ°çš„åŸºç¡€æ¨¡å‹
            adaptation_lr: å¿«é€‚é…çš„å­¦ä¹ ç‡
            kl_threshold: è§¦å‘å¿«é€‚é…çš„KLæ•£åº¦é˜ˆå€¼
            adaptation_steps: å¿«é€‚é…çš„æ­¥æ•°
        """
        self.base_policy = base_policy
        self.adaptation_lr = adaptation_lr
        self.kl_threshold = kl_threshold
        self.adaptation_steps = adaptation_steps
        
        # åˆå§‹åŒ–å¿«é€Ÿé€‚é…å™¨
        self.adapter = FastAdapter(self.base_policy, kl_threshold=self.kl_threshold)
        
        # å¿«é€‚é…ä½¿ç”¨çš„ä¼˜åŒ–å™¨
        self.adaptation_optimizer = None
        
        # ä½¿ç”¨HuberæŸå¤±ï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
        self.loss_func = nn.SmoothL1Loss()
    
    def adapt_to_new_task(self, task_data, new_scenario):
        """
        å¿«é€Ÿé€‚é…åˆ°æ–°ä»»åŠ¡
        
        Args:
            task_data: æ–°ä»»åŠ¡çš„æ•°æ®
            new_scenario: æ–°åœºæ™¯ç±»å‹
        """
        # åˆ›å»ºæ–°åœºæ™¯çš„ç¯å¢ƒ
        env = EnvUltra(scenario_type=new_scenario)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œå¿«é€‚é…
        current_state = {
            'power': np.random.normal(2000, 500, 100),
            'temperature': np.random.normal(10, 10, 100)
        }
        
        if self.adapter.should_update(current_state, task_data):
            print(f"ğŸ”„ æ£€æµ‹åˆ°ç¯å¢ƒå˜åŒ–ï¼Œå¼€å§‹å¯¹åœºæ™¯ '{new_scenario}' è¿›è¡Œå¿«é€‚é…...")
            
            # è·å–é€‚é…åçš„æ¨¡å‹
            adapted_policy = self.adapter.adapt(task_data, self.adaptation_steps)
            
            # åœ¨æ–°åœºæ™¯ä¸Šè¿›è¡Œå¿«é€Ÿå¾®è°ƒ
            adapted_policy = self._fine_tune_on_scenario(adapted_policy, new_scenario)
            
            print(f"âœ… åœºæ™¯ '{new_scenario}' å¿«é€‚é…å®Œæˆ")
            return adapted_policy
        else:
            print(f"âœ… ç¯å¢ƒç¨³å®šï¼Œåœºæ™¯ '{new_scenario}' æ— éœ€å¿«é€‚é…")
            return self.base_policy
    
    def _fine_tune_on_scenario(self, policy, scenario, max_steps=1000):
        """
        åœ¨ç‰¹å®šåœºæ™¯ä¸Šè¿›è¡Œå¿«é€Ÿå¾®è°ƒ
        """
        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        policy.train()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.adaptation_optimizer = optim.Adam(policy.parameters(), lr=self.adaptation_lr, weight_decay=1e-5)
        
        # åˆ›å»ºç¯å¢ƒ
        env = EnvUltra(scenario_type=scenario)
        
        # è¿›è¡Œå¤šæ¬¡é€‚é…è¿­ä»£
        for adapt_step in range(self.adaptation_steps):
            state = env.reset()
            episode_loss = 0.0
            steps = 0
            
            while steps < max_steps:
                # æ¯æ¬¡è¿­ä»£é‡æ–°åˆå§‹åŒ–éšè—çŠ¶æ€ï¼Œé¿å…è®¡ç®—å›¾é‡ç”¨
                hidden = None
                
                # é€‰æ‹©åŠ¨ä½œ
                state_tensor = torch.FloatTensor(state).unsqueeze(0).unsqueeze(1)
                fc_action_out, bat_action_out, sc_action_out, _ = policy(state_tensor, hidden)
                
                # è´ªå©ªé€‰æ‹©åŠ¨ä½œ
                fc_action = torch.argmax(fc_action_out, dim=1).item()
                bat_action = torch.argmax(bat_action_out, dim=1).item()
                sc_action = torch.argmax(sc_action_out, dim=1).item()
                
                action_list = [fc_action, bat_action, sc_action]
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = env.step(action_list)
                
                # è®¡ç®—ç›®æ ‡å€¼
                target = torch.tensor(reward, dtype=torch.float32)
                
                # è®¡ç®—æŸå¤±
                loss_fc = self.loss_func(fc_action_out, target.expand_as(fc_action_out))
                loss_bat = self.loss_func(bat_action_out, target.expand_as(bat_action_out))
                loss_sc = self.loss_func(sc_action_out, target.expand_as(sc_action_out))
                
                total_loss = loss_fc + loss_bat + loss_sc
                
                # æ›´æ–°ç­–ç•¥
                self.adaptation_optimizer.zero_grad()
                total_loss.backward()
                self.adaptation_optimizer.step()
                
                episode_loss += total_loss.item()
                state = next_state
                steps += 1
                
                if done:
                    break
            
            # æ‰“å°é€‚é…è¿›åº¦
            if (adapt_step + 1) % 10 == 0:
                avg_loss = episode_loss / steps if steps > 0 else 0.0
                print(f"  é€‚é…è¿›åº¦: {adapt_step + 1}/{self.adaptation_steps}, å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        # æ¢å¤ä¸ºè¯„ä¼°æ¨¡å¼
        policy.eval()
        return policy
    
    def adapt_to_all_scenarios(self, scenarios):
        """
        å¯¹æ‰€æœ‰æ–°åœºæ™¯è¿›è¡Œå¿«é€‚é…
        
        Args:
            scenarios: éœ€è¦é€‚é…çš„åœºæ™¯åˆ—è¡¨
        """
        adapted_policies = {}
        
        # åˆ›å»ºå…ƒç¯å¢ƒï¼Œç”¨äºç”Ÿæˆä»»åŠ¡æ•°æ®
        meta_env = MetaRLEnvironment()
        
        for scenario in scenarios:
            # ç”Ÿæˆè¯¥åœºæ™¯çš„ä»»åŠ¡æ•°æ®
            task_data = meta_env.generate_mode_data(scenario, duration=200)
            
            # è¿›è¡Œå¿«é€‚é…
            adapted_policy = self.adapt_to_new_task(task_data, scenario)
            adapted_policies[scenario] = adapted_policy
        
        return adapted_policies
    
    def test_adapted_policy(self, policy, scenario, max_steps=1000):
        """
        æµ‹è¯•é€‚é…åçš„ç­–ç•¥
        
        Args:
            policy: é€‚é…åçš„ç­–ç•¥
            scenario: æµ‹è¯•åœºæ™¯
            max_steps: æœ€å¤§æµ‹è¯•æ­¥æ•°
        
        Returns:
            æµ‹è¯•ç»“æœï¼ŒåŒ…æ‹¬åŠŸç‡åˆ†é…æ•°æ®å’Œæ€§èƒ½æŒ‡æ ‡
        """
        # åˆ›å»ºç¯å¢ƒ
        env = EnvUltra(scenario_type=scenario)
        state = env.reset()
        
        # åˆå§‹åŒ–æ•°æ®å­˜å‚¨
        power_fc = []
        power_bat = []
        power_sc = []
        load_power = []
        soc_bat = []
        soc_sc = []
        rewards = []
        
        total_reward = 0.0
        steps = 0
        
        while steps < max_steps:
            # é€‰æ‹©åŠ¨ä½œ
            state_tensor = torch.FloatTensor(state).unsqueeze(0).unsqueeze(1)
            fc_action_out, bat_action_out, sc_action_out, _ = policy(state_tensor)
            
            # è´ªå©ªé€‰æ‹©åŠ¨ä½œ
            fc_action = torch.argmax(fc_action_out, dim=1).item()
            bat_action = torch.argmax(bat_action_out, dim=1).item()
            sc_action = torch.argmax(sc_action_out, dim=1).item()
            
            action_list = [fc_action, bat_action, sc_action]
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action_list)
            
            # è®°å½•æ•°æ®
            power_fc.append(float(next_state[2]))
            power_bat.append(float(next_state[3]))
            power_sc.append(float(next_state[4]))
            load_power.append(float(state[0]))  # è´Ÿè½½åŠŸç‡æ˜¯å½“å‰çŠ¶æ€çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
            soc_bat.append(float(next_state[5]))
            soc_sc.append(float(next_state[6]))
            rewards.append(reward)
            
            total_reward += reward
            state = next_state
            steps += 1
            
            if done:
                break
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        avg_reward = np.mean(rewards) if rewards else 0.0
        std_reward = np.std(rewards) if rewards else 0.0
        
        # å‡†å¤‡åŠŸç‡åˆ†é…æ•°æ®
        power_data = {
            'power_fc': power_fc,
            'power_bat': power_bat,
            'power_sc': power_sc,
            'load_power': load_power,
            'soc_bat': soc_bat,
            'soc_sc': soc_sc,
            'temperature': [env.temperature] * steps  # å‡è®¾ç¯å¢ƒæ¸©åº¦æ’å®š
        }
        
        # å‡†å¤‡æ€§èƒ½æ•°æ®
        performance = {
            'scenario': scenario,
            'total_steps': steps,
            'total_reward': total_reward,
            'average_reward': avg_reward,
            'std_reward': std_reward,
            'power_fc_avg': np.mean(power_fc) if power_fc else 0.0,
            'power_bat_avg': np.mean(power_bat) if power_bat else 0.0,
            'power_sc_avg': np.mean(power_sc) if power_sc else 0.0,
            'soc_bat_min': np.min(soc_bat) if soc_bat else 0.0,
            'soc_bat_max': np.max(soc_bat) if soc_bat else 0.0,
            'soc_sc_min': np.min(soc_sc) if soc_sc else 0.0,
            'soc_sc_max': np.max(soc_sc) if soc_sc else 0.0
        }
        
        return power_data, performance

# ----------------------------------------------------
# å¿«é€‚é…ä¸»å‡½æ•°
# ----------------------------------------------------
def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å¿«é€‚é…è„šæœ¬')
    parser.add_argument('--base-model-path', type=str, required=True, help='æ…¢è®­ç»ƒæ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--adaptation-lr', type=float, default=5e-5, help='å¿«é€‚é…å­¦ä¹ ç‡')
    parser.add_argument('--kl-threshold', type=float, default=0.15, help='KLæ•£åº¦é˜ˆå€¼')
    parser.add_argument('--adaptation-steps', type=int, default=100, help='å¿«é€‚é…æ­¥æ•°')
    parser.add_argument('--hidden-dim', type=int, default=512, help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--output-dir', type=str, default='', help='è¾“å‡ºç›®å½•')
    
    # å¯é€‰å‚æ•°ï¼šæŒ‡å®šéœ€è¦é€‚é…çš„åœºæ™¯
    parser.add_argument('--scenarios', nargs='+', type=str, default=None, 
                        choices=['air', 'surface', 'underwater', 
                                 'air_to_surface', 'surface_to_air',
                                 'air_to_underwater', 'underwater_to_air',
                                 'surface_to_underwater', 'underwater_to_surface'],
                        help='éœ€è¦é€‚é…çš„åœºæ™¯åˆ—è¡¨ï¼Œé»˜è®¤é€‚é…æ‰€æœ‰9ç§åœºæ™¯')
    
    # æµ‹è¯•å‚æ•°
    parser.add_argument('--test-steps', type=int, default=1000, help='æµ‹è¯•æ­¥æ•°')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not args.output_dir:
        output_dir = create_output_dir("fast_adaptation")
    else:
        output_dir = args.output_dir
    
    # åˆå§‹åŒ–ç»“æœä¿å­˜å™¨
    result_saver = ResultSaver(output_dir)
    
    # åˆå§‹åŒ–åŸºç¡€ç­–ç•¥ç½‘ç»œ
    base_policy = MetaRLPolicy(hidden_dim=args.hidden_dim)
    
    # åŠ è½½æ…¢è®­ç»ƒæ¨¡å‹
    if not load_model(base_policy, args.base_model_path):
        print("âŒ æ— æ³•åŠ è½½æ…¢è®­ç»ƒæ¨¡å‹ï¼Œå¿«é€‚é…å¤±è´¥")
        sys.exit(1)
    
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    base_policy.eval()
    
    # åˆå§‹åŒ–å¿«é€‚é…è®­ç»ƒå™¨
    trainer = FastAdaptationTrainer(
        base_policy=base_policy,
        adaptation_lr=args.adaptation_lr,
        kl_threshold=args.kl_threshold,
        adaptation_steps=args.adaptation_steps
    )
    
    # ç¡®å®šéœ€è¦é€‚é…çš„åœºæ™¯
    if args.scenarios:
        scenarios_to_adapt = args.scenarios
    else:
        # é»˜è®¤é€‚é…æ‰€æœ‰9ç§åœºæ™¯
        scenarios_to_adapt = [
            'air', 'surface', 'underwater',
            'air_to_surface', 'surface_to_air',
            'air_to_underwater', 'underwater_to_air',
            'surface_to_underwater', 'underwater_to_surface'
        ]
    
    print("=== å¼€å§‹å¿«é€‚é… ===")
    print(f"åŸºç¡€æ¨¡å‹: {args.base_model_path}")
    print(f"é€‚é…åœºæ™¯: {scenarios_to_adapt}")
    print(f"é€‚é…å­¦ä¹ ç‡: {args.adaptation_lr}")
    print(f"KLé˜ˆå€¼: {args.kl_threshold}")
    print(f"é€‚é…æ­¥æ•°: {args.adaptation_steps}")
    
    # æ‰§è¡Œå¿«é€‚é…
    start_time = time.time()
    adapted_policies = trainer.adapt_to_all_scenarios(scenarios_to_adapt)
    end_time = time.time()
    
    print(f"\n=== å¿«é€‚é…å®Œæˆ ===")
    print(f"é€‚é…åœºæ™¯æ•°é‡: {len(adapted_policies)}")
    print(f"é€‚é…è€—æ—¶: {end_time - start_time:.2f} ç§’")
    
    # ä¿å­˜é€‚é…åçš„æ¨¡å‹å’Œæµ‹è¯•ç»“æœ
    all_test_results = {
        "adaptation_config": {
            "base_model_path": args.base_model_path,
            "adaptation_lr": args.adaptation_lr,
            "kl_threshold": args.kl_threshold,
            "adaptation_steps": args.adaptation_steps,
            "adapted_scenarios": list(adapted_policies.keys()),
            "adaptation_time": end_time - start_time,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "test_params": {
                "test_steps": args.test_steps,
                "seed": args.seed
            }
        },
        "scenario_results": {}
    }
    
    # æµ‹è¯•æ¯ä¸ªé€‚é…åçš„æ¨¡å‹
    for scenario, policy in adapted_policies.items():
        print(f"\nğŸ” æµ‹è¯•åœºæ™¯: {scenario}")
        
        # ä¿å­˜é€‚é…åçš„æ¨¡å‹
        if policy is not base_policy:  # åªä¿å­˜ç»è¿‡é€‚é…çš„æ¨¡å‹
            model_name = f"fast_adapted_model_{scenario}"
            result_saver.save_model(policy, model_name)
        
        # æµ‹è¯•æ¨¡å‹æ€§èƒ½
        power_data, performance = trainer.test_adapted_policy(policy, scenario, max_steps=args.test_steps)
        
        # ä¿å­˜åŠŸç‡åˆ†é…å›¾
        plot_filename = f"power_distribution_{scenario}.svg"
        result_saver.save_power_distribution_plot(power_data, scenario, filename=plot_filename)
        
        # ä¿å­˜æ€§èƒ½æ•°æ®
        all_test_results["scenario_results"][scenario] = {
            "performance": performance,
            "power_data": power_data
        }
        
        print(f"âœ… åœºæ™¯ '{scenario}' æµ‹è¯•å®Œæˆ")
        print(f"   æ€»å¥–åŠ±: {performance['total_reward']:.4f}")
        print(f"   å¹³å‡å¥–åŠ±: {performance['average_reward']:.4f}")
    
    # ä¿å­˜åŸºç¡€æ¨¡å‹ä½œä¸ºå‚è€ƒ
    result_saver.save_model(base_policy, "base_slow_trained_model")
    
    # ä¿å­˜æ‰€æœ‰æµ‹è¯•ç»“æœ
    result_saver.save_results_json(all_test_results, "fast_adaptation_test_results.json")
    
    print(f"\n=== æ‰€æœ‰é€‚é…å’Œæµ‹è¯•å®Œæˆ ===")
    print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

if __name__ == "__main__":
    main()
