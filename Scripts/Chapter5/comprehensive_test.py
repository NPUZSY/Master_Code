#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆæµ‹è¯•è„šæœ¬ï¼šæµ‹è¯•ä¸åŒç« èŠ‚çš„æ™ºèƒ½ä½“åœ¨è¶…çº§ç¯å¢ƒä¸­çš„è¡¨ç°

åŠŸèƒ½ï¼š
1. æ”¯æŒæµ‹è¯•Chapter3å’ŒChapter4çš„æ™ºèƒ½ä½“
2. å…¼å®¹ç¬¬äº”ç« çš„æ…¢å­¦ä¹ å’Œåç»­çš„å¿«å­¦ä¹ 
3. æ”¯æŒåœ¨è¶…çº§ç¯å¢ƒçš„æ‰€æœ‰åœºæ™¯ä¸­æµ‹è¯•
4. ç”ŸæˆæŸ±çŠ¶å›¾å¯¹æ¯”ä¸åŒç­–ç•¥çš„è¡¨ç°
"""

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from Scripts.Chapter5.Env_Ultra import EnvUltra
from Scripts.Chapter5.baseline_strategies import BaselineStrategies

def test_baseline_strategy(strategy_name, env, episodes=1):
    """
    æµ‹è¯•åŸºå‡†ç­–ç•¥
    
    Args:
        strategy_name: ç­–ç•¥åç§° ('rule_based')
        env: ç¯å¢ƒå®ä¾‹
        episodes: æµ‹è¯•å›åˆæ•°
    
    Returns:
        avg_reward: å¹³å‡å¥–åŠ±
        avg_steps: å¹³å‡æ­¥æ•°
    """
    strategies = BaselineStrategies(env)
    total_reward = 0.0
    total_steps = 0
    
    for episode in range(episodes):
        state = env.reset()
        done = False
        episode_reward = 0.0
        episode_steps = 0
        
        while not done:
            if strategy_name == 'rule_based':
                action_list = strategies.rule_based_strategy(state)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„åŸºå‡†ç­–ç•¥: {strategy_name}")
            
            next_state, reward, done, info = env.step(action_list)
            episode_reward += reward
            episode_steps += 1
            state = next_state
        
        total_reward += episode_reward
        total_steps += episode_steps
    
    avg_steps = total_steps / episodes
    avg_reward = total_reward / episodes / avg_steps
    
    return avg_reward, avg_steps

def test_chapter3_agent(env, agent_path, episodes=1, output_dir=None, strategy_name=None):
    """
    æµ‹è¯•Chapter3çš„å¤šæ™ºèƒ½ä½“
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        agent_path: æ™ºèƒ½ä½“æ¨¡å‹è·¯å¾„
        episodes: æµ‹è¯•å›åˆæ•°
        output_dir: è¾“å‡ºç›®å½•
        strategy_name: ç­–ç•¥åç§°
    
    Returns:
        avg_reward: å¹³å‡å¥–åŠ±
        avg_steps: å¹³å‡æ­¥æ•°
    """
    try:
        # ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œè°ƒç”¨Chapter3çš„æµ‹è¯•è„šæœ¬
        import subprocess
        import sys
        import os
        import json
        
        # æ„é€ ç­–ç•¥-ç¯å¢ƒæ–‡ä»¶å¤¹è·¯å¾„
        if output_dir and strategy_name:
            strategy_env_dir = os.path.join(output_dir, strategy_name, env.scenario_type)
            os.makedirs(strategy_env_dir, exist_ok=True)
        else:
            strategy_env_dir = output_dir
        
        # æ„é€ å‘½ä»¤è¡Œå‚æ•°
        chapter3_test_script = os.path.join(os.path.dirname(__file__), '../Chapter3/test.py')
        cmd = [
            sys.executable, chapter3_test_script,
            '--net-date', '1218',
            '--train-id', '36',
            '--use-ultra-env',
            '--scenario', env.scenario_type
            # ç§»é™¤--show-plot falseï¼Œä½¿ç”¨é»˜è®¤å€¼
        ]
        
        # æ·»åŠ --save-dirå‚æ•°
        if strategy_env_dir:
            cmd.extend(['--save-dir', strategy_env_dir])
        
        # è¿è¡Œæµ‹è¯•è„šæœ¬
        print(f"è¿è¡ŒChapter3æµ‹è¯•è„šæœ¬: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # è§£ææµ‹è¯•ç»“æœ
        if result.returncode == 0:
            print(f"âœ… Chapter3æ™ºèƒ½ä½“æµ‹è¯•å®Œæˆ")
            # ä»ç”Ÿæˆçš„JSONæ–‡ä»¶ä¸­è¯»å–å¥–åŠ±ä¿¡æ¯
            json_file_path = os.path.join(strategy_env_dir, "MARL_Model_Test_Results.json")
            if os.path.exists(json_file_path):
                with open(json_file_path, 'r', encoding='utf-8') as f:
                    test_results = json.load(f)
                total_reward = test_results['core_metrics']['total_reward']
                total_steps = test_results['time_metrics']['total_steps']
                print(f"ğŸ“Š ä»JSONæ–‡ä»¶è¯»å–åˆ°çš„å¥–åŠ±: {total_reward:.2f}")
                print(f"ğŸ“Š ä»JSONæ–‡ä»¶è¯»å–åˆ°çš„æ­¥æ•°: {total_steps}")
                # æ ¹æ®episodesè®¡ç®—å¹³å‡æ­¥æ•°å’Œå•æ­¥å¹³å‡å¥–åŠ±
                avg_steps = total_steps / episodes
                avg_reward = total_reward / episodes / avg_steps
                return avg_reward, avg_steps
            else:
                print(f"è­¦å‘Š: æµ‹è¯•ç»“æœJSONæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿")
        else:
            print(f"âŒ Chapter3æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {result.stderr}")
            print(f"ä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿")
        
        return test_baseline_strategy('rule_based', env, episodes)
    except Exception as e:
        print(f"é”™è¯¯: Chapter3æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        return test_baseline_strategy('rule_based', env, episodes)

def test_chapter4_agent(env, agent_path, episodes=1, output_dir=None, strategy_name=None):
    """
    æµ‹è¯•Chapter4çš„è”åˆç½‘ç»œæ™ºèƒ½ä½“
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        agent_path: æ™ºèƒ½ä½“æ¨¡å‹è·¯å¾„
        episodes: æµ‹è¯•å›åˆæ•°
        output_dir: è¾“å‡ºç›®å½•
        strategy_name: ç­–ç•¥åç§°
    
    Returns:
        avg_reward: å¹³å‡å¥–åŠ±
        avg_steps: å¹³å‡æ­¥æ•°
    """
    try:
        # ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œè°ƒç”¨Chapter4çš„æµ‹è¯•è„šæœ¬
        import subprocess
        import sys
        import os
        import json
        
        # æ„é€ ç­–ç•¥-ç¯å¢ƒæ–‡ä»¶å¤¹è·¯å¾„
        if output_dir and strategy_name:
            strategy_env_dir = os.path.join(output_dir, strategy_name, env.scenario_type)
            os.makedirs(strategy_env_dir, exist_ok=True)
        else:
            strategy_env_dir = output_dir
        
        # æ„é€ å‘½ä»¤è¡Œå‚æ•°
        chapter4_test_script = os.path.join(os.path.dirname(__file__), '../Chapter4/test_Joint.py')
        cmd = [
            sys.executable, chapter4_test_script,
            '--net-date', '1223',
            '--train-id', '2',
            '--use-ultra-env',
            '--scenario', env.scenario_type
            # ç§»é™¤--show-plot falseï¼Œä½¿ç”¨é»˜è®¤å€¼
        ]
        
        # æ·»åŠ --save-dirå‚æ•°
        if strategy_env_dir:
            cmd.extend(['--save-dir', strategy_env_dir])
        
        # è¿è¡Œæµ‹è¯•è„šæœ¬
        print(f"è¿è¡ŒChapter4æµ‹è¯•è„šæœ¬: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # è§£ææµ‹è¯•ç»“æœ
        if result.returncode == 0:
            print(f"âœ… Chapter4æ™ºèƒ½ä½“æµ‹è¯•å®Œæˆ")
            # ä»ç”Ÿæˆçš„JSONæ–‡ä»¶ä¸­è¯»å–å¥–åŠ±ä¿¡æ¯
            json_file_path = os.path.join(strategy_env_dir, "Joint_Model_Test_Results.json")
            if os.path.exists(json_file_path):
                with open(json_file_path, 'r', encoding='utf-8') as f:
                    test_results = json.load(f)
                total_reward = test_results['core_metrics']['total_reward']
                total_steps = test_results['time_metrics']['total_steps']
                print(f"ğŸ“Š ä»JSONæ–‡ä»¶è¯»å–åˆ°çš„å¥–åŠ±: {total_reward:.2f}")
                print(f"ğŸ“Š ä»JSONæ–‡ä»¶è¯»å–åˆ°çš„æ­¥æ•°: {total_steps}")
                # æ ¹æ®episodesè®¡ç®—å¹³å‡æ­¥æ•°å’Œå•æ­¥å¹³å‡å¥–åŠ±
                avg_steps = total_steps / episodes
                avg_reward = total_reward / episodes / avg_steps
                return avg_reward, avg_steps
            else:
                print(f"è­¦å‘Š: æµ‹è¯•ç»“æœJSONæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿")
        else:
            print(f"âŒ Chapter4æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {result.stderr}")
            print(f"ä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿")
        
        return test_baseline_strategy('rule_based', env, episodes)
    except Exception as e:
        print(f"é”™è¯¯: Chapter4æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        return test_baseline_strategy('rule_based', env, episodes)

def test_slow_learning_agent(env, agent_path, episodes=1, output_dir=None, strategy_name=None):
    """
    æµ‹è¯•ç¬¬äº”ç« çš„æ…¢å­¦ä¹ æ™ºèƒ½ä½“
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        agent_path: æ™ºèƒ½ä½“æ¨¡å‹è·¯å¾„
        episodes: æµ‹è¯•å›åˆæ•°
        output_dir: è¾“å‡ºç›®å½•
        strategy_name: ç­–ç•¥åç§°
    
    Returns:
        avg_reward: å¹³å‡å¥–åŠ±
        avg_steps: å¹³å‡æ­¥æ•°
    """
    try:
        # ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œè°ƒç”¨Chapter5çš„æ…¢å­¦ä¹ æµ‹è¯•è„šæœ¬
        import subprocess
        import sys
        import os
        import json
        
        # æ„é€ å‘½ä»¤è¡Œå‚æ•°
        slow_test_script = os.path.join(os.path.dirname(__file__), 'test_slow_training.py')
        
        # æ£€æŸ¥æ…¢å­¦ä¹ æµ‹è¯•è„šæœ¬æ˜¯å¦å­˜åœ¨
        if not os.path.exists(slow_test_script):
            print(f"è­¦å‘Š: æ…¢å­¦ä¹ æµ‹è¯•è„šæœ¬ä¸å­˜åœ¨ï¼Œä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿")
            return test_baseline_strategy('rule_based', env, episodes)
        
        # æ„é€ ç­–ç•¥-ç¯å¢ƒæ–‡ä»¶å¤¹è·¯å¾„
        if output_dir and strategy_name:
            strategy_env_dir = os.path.join(output_dir, strategy_name, env.scenario_type)
            os.makedirs(strategy_env_dir, exist_ok=True)
        else:
            strategy_env_dir = output_dir
        
        # æ„å»ºå‘½ä»¤è¡Œå‚æ•°
        cmd = [
            sys.executable, slow_test_script,
            '--max-steps', '1800',   # ä½¿ç”¨1800æ­¥æµ‹è¯•
            '--episodes', str(episodes)  # æ·»åŠ å›åˆæ•°å‚æ•°
            # ä¸æ·»åŠ --show-plotå‚æ•°ï¼Œé»˜è®¤ä¸æ˜¾ç¤ºå›¾åƒ
        ]
        
        # æ·»åŠ æ¨¡å‹è·¯å¾„ï¼ˆå¿…é¡»å‚æ•°ï¼‰
        if agent_path:
            cmd.extend(['--model-path', agent_path])
        else:
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹è·¯å¾„
            default_model_path = os.path.join(os.path.dirname(__file__), '../../nets/Chap5/slow_training/model.pth')
            cmd.extend(['--model-path', default_model_path])
            print(f"è­¦å‘Š: æœªæä¾›æ…¢å­¦ä¹ æ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {default_model_path}")
        
        # æ·»åŠ ä¿å­˜ç›®å½•å‚æ•°
        if strategy_env_dir:
            cmd.extend(['--save-dir', strategy_env_dir])
        
        # è¿è¡Œæµ‹è¯•è„šæœ¬
        print(f"è¿è¡Œæ…¢å­¦ä¹ æµ‹è¯•è„šæœ¬: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # è§£ææµ‹è¯•ç»“æœ
        if result.returncode == 0:
            print(f"âœ… æ…¢å­¦ä¹ æ™ºèƒ½ä½“æµ‹è¯•å®Œæˆ")
            # ä»ç”Ÿæˆçš„JSONæ–‡ä»¶ä¸­è¯»å–å¥–åŠ±ä¿¡æ¯
            json_file_path = os.path.join(strategy_env_dir, f"test_result_{env.scenario_type}.json")
            if os.path.exists(json_file_path):
                with open(json_file_path, 'r', encoding='utf-8') as f:
                    test_results = json.load(f)
                total_reward = test_results['total_reward']
                total_steps = test_results['total_steps']
                print(f"ğŸ“Š ä»JSONæ–‡ä»¶è¯»å–åˆ°çš„å¥–åŠ±: {total_reward:.2f}")
                print(f"ğŸ“Š ä»JSONæ–‡ä»¶è¯»å–åˆ°çš„æ­¥æ•°: {total_steps}")
                # æ ¹æ®episodesè®¡ç®—å¹³å‡æ­¥æ•°å’Œå•æ­¥å¹³å‡å¥–åŠ±
                avg_steps = total_steps / episodes if total_steps > 0 else 0.0
                avg_reward = total_reward / episodes / avg_steps if avg_steps > 0 else 0.0
                return avg_reward, avg_steps
            else:
                print(f"è­¦å‘Š: æµ‹è¯•ç»“æœJSONæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿")
        else:
            print(f"âŒ æ…¢å­¦ä¹ æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {result.stderr}")
            print(f"ä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿")
        
        return test_baseline_strategy('rule_based', env, episodes)
    except Exception as e:
        print(f"é”™è¯¯: æ…¢å­¦ä¹ æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        return test_baseline_strategy('rule_based', env, episodes)

def test_fast_learning_agent(env, agent_path, episodes=1, output_dir=None, strategy_name=None):
    """
    æµ‹è¯•ç¬¬äº”ç« çš„å¿«å­¦ä¹ æ™ºèƒ½ä½“
    
    Args:
        env: ç¯å¢ƒå®ä¾‹
        agent_path: æ™ºèƒ½ä½“æ¨¡å‹è·¯å¾„
        episodes: æµ‹è¯•å›åˆæ•°
        output_dir: è¾“å‡ºç›®å½•
        strategy_name: ç­–ç•¥åç§°
    
    Returns:
        avg_reward: å¹³å‡å¥–åŠ±
        avg_steps: å¹³å‡æ­¥æ•°
    """
    try:
        # ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œè°ƒç”¨Chapter5çš„å¿«å­¦ä¹ æµ‹è¯•è„šæœ¬
        import subprocess
        import sys
        import os
        import json
        
        # æ„é€ å‘½ä»¤è¡Œå‚æ•°
        fast_test_script = os.path.join(os.path.dirname(__file__), 'fast_adaptation.py')
        
        # æ£€æŸ¥å¿«å­¦ä¹ æµ‹è¯•è„šæœ¬æ˜¯å¦å­˜åœ¨
        if not os.path.exists(fast_test_script):
            print(f"è­¦å‘Š: å¿«å­¦ä¹ æµ‹è¯•è„šæœ¬ä¸å­˜åœ¨ï¼Œä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿")
            return test_baseline_strategy('rule_based', env, episodes)
        
        # æ„é€ ç­–ç•¥-ç¯å¢ƒæ–‡ä»¶å¤¹è·¯å¾„
        if output_dir and strategy_name:
            strategy_env_dir = os.path.join(output_dir, strategy_name, env.scenario_type)
            os.makedirs(strategy_env_dir, exist_ok=True)
        else:
            strategy_env_dir = output_dir
        
        # æ„å»ºå‘½ä»¤è¡Œå‚æ•°
        cmd = [
            sys.executable, fast_test_script,
            '--max-steps', '1800',  # ä½¿ç”¨1800æ­¥æµ‹è¯•
            '--save-results',        # ä¿å­˜æµ‹è¯•ç»“æœ
            '--episodes', str(episodes)  # æ·»åŠ å›åˆæ•°å‚æ•°
        ]
        
        # æ·»åŠ æ¨¡å‹è·¯å¾„ï¼ˆå¿…é¡»å‚æ•°ï¼‰
        if agent_path:
            cmd.extend(['--model-path', agent_path])
        else:
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹è·¯å¾„
            default_model_path = os.path.join(os.path.dirname(__file__), '../../nets/Chap5/fast_adaptation/model.pth')
            cmd.extend(['--model-path', default_model_path])
            print(f"è­¦å‘Š: æœªæä¾›å¿«å­¦ä¹ æ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {default_model_path}")
        
        # æ·»åŠ åœºæ™¯å‚æ•°
        cmd.extend(['--scenario', env.scenario_type])
        
        # è¿è¡Œæµ‹è¯•è„šæœ¬
        print(f"è¿è¡Œå¿«å­¦ä¹ æµ‹è¯•è„šæœ¬: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # è§£ææµ‹è¯•ç»“æœ
        if result.returncode == 0:
            print(f"âœ… å¿«å­¦ä¹ æ™ºèƒ½ä½“æµ‹è¯•å®Œæˆ")
            # ç›´æ¥ä»è¾“å‡ºä¸­æå–å¥–åŠ±å’Œæ­¥æ•°ä¿¡æ¯
            # æŸ¥æ‰¾è¾“å‡ºä¸­çš„å¥–åŠ±ä¿¡æ¯è¡Œ
            output_lines = result.stdout.split('\n')
            total_reward = None
            total_steps = None
            
            for line in output_lines:
                if 'æ€»å¥–åŠ±:' in line:
                    # æå–æ€»å¥–åŠ±
                    total_reward_str = line.split('æ€»å¥–åŠ±:')[1].strip()
                    try:
                        total_reward = float(total_reward_str)
                    except ValueError:
                        pass
                elif 'è§¦å‘æ›´æ–°æ¬¡æ•°:' in line:
                    # æå–æ­¥æ•°ä¿¡æ¯ï¼Œæ­¥æ•°æ˜¯max_steps - 1
                    total_steps = 1800 - 1  # 1800æ­¥æµ‹è¯•ï¼Œå®é™…æ˜¯1799æ­¥
                    break
            
            if total_reward is not None and total_steps is not None:
                print(f"ğŸ“Š ä»è¾“å‡ºä¸­è¯»å–åˆ°çš„å¥–åŠ±: {total_reward:.2f}")
                print(f"ğŸ“Š è®¡ç®—å¾—åˆ°çš„æ­¥æ•°: {total_steps}")
                # æ ¹æ®episodesè®¡ç®—å¹³å‡æ­¥æ•°å’Œå•æ­¥å¹³å‡å¥–åŠ±
                avg_steps = total_steps / episodes
                avg_reward = total_reward / episodes / avg_steps
                return avg_reward, avg_steps
            else:
                print(f"è­¦å‘Š: æ— æ³•ä»å¿«å­¦ä¹ è¾“å‡ºä¸­æå–å¥–åŠ±ä¿¡æ¯ï¼Œä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿")
        else:
            print(f"âŒ å¿«å­¦ä¹ æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {result.stderr}")
            print(f"ä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿")
        
        return test_baseline_strategy('rule_based', env, episodes)
    except Exception as e:
        print(f"é”™è¯¯: å¿«å­¦ä¹ æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™ç­–ç•¥ä»£æ›¿ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        return test_baseline_strategy('rule_based', env, episodes)

def run_comprehensive_test():
    """
    è¿è¡Œç»¼åˆæµ‹è¯•
    """
    # å¯¼å…¥å¤šçº¿ç¨‹åº“
    import concurrent.futures
    
    # å®šä¹‰æµ‹è¯•åœºæ™¯
    scenarios = EnvUltra.SCENARIO_TYPES
    
    # å®šä¹‰æµ‹è¯•ç­–ç•¥
    # ä½¿ç”¨æŒ‡å®šçš„æœ€ä¼˜æ…¢å­¦ä¹ æ¨¡å‹è·¯å¾„
    best_slow_model_path = '/home/siyu/Master_Code/nets/Chap5/slow_training/0101_200526/slow_training_model_best.pth'
    
    strategies = [
        {'name': 'Rule-Based', 'type': 'baseline', 'path': None},
        {'name': 'Chapter3 MARL', 'type': 'chapter3', 'path': '/home/siyu/Master_Code/nets/Chap3/1218/36'},
        {'name': 'Chapter4 Joint Net', 'type': 'chapter4', 'path': '/home/siyu/Master_Code/nets/Chap4/Joint_Net/1223/2'},
        {'name': 'Slow Learning', 'type': 'slow_learning', 'path': best_slow_model_path},
        {'name': 'Fast Learning', 'type': 'fast_learning', 'path': best_slow_model_path}  # å¿«å­¦ä¹ åŸºäºæ…¢å­¦ä¹ æ¨¡å‹
    ]
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%m%d_%H%M%S")
    output_dir = os.path.join('/home/siyu/Master_Code/nets/Chap5', 'comprehensive_test_results', timestamp)
    os.makedirs(output_dir, exist_ok=True)
    
    # å®šä¹‰æµ‹è¯•ä»»åŠ¡å‡½æ•°
    def test_task(scenario, strategy):
        """
        å•ä¸ªæµ‹è¯•ä»»åŠ¡
        
        Args:
            scenario: æµ‹è¯•åœºæ™¯
            strategy: æµ‹è¯•ç­–ç•¥
        
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        print(f"\n--- æµ‹è¯•ç­–ç•¥: {strategy['name']}ï¼Œåœºæ™¯: {scenario} ---")
        
        episodes = 1
        
        print(f"ğŸ“Š ä½¿ç”¨ {episodes} ä¸ªå›åˆæµ‹è¯•è¯¥åœºæ™¯")
        
        # åˆ›å»ºç¯å¢ƒ
        env = EnvUltra(scenario_type=scenario)
        
        # æ ¹æ®ç­–ç•¥ç±»å‹é€‰æ‹©æµ‹è¯•å‡½æ•°
        if strategy['type'] == 'baseline':
            avg_reward, avg_steps = test_baseline_strategy('rule_based', env, episodes)
        elif strategy['type'] == 'chapter3':
            avg_reward, avg_steps = test_chapter3_agent(env, strategy['path'], episodes, output_dir, strategy['name'])
        elif strategy['type'] == 'chapter4':
            avg_reward, avg_steps = test_chapter4_agent(env, strategy['path'], episodes, output_dir, strategy['name'])
        elif strategy['type'] == 'slow_learning':
            avg_reward, avg_steps = test_slow_learning_agent(env, strategy['path'], episodes, output_dir, strategy['name'])
        elif strategy['type'] == 'fast_learning':
            avg_reward, avg_steps = test_fast_learning_agent(env, strategy['path'], episodes, output_dir, strategy['name'])
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç­–ç•¥ç±»å‹: {strategy['type']}")
        
        print(f"ç­–ç•¥: {strategy['name']}ï¼Œåœºæ™¯: {scenario} æµ‹è¯•å®Œæˆ")
        print(f"å¹³å‡å¥–åŠ±: {avg_reward:.4f}")
        print(f"å¹³å‡æ­¥æ•°: {avg_steps:.2f}")
        
        # è¿”å›æµ‹è¯•ç»“æœ
        return {
            'scenario': scenario,
            'strategy': strategy['name'],
            'avg_reward': avg_reward,
            'avg_steps': avg_steps
        }
    
    # å­˜å‚¨æµ‹è¯•ç»“æœ
    results = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œæµ‹è¯•ä»»åŠ¡
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        future_to_task = {}
        for scenario in scenarios:
            for strategy in strategies:
                future = executor.submit(test_task, scenario, strategy)
                future_to_task[future] = (scenario, strategy['name'])
        
        # æ”¶é›†æµ‹è¯•ç»“æœ
        for future in concurrent.futures.as_completed(future_to_task):
            scenario, strategy_name = future_to_task[future]
            try:
                task_result = future.result()
                results.append(task_result)
            except Exception as e:
                print(f"ç­–ç•¥: {strategy_name}ï¼Œåœºæ™¯: {scenario} æµ‹è¯•å¤±è´¥: {e}")
    
    return results, output_dir

def plot_comparison(results, output_dir):
    """
    ç»˜åˆ¶ä¸åŒç­–ç•¥åœ¨ä¸åŒåœºæ™¯ä¸‹çš„å•æ­¥å¹³å‡å¥–åŠ±å¯¹æ¯”å›¾
    
    Args:
        results: æµ‹è¯•ç»“æœåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    """
    # æå–å”¯ä¸€çš„åœºæ™¯å’Œç­–ç•¥
    scenarios = sorted(list(set(r['scenario'] for r in results)))
    strategies = sorted(list(set(r['strategy'] for r in results)))
    
    # å‡†å¤‡æ•°æ®
    data = {}
    for scenario in scenarios:
        data[scenario] = {}
        for strategy in strategies:
            # æŸ¥æ‰¾å¯¹åº”ç»“æœ
            for r in results:
                if r['scenario'] == scenario and r['strategy'] == strategy:
                    data[scenario][strategy] = r['avg_reward']
                    break
    
    # åˆ›å»ºå›¾è¡¨
    num_scenarios = len(scenarios)
    num_strategies = len(strategies)
    
    # ä½¿ç”¨æ›´å®½çš„å›¾è¡¨å’Œæ›´æ¸…æ™°çš„å¸ƒå±€
    fig, ax = plt.subplots(figsize=(20, 10))
    
    # ä½¿ç”¨æ›´æ¸…æ™°çš„é¢œè‰²æ–¹æ¡ˆ
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
    
    # è®¾ç½®æŸ±çŠ¶å›¾å®½åº¦å’Œä½ç½®
    bar_width = 0.12
    x = np.arange(num_scenarios)
    
    # è·å–æ‰€æœ‰å¥–åŠ±å€¼ï¼Œç”¨äºè®¾ç½®Yè½´èŒƒå›´
    all_rewards = [data[scenario][strategy] for scenario in scenarios for strategy in strategies]
    
    # ä¸ºæ¯ä¸ªç­–ç•¥ç»˜åˆ¶æŸ±çŠ¶å›¾ï¼Œä½¿ç”¨å®é™…çš„å¥–åŠ±å€¼
    for i, strategy in enumerate(strategies):
        rewards = [data[scenario][strategy] for scenario in scenarios]
        ax.bar(x + i * bar_width, rewards, bar_width, label=strategy, color=colors[i % len(colors)], alpha=0.8)
    
    # è®¾ç½®å›¾è¡¨å±æ€§
    ax.set_xlabel('Scene', fontsize=14, fontweight='bold')
    ax.set_ylabel('Single-step Average Reward (Symmetric Log Scale)', fontsize=14, fontweight='bold')
    ax.set_title('Single-step Average Reward Comparison Across Strategies and Scenarios (Symmetric Log Scale)', fontsize=16, fontweight='bold', pad=20)
    ax.set_xticks(x + bar_width * (num_strategies - 1) / 2)
    ax.set_xticklabels(scenarios, rotation=45, ha='right', fontsize=12)
    
    # è®¾ç½®Yè½´ä¸ºå¯¹ç§°å¯¹æ•°åˆ»åº¦ï¼Œå¯ä»¥å¤„ç†è´Ÿå€¼
    ax.set_yscale('symlog')
    
    # è®¾ç½®Yè½´èŒƒå›´ï¼Œä½¿ç”¨å®é™…å¥–åŠ±å€¼çš„èŒƒå›´
    y_min = min(all_rewards) * 1.1
    y_max = max(all_rewards) * 1.1
    ax.set_ylim(y_min, y_max)
    
    # æ·»åŠ ç½‘æ ¼çº¿ï¼Œä½¿æ•°æ®æ›´æ˜“äºè§‚å¯Ÿ
    ax.grid(True, linestyle='--', alpha=0.7, axis='y')
    
    # æ·»åŠ æ›´æ¸…æ™°çš„å›¾ä¾‹
    ax.legend(fontsize=12, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)
    
    # è°ƒæ•´å¸ƒå±€ï¼Œå¢åŠ è¾¹è·
    plt.tight_layout(rect=[0, 0.1, 1, 0.95])
    
    # ä¿å­˜å›¾è¡¨ä¸ºSVGå’ŒPNGæ ¼å¼ï¼ŒSVGé€‚åˆè¿›ä¸€æ­¥ç¼–è¾‘
    fig_path_png = os.path.join(output_dir, 'strategy_comparison.png')
    fig_path_svg = os.path.join(output_dir, 'strategy_comparison.svg')
    plt.savefig(fig_path_png, dpi=300, bbox_inches='tight')
    plt.savefig(fig_path_svg, dpi=300, bbox_inches='tight')
    print(f"\n=== å›¾è¡¨å·²ä¿å­˜åˆ°: {fig_path_png} ===")
    print(f"=== å›¾è¡¨å·²ä¿å­˜åˆ°: {fig_path_svg} ===")
    
    # ä¿å­˜ç»“æœæ•°æ®
    results_path = os.path.join(output_dir, 'test_results.json')
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=4, ensure_ascii=False)
    print(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
    
    plt.close()
    
    # é¢å¤–åˆ›å»ºä¸€ä¸ªæŠ˜çº¿å›¾ï¼Œæ˜¾ç¤ºæ¯ä¸ªç­–ç•¥çš„å¹³å‡è¡¨ç°
    # è®¡ç®—æ¯ä¸ªç­–ç•¥åœ¨æ‰€æœ‰åœºæ™¯ä¸‹çš„å¹³å‡å¥–åŠ±
    avg_rewards = {}
    for strategy in strategies:
        avg_rewards[strategy] = np.mean([data[scenario][strategy] for scenario in scenarios])
    
    # åˆ›å»ºæŠ˜çº¿å›¾
    fig, ax = plt.subplots(figsize=(15, 8))
    
    # ç»˜åˆ¶æŠ˜çº¿å›¾
    sorted_strategies = sorted(avg_rewards.items(), key=lambda x: x[1], reverse=True)
    strategy_names = [s[0] for s in sorted_strategies]
    avg_values = [s[1] for s in sorted_strategies]
    
    # ç›´æ¥ä½¿ç”¨å®é™…çš„å¥–åŠ±å€¼ç»˜åˆ¶æŠ˜çº¿å›¾
    ax.plot(strategy_names, avg_values, marker='o', linewidth=2, markersize=8, markerfacecolor='white', markeredgewidth=2)
    
    # ä¸ºæ¯ä¸ªç‚¹æ·»åŠ æ•°å€¼æ ‡ç­¾ï¼Œæ˜¾ç¤ºå®é™…å€¼
    for i, v in enumerate(avg_values):
        # æ ¹æ®å€¼çš„æ­£è´Ÿè°ƒæ•´æ ‡ç­¾ä½ç½®
        offset = 0.5 if v > 0 else -0.5
        va = 'bottom' if v > 0 else 'top'
        ax.text(i, v + offset, f'{v:.2f}', ha='center', va=va, fontweight='bold')
    
    # è®¾ç½®å›¾è¡¨å±æ€§
    ax.set_xlabel('Strategy', fontsize=14, fontweight='bold')
    ax.set_ylabel('Average Reward Across Scenarios (Symmetric Log Scale)', fontsize=14, fontweight='bold')
    ax.set_title('Average Performance of Different Strategies (Symmetric Log Scale)', fontsize=16, fontweight='bold', pad=20)
    ax.tick_params(axis='x', rotation=45)
    
    # è®¾ç½®Yè½´ä¸ºå¯¹ç§°å¯¹æ•°åˆ»åº¦ï¼Œå¯ä»¥å¤„ç†è´Ÿå€¼
    ax.set_yscale('symlog')
    
    # è®¾ç½®Yè½´èŒƒå›´
    y_min = min(avg_values) * 1.1
    y_max = max(avg_values) * 1.1
    ax.set_ylim(y_min, y_max)
    
    ax.grid(True, linestyle='--', alpha=0.7, axis='y')
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout(rect=[0, 0.1, 1, 0.95])
    
    # ä¿å­˜æŠ˜çº¿å›¾
    line_fig_path_png = os.path.join(output_dir, 'strategy_average_comparison.png')
    line_fig_path_svg = os.path.join(output_dir, 'strategy_average_comparison.svg')
    plt.savefig(line_fig_path_png, dpi=300, bbox_inches='tight')
    plt.savefig(line_fig_path_svg, dpi=300, bbox_inches='tight')
    print(f"=== æŠ˜çº¿å›¾å·²ä¿å­˜åˆ°: {line_fig_path_png} ===")
    print(f"=== æŠ˜çº¿å›¾å·²ä¿å­˜åˆ°: {line_fig_path_svg} ===")
    
    plt.close()

def main():
    """
    ä¸»å‡½æ•°
    """
    print("=== å¼€å§‹ç»¼åˆæµ‹è¯• ===")
    
    # è¿è¡Œæµ‹è¯•
    results, output_dir = run_comprehensive_test()
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    plot_comparison(results, output_dir)
    
    print("\n=== ç»¼åˆæµ‹è¯•å®Œæˆ ===")

if __name__ == "__main__":
    main()
