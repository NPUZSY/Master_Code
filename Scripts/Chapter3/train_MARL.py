import os
import sys  # â— å¼•å…¥ sys æ¨¡å—

# --------------------------------------------------------------------
# å¯¼å…¥è·¯å¾„ä¿®æ­£ï¼šå°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
# --------------------------------------------------------------------
# è·å–å½“å‰æ–‡ä»¶ï¼ˆtrain_MARL.pyï¼‰çš„ç›®å½•
script_dir = os.path.dirname(os.path.abspath(__file__))
# å‘ä¸Šä¸¤çº§ï¼Œå¾—åˆ°é¡¹ç›®æ ¹ç›®å½•: E:\Master\æ¯•ä¸š\ç¡•å£«æ¯•ä¸šè®ºæ–‡ä»£ç ä»“åº“ (å‡è®¾æ‚¨çš„é¡¹ç›®ç»“æ„)
project_root = os.path.abspath(os.path.join(script_dir, '..', '..'))

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python æœç´¢è·¯å¾„ä¸­
if project_root not in sys.path:
    sys.path.append(project_root)
# --------------------------------------------------------------------


import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter
import time
import numpy as np
from tqdm import tqdm

# ä¿®æ­£ï¼šç°åœ¨å¯ä»¥æ­£ç¡®åœ°é€šè¿‡ Scripts æ‰¾åˆ° Env æ¨¡å—
from Scripts.Env import Envs

# ====================================================================
# å…¨å±€è®¾ç½®ä¸è¶…å‚æ•°
# ====================================================================
# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

env = Envs()
writer = SummaryWriter()
torch.set_default_dtype(torch.float32)

# Hyper Parameters (ä¿æŒåŸè®¾å®š)
BATCH_SIZE = 64
LR = 0.005
EPSILON = 0.9
GAMMA = 0.9
TARGET_REPLACE_ITER = 100
POOL_SIZE = 10
EPISODE = 1000
# å­¦ä¹ é¢‘ç‡
LEARN_FREQUENCY = 10

REAL_TIME_DRAW = False

MEMORY_CAPACITY = env.step_length * POOL_SIZE
current_timestamp = time.time()
local_time = time.localtime(current_timestamp)
execute_date = time.strftime("%m%d", local_time)
remark = "MARL_IQL_32x20x2"

# Environment Constants
N_STATES = env.observation_space.shape[0]
N_TOTAL_ACTIONS = env.N_ACTIONS

# --------------------------------------------------------------------
# MARL åŠ¨ä½œåˆ†è§£ä¿®æ­£ (Action Decomposition for I-DQN)
# --------------------------------------------------------------------
N_FC_ACTIONS = 32  # FC åŠŸç‡å˜åŒ–ç‡ (32 ä»½)
N_BAT_ACTIONS = 20  # Battery è¾“å‡ºåŠŸç‡ (20 ä»½)
N_SC_ACTIONS = 2  # SuperCap çŠ¶æ€ (åˆ‡å…¥/åˆ‡å‡º)

N_EXPECTED_ACTIONS = N_FC_ACTIONS * N_BAT_ACTIONS * N_SC_ACTIONS

if N_EXPECTED_ACTIONS != N_TOTAL_ACTIONS:
    print(
        f"è­¦å‘Šï¼šåŠ¨ä½œåˆ†è§£ {N_EXPECTED_ACTIONS} ä¸ç¯å¢ƒ N_TOTAL_ACTIONS({N_TOTAL_ACTIONS}) ä¸åŒ¹é…ã€‚ä»£ç å°†ç»§ç»­è¿è¡Œï¼Œä½†è¯·æ£€æŸ¥ Env.pyã€‚")
    pass  # å…è®¸ä¸åŒ¹é…ç»§ç»­è¿è¡Œï¼Œä½†ç”¨æˆ·åº”ç¡®ä¿ç¯å¢ƒåŠ¨ä½œç©ºé—´æ­£ç¡®

Base_Model_Name = ""


class Net(nn.Module):
    """
    é€šç”¨ Q-ç½‘ç»œç»“æ„ã€‚è¾“å…¥å…¨å±€çŠ¶æ€ N_STATESï¼Œè¾“å‡ºå„è‡ªå±€éƒ¨åŠ¨ä½œç©ºé—´ N_ACTIONSã€‚
    """

    def __init__(self, N_ACTIONS):
        torch.manual_seed(0)
        super(Net, self).__init__()
        # ä½¿ç”¨æ›´å¤§çš„ç½‘ç»œå±‚ä»¥é€‚åº”æ›´å¤§çš„åŠ¨ä½œç©ºé—´
        self.input = nn.Linear(N_STATES, 64)
        self.input.weight.data.normal_(0, 0.1)

        self.lay1 = nn.Linear(64, 64)
        self.lay1.weight.data.normal_(0, 0.1)

        self.output = nn.Linear(64, N_ACTIONS)
        self.output.weight.data.normal_(0, 0.1)

    def forward(self, x):
        x = self.input(x)
        x = F.relu(x)
        x = self.lay1(x)
        x = F.relu(x)
        actions_value = self.output(x)
        return actions_value


class IndependentDQN(object):
    """
    Independent DQN (I-DQN) æ™ºèƒ½ä½“ç±»
    """

    def __init__(self, agent_name, N_AGENT_ACTIONS, shared_memory, memory_counter_ref):

        self.agent_name = agent_name
        self.N_AGENT_ACTIONS = N_AGENT_ACTIONS

        # ä½¿ç”¨å±€éƒ¨åŠ¨ä½œç©ºé—´å¤§å°åˆå§‹åŒ–ç½‘ç»œ
        self.eval_net = Net(N_AGENT_ACTIONS).to(device)
        self.target_net = Net(N_AGENT_ACTIONS).to(device)

        self.learn_step_counter = 0
        self.memory = shared_memory  # å¼•ç”¨å…±äº«å†…å­˜
        self.memory_counter_ref = memory_counter_ref  # å¼•ç”¨å†…å­˜è®¡æ•°å™¨

        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)
        self.loss_func = nn.MSELoss()

    def load_net(self, path):
        self.eval_net.load_state_dict(torch.load(path, map_location=device))
        self.eval_net.to(device)
        self.target_net.load_state_dict(self.eval_net.state_dict())

    def choose_action(self, state_input: torch.Tensor, train=True):
        temp = torch.FloatTensor(state_input)
        state_input = torch.unsqueeze(temp.to(device), 0)

        # ç­–ç•¥ï¼šè®­ç»ƒåˆæœŸéšæœºæ¢ç´¢ï¼ŒåæœŸÎµ-greedy
        epsilon = 1.0 if train else EPSILON

        if np.random.uniform() < epsilon:  # greedy
            with torch.no_grad():
                actions_value = self.eval_net.forward(state_input)
                # é€‰æ‹© Q å€¼æœ€å¤§çš„å±€éƒ¨åŠ¨ä½œç´¢å¼•
                action_index = torch.max(actions_value, 1)[1].item()
        else:  # random
            action_index = np.random.randint(0, self.N_AGENT_ACTIONS)

        return action_index

    # learn æ–¹æ³•ç°åœ¨æ¥å— agent_idx æ¥ç´¢å¼•å…±äº«å†…å­˜ä¸­çš„å±€éƒ¨åŠ¨ä½œ
    def learn(self, agent_idx):
        memory_counter = self.memory_counter_ref[0]

        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:
            self.target_net.load_state_dict(self.eval_net.state_dict())
        self.learn_step_counter += 1

        # sample batch transitions from shared memory
        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)
        b_memory = self.memory[sample_index, :]

        b_s = torch.FloatTensor(b_memory[:, :N_STATES]).to(device)

        # å±€éƒ¨åŠ¨ä½œç´¢å¼•ï¼šFC=0, Bat=1, SC=2
        action_column_index = N_STATES + agent_idx
        b_a = torch.LongTensor(b_memory[:, action_column_index:action_column_index + 1].astype(int)).to(device)

        # å¥–åŠ±åœ¨ N_STATES + 3 å¤„ (å› ä¸ºæœ‰ 3 ä¸ªåŠ¨ä½œåˆ—)
        b_r = torch.FloatTensor(b_memory[:, N_STATES + 3:N_STATES + 4]).to(device)
        # s' åœ¨ N_STATES + 4 å¤„å¼€å§‹
        b_s_ = torch.FloatTensor(b_memory[:, N_STATES + 4:]).to(device)

        # I-DQN Q-target è®¡ç®—
        q_eval = self.eval_net(b_s).gather(1, b_a)
        q_next = self.target_net(b_s_).detach()
        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)

        loss = self.loss_func(q_eval, q_target)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()


def get_max_folder_name(directory):
    if not os.path.exists(directory):
        return 0
    # è¿‡æ»¤å‡ºç›®å½•åä¸”ä¸ºæ•°å­—çš„æ–‡ä»¶å¤¹
    folders = [int(name) for name in os.listdir(directory) if
               os.path.isdir(os.path.join(directory, name)) and name.isdigit()]
    if not folders:
        return 0
    return max(folders) + 1


# --------------------------------------------------------------------
# â— æ–°å¢ï¼šç”¨äºæ‰“å°è€—æ—¶åˆ†æçš„è¾…åŠ©å‡½æ•°
# --------------------------------------------------------------------
def print_time_breakdown(episode, episode_times):
    """æ‰“å°æœ¬å›åˆçš„è€—æ—¶åˆ†è§£ç»“æœ"""
    total_time = sum(episode_times.values())

    # é˜²æ­¢é™¤é›¶
    if total_time < 1e-6:
        print(f"å›åˆ {episode} è€—æ—¶è¿‡çŸ­ï¼Œè·³è¿‡è€—æ—¶åˆ†æã€‚")
        return

    print("\n" + "=" * 45)
    print(f"ğŸš€ å›åˆ {episode} è€—æ—¶åˆ†è§£ (æ€»è€—æ—¶: {total_time:.4f} s)")
    print("-" * 45)

    # æ‰“å°æ¯ä¸ªéƒ¨åˆ†çš„è€—æ—¶åŠå…¶å æ€»æ—¶é—´çš„ç™¾åˆ†æ¯”
    sorted_times = sorted(episode_times.items(), key=lambda item: item[1], reverse=True)
    for name, time_val in sorted_times:
        percentage = (time_val / total_time) * 100
        print(f"| {name.ljust(15)} | {time_val:9.4f} s | {percentage:6.2f} % |")

    print("=" * 45)


# --------------------------------------------------------------------


if __name__ == '__main__':
    # --------------------------------------------------------------------
    # è·¯å¾„å’Œæ—¥å¿—è®¾ç½®
    # --------------------------------------------------------------------
    TARGET_BASE_DIR = os.path.join(project_root, "nets", "Chap3", execute_date)
    os.makedirs(TARGET_BASE_DIR, exist_ok=True)

    # è‡ªåŠ¨è·å–ä¸‹ä¸€ä¸ªè®­ç»ƒID
    train_id = get_max_folder_name(TARGET_BASE_DIR)

    # æœ€ç»ˆçš„ä¿å­˜è·¯å¾„
    base_path = f"{TARGET_BASE_DIR}/{train_id}"
    os.makedirs(base_path)

    # åˆå§‹åŒ–å…±äº«å†…å­˜å’Œè®¡æ•°å™¨
    # å†…å­˜å­˜å‚¨ç»“æ„: [s, a_fc, a_bat, a_sc, r, s_] (N_STATES * 2 + 4)
    MEMORY_WIDTH = N_STATES * 2 + 4
    shared_memory = np.zeros((MEMORY_CAPACITY, MEMORY_WIDTH))
    memory_counter = [0]  # ä½¿ç”¨åˆ—è¡¨ä½œä¸ºå¯å˜å¼•ç”¨ï¼Œä»¥ä¾¿åœ¨ç±»ä¸­æ›´æ–°

    # å®ä¾‹åŒ–ä¸‰ä¸ªç‹¬ç«‹çš„ DQN æ™ºèƒ½ä½“
    FC_Agent = IndependentDQN("FC_Agent", N_FC_ACTIONS, shared_memory, memory_counter)
    Bat_Agent = IndependentDQN("Bat_Agent", N_BAT_ACTIONS, shared_memory, memory_counter)
    SC_Agent = IndependentDQN("SC_Agent", N_SC_ACTIONS, shared_memory, memory_counter)

    print('\nCollecting experience and learning (I-DQN, 3-Agent)...')
    start_time_total = time.time()
    reward_max = -1e6
    x, y = [], []

    if REAL_TIME_DRAW:
        plt.ion()
        fig, ax = plt.subplots()
        line, = ax.plot(x, y)

    # ä½¿ç”¨ tqdm åŒ…è£…ä¸»å¾ªç¯ï¼Œå®ç°å®æ—¶è¿›åº¦è¾“å‡º
    episode_pbar = tqdm(range(EPISODE), desc=f"RL Training ({remark})")

    for i_episode in episode_pbar:
        s = env.reset()
        ep_r = 0

        # â— åˆå§‹åŒ–æœ¬å›åˆçš„è€—æ—¶è¿½è¸ªå™¨
        episode_times = {
            'Action_Select': 0.0,
            'Env_Step': 0.0,
            'Data_Store': 0.0,
            'DQN_Learn': 0.0
        }

        step_count = 0

        while True:
            # --------------------------------------------------------
            # 1. åŠ¨ä½œé€‰æ‹© (Action Selection)
            # --------------------------------------------------------
            time_start_action = time.time()
            a_fc = FC_Agent.choose_action(s)  # FC å±€éƒ¨åŠ¨ä½œç´¢å¼• a_fc âˆˆ {0, ..., 31}
            a_bat = Bat_Agent.choose_action(s)  # Bat å±€éƒ¨åŠ¨ä½œç´¢å¼• a_bat âˆˆ {0, ..., 19}
            a_sc = SC_Agent.choose_action(s)  # SC å±€éƒ¨åŠ¨ä½œç´¢å¼• a_sc âˆˆ {0, 1}
            episode_times['Action_Select'] += (time.time() - time_start_action)

            # --------------------------------------------------------
            # 2. ç¯å¢ƒäº¤äº’ (Env Step)
            # --------------------------------------------------------
            action_list = [a_fc, a_bat, a_sc]
            time_start_step = time.time()
            s_, r, done, _ = env.step(action_list)
            episode_times['Env_Step'] += (time.time() - time_start_step)

            # --------------------------------------------------------
            # 3. å­˜å‚¨è½¬æ¢ (Data Storage)
            # --------------------------------------------------------
            time_start_store = time.time()
            transition = np.hstack((s, a_fc, a_bat, a_sc, r, s_))

            index = memory_counter[0] % MEMORY_CAPACITY
            if transition.shape[0] != MEMORY_WIDTH:
                raise RuntimeError(
                    f"å­˜å‚¨è½¬æ¢é•¿åº¦é”™è¯¯: æœŸæœ› {MEMORY_WIDTH}, å®é™… {transition.shape[0]}. è¯·æ£€æŸ¥ N_STATES å’ŒåŠ¨ä½œåˆ†è§£æ˜¯å¦æ­£ç¡®ã€‚")

            shared_memory[index, :] = transition
            memory_counter[0] += 1
            episode_times['Data_Store'] += (time.time() - time_start_store)

            ep_r += r
            step_count += 1

            # --------------------------------------------------------
            # 4. I-DQN ç‹¬ç«‹å­¦ä¹  (DQN Learn)
            # --------------------------------------------------------
            if memory_counter[0] > MEMORY_CAPACITY and memory_counter[0] % LEARN_FREQUENCY == 0:
                time_start_learn = time.time()
                # 0 for FC action column, 1 for Bat action column, 2 for SC action column
                FC_Agent.learn(0)
                Bat_Agent.learn(1)
                SC_Agent.learn(2)
                episode_times['DQN_Learn'] += (time.time() - time_start_learn)

            if done:
                writer.add_scalar("Ep_r/Ep", ep_r, i_episode)
                using_time_total = time.time() - start_time_total

                # ä½¿ç”¨ set_postfix å®æ—¶æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                episode_pbar.set_postfix({
                    'Ep_r': f'{ep_r:.2f}',
                    'Total_Time': f'{using_time_total:.2f}s',
                    'Env_Step_Time_ms': f"{(episode_times['Env_Step'] / step_count) * 1000:.2f}",
                })

                # â— æ‰“å°è¯¦ç»†çš„è€—æ—¶åˆ†è§£ç»“æœï¼ˆä»…åœ¨å‰ 5 å›åˆå’Œæ¯ 10 å›åˆæ‰“å°ï¼‰
                if i_episode < 2 or (i_episode + 1) % 100 == 0:
                    print_time_breakdown(i_episode + 1, episode_times)

                break

            s = s_

        x.append(int(i_episode))
        y.append(float(ep_r))

        # --------------------------------------------------------
        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        # --------------------------------------------------------
        if ep_r > reward_max:
            reward_max = ep_r
            net_name_base = (f"{base_path}/bs{BATCH_SIZE}_lr{int(LR * 10000)}_episode_{i_episode + 1}"
                             f"_pool{POOL_SIZE}_freq{LEARN_FREQUENCY}_MARL_{remark}_MAX_R{int(reward_max)}")

            # å¿…é¡»ä¿å­˜æ‰€æœ‰ä¸‰ä¸ªæ™ºèƒ½ä½“æ¨¡å‹
            torch.save(FC_Agent.eval_net.state_dict(), f"{net_name_base}_FC.pth")
            torch.save(Bat_Agent.eval_net.state_dict(), f"{net_name_base}_BAT.pth")
            torch.save(SC_Agent.eval_net.state_dict(), f"{net_name_base}_SC.pth")
            print(f"\nNew Max Value Models saved: {net_name_base}")  # åŠ æ¢è¡Œé˜²æ­¢è¢« tqdm è¦†ç›–

            if REAL_TIME_DRAW:
                ax.plot(i_episode, reward_max, 'ro')

        # å®æ—¶ç»˜å›¾
        if REAL_TIME_DRAW:
            line.set_xdata(x)
            line.set_ydata(y)
            ax.relim()
            ax.autoscale_view()
            plt.draw()
            plt.pause(0.01)

    # æœ€ç»ˆæ¨¡å‹ä¿å­˜
    final_net_name_base = (f"{base_path}/bs{BATCH_SIZE}_lr{int(LR * 10000)}_episode_{EPISODE}_pool{POOL_SIZE}"
                           f"_freq{LEARN_FREQUENCY}_MARL_{remark}_FINAL")
    torch.save(FC_Agent.eval_net.state_dict(), f"{final_net_name_base}_FC.pth")
    torch.save(Bat_Agent.eval_net.state_dict(), f"{final_net_name_base}_BAT.pth")
    torch.save(SC_Agent.eval_net.state_dict(), f"{final_net_name_base}_SC.pth")
    print(f"\nFinal models saved: {final_net_name_base}")

    writer.flush()
    writer.close()

    # ç»˜åˆ¶å¹¶ä¿å­˜æ›²çº¿
    if not REAL_TIME_DRAW:
        fig, ax = plt.subplots()
        ax.plot(x, y)

    try:
        plt.get_current_fig_manager().window.showMaximized()
    except Exception:
        pass

    plt.savefig(f"{base_path}/train_curve_MARL_IQL_bs{BATCH_SIZE}_lr{int(LR * 10000)}_ep{EPISODE}.svg")

    if REAL_TIME_DRAW:
        plt.ioff()
    plt.show()