import os
import time
import json
import subprocess
import sys
import argparse  # æ–°å¢ï¼šå¯¼å…¥å‚æ•°è§£ææ¨¡å—
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import torch

# å¯¼å…¥å…¬å…±æ¨¡å—
from MARL_Engine import (
    setup_project_root, device, 
    IndependentDQN, get_max_folder_name
)
project_root = setup_project_root()
from Scripts.Env import Envs

from Scripts.utils.global_utils import *
# è·å–å­—ä½“ï¼ˆä¼˜å…ˆå®‹ä½“+Times New Romanï¼Œè§£å†³ä¸­æ–‡/è´Ÿå·æ˜¾ç¤ºï¼‰
font_get()

# ====================== æ–°å¢ï¼šå‘½ä»¤è¡Œå‚æ•°è§£æ ======================
def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='MARLè®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒä»å¤´è®­ç»ƒ/ç»§ç»­è®­ç»ƒï¼‰')
    
    # æ ¸å¿ƒè®­ç»ƒæ¨¡å¼å‚æ•°
    parser.add_argument('--resume-training', action='store_true', 
                        help='æ˜¯å¦åŸºäºå·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆé»˜è®¤ï¼šä»å¤´è®­ç»ƒï¼‰')
    parser.add_argument('--pretrain-date', type=str, default="1213",
                        help='é¢„è®­ç»ƒæ¨¡å‹çš„æ—¥æœŸæ–‡ä»¶å¤¹ï¼ˆä»…resume-training=Trueæ—¶ç”Ÿæ•ˆï¼‰')
    parser.add_argument('--pretrain-train-id', type=str, default="6",
                        help='é¢„è®­ç»ƒæ¨¡å‹çš„train_idï¼ˆä»…resume-training=Trueæ—¶ç”Ÿæ•ˆï¼‰')
    parser.add_argument('--pretrain-model-prefix', type=str, 
                        default="bs64_lr1_ep_79_pool50_freq50_MARL_MARL_IQL_32x20x2_MAX_R-18",
                        help='é¢„è®­ç»ƒæ¨¡å‹å‰ç¼€ï¼ˆä»…resume-training=Trueæ—¶ç”Ÿæ•ˆï¼‰')
    
    # è®­ç»ƒè¶…å‚æ•°ï¼ˆå¯é€‰ï¼Œæ”¯æŒå‘½ä»¤è¡Œè¦†ç›–é»˜è®¤å€¼ï¼‰
    parser.add_argument('--batch-size', type=int, default=64, help='æ‰¹å¤§å°ï¼ˆé»˜è®¤ï¼š64ï¼‰')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡ï¼ˆé»˜è®¤ï¼š1e-4ï¼‰')
    parser.add_argument('--epsilon', type=float, default=0.9, help='æ¢ç´¢ç‡ï¼ˆé»˜è®¤ï¼š0.9ï¼‰')
    parser.add_argument('--gamma', type=float, default=0.9, help='æŠ˜æ‰£å› å­ï¼ˆé»˜è®¤ï¼š0.95ï¼‰')
    parser.add_argument('--pool-size', type=int, default=100, help='æ± å¤§å°ï¼ˆé»˜è®¤ï¼š50ï¼‰')
    parser.add_argument('--episode', type=int, default=1000, help='è®­ç»ƒå›åˆæ•°ï¼ˆé»˜è®¤ï¼š2000ï¼‰')
    parser.add_argument('--learn-frequency', type=int, default=50, help='å­¦ä¹ é¢‘ç‡ï¼ˆé»˜è®¤ï¼š50ï¼‰')
    
    # è·¯å¾„å‚æ•°ï¼ˆå¯é€‰ï¼‰
    parser.add_argument('--log-dir', type=str, default=None, help='TensorBoardæ—¥å¿—ç›®å½•ï¼ˆé»˜è®¤ï¼šè‡ªåŠ¨ç”Ÿæˆï¼‰')
    
    return parser.parse_args()

# è§£æå‚æ•°
args = parse_args()
# =====================================================================

# å…¨å±€è®¾ç½®ä¸è¶…å‚æ•°
env = Envs()
writer = SummaryWriter(log_dir=args.log_dir)  # ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æ—¥å¿—ç›®å½•
torch.set_default_dtype(torch.float32)

# ====================== åŠ¨æ€é…ç½®è¶…å‚æ•°ï¼ˆä»å‘½ä»¤è¡Œå‚æ•°è¯»å–ï¼‰ ======================
# æ ¸å¿ƒè¶…å‚æ•°ï¼ˆæ”¯æŒå‘½ä»¤è¡Œè¦†ç›–ï¼‰
BATCH_SIZE = args.batch_size
LR = args.lr
EPSILON = args.epsilon
GAMMA = args.gamma
TARGET_REPLACE_ITER = 100
POOL_SIZE = args.pool_size
EPISODE = args.episode
LEARN_FREQUENCY = args.learn_frequency
REAL_TIME_DRAW = False

# ç»§ç»­è®­ç»ƒé…ç½®ï¼ˆä»å‘½ä»¤è¡Œå‚æ•°è¯»å–ï¼‰
RESUME_TRAINING = args.resume_training
PRETRAIN_DATE = args.pretrain_date
PRETRAIN_TRAIN_ID = args.pretrain_train_id
PRETRAIN_MODEL_PREFIX = args.pretrain_model_prefix

# å­¦ä¹ ç‡è°ƒåº¦ä¸æ—©åœå‚æ•°
LR_PATIENCE = 50
LR_FACTOR = 0.5
EARLY_STOP_PATIENCE = 1000
REWARD_THRESHOLD = 0.001

# ç¯å¢ƒå‚æ•°
N_STATES = env.observation_space.shape[0]
N_TOTAL_ACTIONS = env.N_ACTIONS
N_FC_ACTIONS = 32
N_BAT_ACTIONS = 20
N_SC_ACTIONS = 2

# å†…å­˜é…ç½®
MEMORY_CAPACITY = env.step_length * POOL_SIZE
current_timestamp = time.time()
local_time = time.localtime(current_timestamp)
execute_date = time.strftime("%m%d", local_time)
execute_time = time.strftime("%H%M%S", local_time)  # æ–°å¢ï¼šè®°å½•å…·ä½“æ—¶é—´

# ====================== åŠ¨æ€ç”Ÿæˆremarkï¼ˆåŒ…å«å‘½ä»¤è¡Œå‚æ•°ä¿¡æ¯ï¼‰ ======================
if RESUME_TRAINING:
    # ç»§ç»­è®­ç»ƒæ—¶ï¼Œremarkæ ‡è®°åŸºç¡€æ¨¡å‹ä¿¡æ¯å’Œå‘½ä»¤è¡Œå‚æ•°
    remark = f"RESUME_{PRETRAIN_MODEL_PREFIX}_bs{BATCH_SIZE}_lr{int(LR*10000)}_MARL_IQL_32x20x2"
else:
    # ä»å¤´è®­ç»ƒæ—¶ï¼ŒremarkåŒ…å«è¶…å‚æ•°ä¿¡æ¯
    remark = f"FROM_SCRATCH_bs{BATCH_SIZE}_lr{int(LR*10000)}_MARL_IQL_32x20x2"
# =====================================================================

# æ–°å¢ï¼šå…¨å±€å˜é‡å­˜å‚¨æœ€ä¼˜æ¨¡å‹æ–‡ä»¶å
best_model_base_name = ""

# éªŒè¯åŠ¨ä½œåˆ†è§£
N_EXPECTED_ACTIONS = N_FC_ACTIONS * N_BAT_ACTIONS * N_SC_ACTIONS
if N_EXPECTED_ACTIONS != N_TOTAL_ACTIONS:
    print(f"è­¦å‘Šï¼šåŠ¨ä½œåˆ†è§£ {N_EXPECTED_ACTIONS} ä¸ç¯å¢ƒ N_TOTAL_ACTIONS({N_TOTAL_ACTIONS}) ä¸åŒ¹é…")

# æ–°å¢ï¼šå®šä¹‰ä¿å­˜è¶…å‚æ•°çš„å‡½æ•°ï¼ˆæ–°å¢å‘½ä»¤è¡Œå‚æ•°è®°å½•ï¼‰
def save_hyperparameters(save_path, final_metrics=None):
    """
    ä¿å­˜è¶…å‚æ•°åˆ°æŒ‡å®šè·¯å¾„ï¼ˆtxtå’Œjsonæ ¼å¼ï¼‰
    :param save_path: ä¿å­˜ç›®å½•
    :param final_metrics: è®­ç»ƒæœ€ç»ˆæŒ‡æ ‡ï¼ˆå¦‚æœ€å¤§å¥–åŠ±ã€æœ€ç»ˆå¥–åŠ±ç­‰ï¼‰
    """
    # æ•´ç†è¶…å‚æ•°å­—å…¸ï¼ˆæ–°å¢å‘½ä»¤è¡Œå‚æ•°è®°å½•ï¼‰
    hyperparams = {
        # åŸºç¡€ä¿¡æ¯
        "train_info": {
            "execute_date": execute_date,
            "execute_time": execute_time,
            "train_id": os.path.basename(save_path),
            "remark": remark,
            "device": str(device),
            "total_training_time_s": round(time.time() - start_time_total, 2) if 'start_time_total' in globals() else 0,
            "best_model_base_name": best_model_base_name,
            "best_model_full_path": os.path.join(save_path, best_model_base_name) if best_model_base_name else "",
            "resume_training": RESUME_TRAINING,
            "command_line_args": vars(args),  # æ–°å¢ï¼šè®°å½•æ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°
            "pretrain_model_info": {
                "pretrain_date": PRETRAIN_DATE if RESUME_TRAINING else "",
                "pretrain_train_id": PRETRAIN_TRAIN_ID if RESUME_TRAINING else "",
                "pretrain_model_prefix": PRETRAIN_MODEL_PREFIX if RESUME_TRAINING else ""
            }
        },
        # æ ¸å¿ƒè¶…å‚æ•°
        "core_hyperparams": {
            "BATCH_SIZE": BATCH_SIZE,
            "LR": LR,
            "EPSILON": EPSILON,
            "GAMMA": GAMMA,
            "TARGET_REPLACE_ITER": TARGET_REPLACE_ITER,
            "POOL_SIZE": POOL_SIZE,
            "EPISODE": EPISODE,
            "LEARN_FREQUENCY": LEARN_FREQUENCY,
            "MEMORY_CAPACITY": MEMORY_CAPACITY,
            "REAL_TIME_DRAW": REAL_TIME_DRAW
        },
        # å­¦ä¹ ç‡è°ƒåº¦ä¸æ—©åœå‚æ•°
        "lr_earlystop_params": {
            "LR_PATIENCE": LR_PATIENCE,
            "LR_FACTOR": LR_FACTOR,
            "EARLY_STOP_PATIENCE": EARLY_STOP_PATIENCE,
            "REWARD_THRESHOLD": REWARD_THRESHOLD
        },
        # ç¯å¢ƒå‚æ•°
        "env_params": {
            "N_STATES": N_STATES,
            "N_TOTAL_ACTIONS": N_TOTAL_ACTIONS,
            "N_FC_ACTIONS": N_FC_ACTIONS,
            "N_BAT_ACTIONS": N_BAT_ACTIONS,
            "N_SC_ACTIONS": N_SC_ACTIONS,
            "N_EXPECTED_ACTIONS": N_EXPECTED_ACTIONS,
            "step_length": env.step_length if hasattr(env, 'step_length') else "unknown"
        },
        # è®­ç»ƒç»“æœæŒ‡æ ‡
        "training_metrics": final_metrics or {}
    }

    # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆä¾¿äºç¨‹åºè§£æï¼‰
    json_path = os.path.join(save_path, "hyperparameters.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(hyperparams, f, indent=4, ensure_ascii=False)

    # ä¿å­˜ä¸ºTXTæ ¼å¼ï¼ˆä¾¿äºäººå·¥é˜…è¯»ï¼‰
    txt_path = os.path.join(save_path, "hyperparameters.txt")
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("                è®­ç»ƒè¶…å‚æ•°æ±‡æ€»                \n")
        f.write("=" * 80 + "\n\n")
        
        for section, params in hyperparams.items():
            f.write(f"ã€{section.upper()}ã€‘\n")
            f.write("-" * 60 + "\n")
            for key, value in params.items():
                # å¯¹å…³é”®ä¿¡æ¯é«˜äº®æ˜¾ç¤º
                if key in ["best_model_base_name", "best_model_full_path", "resume_training", "pretrain_model_prefix", "command_line_args"]:
                    f.write(f"{key:<30}: \033[1;32m{value}\033[0m\n")  # ç»¿è‰²é«˜äº®
                else:
                    f.write(f"{key:<30}: {value}\n")
            f.write("\n")

    # å•ç‹¬æ‰“å°æœ€ä¼˜æ¨¡å‹åç§°ï¼ˆæ–¹ä¾¿ç›´æ¥å¤åˆ¶ï¼‰
    if best_model_base_name:
        print(f"\nğŸ¯ æœ€ä¼˜æ¨¡å‹æ–‡ä»¶åå‰ç¼€ï¼ˆå¯ç›´æ¥å¤åˆ¶ï¼‰ï¼š")
        print(f"   {best_model_base_name}")
    print(f"\nâœ… è¶…å‚æ•°å·²ä¿å­˜åˆ°ï¼š")
    print(f"   JSONæ ¼å¼: {json_path}")
    print(f"   TXTæ ¼å¼: {txt_path}")

# æ—¶é—´åˆ†è§£æ‰“å°å‡½æ•°
def print_time_breakdown(episode, episode_times):
    total_time = sum(episode_times.values())
    if total_time < 1e-6:
        print(f"å›åˆ {episode} è€—æ—¶è¿‡çŸ­ï¼Œè·³è¿‡è€—æ—¶åˆ†æã€‚")
        return

    print("\n" + "=" * 45)
    print(f"ğŸš€ å›åˆ {episode} è€—æ—¶åˆ†è§£ (æ€»è€—æ—¶: {total_time:.4f} s)")
    print("-" * 45)
    for name, time_val in sorted(episode_times.items(), key=lambda x: x[1], reverse=True):
        percentage = (time_val / total_time) * 100
        print(f"| {name.ljust(15)} | {time_val:9.4f} s | {percentage:6.2f} % |")
    print("=" * 45)

# ====================== åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰ ======================
def load_pretrained_models(agents, pretrain_date, pretrain_train_id, model_prefix):
    """
    åŠ è½½é¢„è®­ç»ƒæ¨¡å‹åˆ°æ™ºèƒ½ä½“
    :param agents: æ™ºèƒ½ä½“åˆ—è¡¨ [FC_Agent, Bat_Agent, SC_Agent]
    :param pretrain_date: é¢„è®­ç»ƒæ¨¡å‹çš„æ—¥æœŸæ–‡ä»¶å¤¹
    :param pretrain_train_id: é¢„è®­ç»ƒæ¨¡å‹çš„train_id
    :param model_prefix: é¢„è®­ç»ƒæ¨¡å‹å‰ç¼€
    """
    pretrain_base_dir = os.path.join(project_root, "nets", "Chap3", pretrain_date, pretrain_train_id)
    model_paths = {
        "FC_Agent": os.path.join(pretrain_base_dir, f"{model_prefix}_FC.pth"),
        "Bat_Agent": os.path.join(pretrain_base_dir, f"{model_prefix}_BAT.pth"),
        "SC_Agent": os.path.join(pretrain_base_dir, f"{model_prefix}_SC.pth")
    }

    for agent in agents:
        model_path = model_paths[agent.agent_name]
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        try:
            agent.eval_net.load_state_dict(torch.load(model_path, map_location=device))
            agent.target_net.load_state_dict(agent.eval_net.state_dict())
            print(f"âœ… æˆåŠŸåŠ è½½{agent.agent_name}é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½{agent.agent_name}æ¨¡å‹å¤±è´¥: {e}")

    print("\nğŸ‰ æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆï¼")
# =====================================================================

if __name__ == '__main__':
    # æ‰“å°å‘½ä»¤è¡Œå‚æ•°ï¼ˆä¾¿äºç¡®è®¤é…ç½®ï¼‰
    print("=" * 80)
    print("                    è®­ç»ƒé…ç½®ç¡®è®¤                  ")
    print("=" * 80)
    print(f"è®­ç»ƒæ¨¡å¼: {'ç»§ç»­è®­ç»ƒï¼ˆåŸºäºå·²æœ‰æ¨¡å‹ï¼‰' if RESUME_TRAINING else 'ä»å¤´è®­ç»ƒ'}")
    if RESUME_TRAINING:
        print(f"é¢„è®­ç»ƒæ¨¡å‹é…ç½®:")
        print(f"  - æ—¥æœŸæ–‡ä»¶å¤¹: {PRETRAIN_DATE}")
        print(f"  - Train ID: {PRETRAIN_TRAIN_ID}")
        print(f"  - æ¨¡å‹å‰ç¼€: {PRETRAIN_MODEL_PREFIX}")
    print(f"æ ¸å¿ƒè¶…å‚æ•°:")
    print(f"  - æ‰¹å¤§å°: {BATCH_SIZE}")
    print(f"  - å­¦ä¹ ç‡: {LR:.6f}")
    print(f"  - æ¢ç´¢ç‡: {EPSILON}")
    print(f"  - è®­ç»ƒå›åˆæ•°: {EPISODE}")
    print("=" * 80 + "\n")

    # è·¯å¾„è®¾ç½®
    TARGET_BASE_DIR = os.path.join(project_root, "nets", "Chap3", execute_date)
    os.makedirs(TARGET_BASE_DIR, exist_ok=True)
    train_id = get_max_folder_name(TARGET_BASE_DIR)
    base_path = f"{TARGET_BASE_DIR}/{train_id}"
    os.makedirs(base_path)

    # å…±äº«å†…å­˜åˆå§‹åŒ–
    MEMORY_WIDTH = N_STATES * 2 + 4
    shared_memory = np.zeros((MEMORY_CAPACITY, MEMORY_WIDTH))
    memory_counter = [0]

    # åˆå§‹åŒ–æ™ºèƒ½ä½“
    FC_Agent = IndependentDQN(
        "FC_Agent", N_STATES, N_FC_ACTIONS, 
        shared_memory, memory_counter
    )
    Bat_Agent = IndependentDQN(
        "Bat_Agent", N_STATES, N_BAT_ACTIONS, 
        shared_memory, memory_counter
    )
    SC_Agent = IndependentDQN(
        "SC_Agent", N_STATES, N_SC_ACTIONS, 
        shared_memory, memory_counter
    )
    all_agents = [FC_Agent, Bat_Agent, SC_Agent]

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä»å‘½ä»¤è¡Œå‚æ•°åˆ¤æ–­ï¼‰
    if RESUME_TRAINING:
        print("\nğŸ“Œ å¼€å§‹åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        load_pretrained_models(all_agents, PRETRAIN_DATE, PRETRAIN_TRAIN_ID, PRETRAIN_MODEL_PREFIX)

    # è®¾ç½®ä¼˜åŒ–å™¨
    for agent in all_agents:
        agent.setup_optimizer(LR, LR_FACTOR, LR_PATIENCE)

    # è®­ç»ƒè¿‡ç¨‹
    print('\nCollecting experience and learning (I-DQN, 3-Agent)...')
    start_time_total = time.time()
    reward_max = -float('inf')
    reward_not_improve_episodes = 0
    training_done = False
    x, y = [], []

    if REAL_TIME_DRAW:
        plt.ion()
        fig, ax = plt.subplots()
        line, = ax.plot(x, y)

    episode_pbar = tqdm(range(EPISODE), desc=f"RL Training")
    for i_episode in episode_pbar:
        if training_done:
            break

        s = env.reset()
        ep_r = 0
        episode_times = {
            'Action_Select': 0.0,
            'Env_Step': 0.0,
            'Data_Store': 0.0,
            'DQN_Learn': 0.0
        }
        step_count = 0

        while True:
            # åŠ¨ä½œé€‰æ‹©
            time_start_action = time.time()
            a_fc = FC_Agent.choose_action(s, train=True, epsilon=EPSILON)
            a_bat = Bat_Agent.choose_action(s, train=True, epsilon=EPSILON)
            a_sc = SC_Agent.choose_action(s, train=True, epsilon=EPSILON)
            episode_times['Action_Select'] += (time.time() - time_start_action)

            # ç¯å¢ƒäº¤äº’
            action_list = [a_fc, a_bat, a_sc]
            time_start_step = time.time()
            s_, r, done, _ = env.step(action_list)
            episode_times['Env_Step'] += (time.time() - time_start_step)

            # å­˜å‚¨è½¬æ¢
            time_start_store = time.time()
            transition = np.hstack((s, a_fc, a_bat, a_sc, r, s_))
            index = memory_counter[0] % MEMORY_CAPACITY
            if transition.shape[0] != MEMORY_WIDTH:
                raise RuntimeError(f"å­˜å‚¨è½¬æ¢é•¿åº¦é”™è¯¯: æœŸæœ› {MEMORY_WIDTH}, å®é™… {transition.shape[0]}")
            shared_memory[index, :] = transition
            memory_counter[0] += 1
            episode_times['Data_Store'] += (time.time() - time_start_store)

            ep_r += r
            step_count += 1

            # å­¦ä¹ è¿‡ç¨‹
            if memory_counter[0] > MEMORY_CAPACITY and memory_counter[0] % LEARN_FREQUENCY == 0:
                time_start_learn = time.time()
                FC_Agent.learn(0, N_STATES, GAMMA, TARGET_REPLACE_ITER, BATCH_SIZE)
                Bat_Agent.learn(1, N_STATES, GAMMA, TARGET_REPLACE_ITER, BATCH_SIZE)
                SC_Agent.learn(2, N_STATES, GAMMA, TARGET_REPLACE_ITER, BATCH_SIZE)
                episode_times['DQN_Learn'] += (time.time() - time_start_learn)

            if done:
                writer.add_scalar("Ep_r/Ep", ep_r, i_episode)
                using_time_total = time.time() - start_time_total
                current_lr = FC_Agent.optimizer.param_groups[0]["lr"]
                episode_pbar.set_postfix({
                    'Ep_r': f'{ep_r:.2f}',
                    'LR': f'{current_lr:.2e}',
                    'Total_Time': f'{using_time_total:.2f}s',
                })

                if i_episode < 2 or (i_episode + 1) % 100 == 0:
                    print_time_breakdown(i_episode + 1, episode_times)
                break

            s = s_

        x.append(i_episode)
        y.append(ep_r)

        # æ¨¡å‹ä¿å­˜ä¸æ—©åœé€»è¾‘
        if ep_r > reward_max + REWARD_THRESHOLD:
            reward_max = ep_r
            reward_not_improve_episodes = 0
            best_model_base_name = (f"bs{BATCH_SIZE}_lr{int(LR*10000)}_ep_{i_episode+1}"
                                   f"_pool{POOL_SIZE}_freq{LEARN_FREQUENCY}_MARL_{remark}_MAX_R{int(reward_max)}")
            net_name_base = best_model_base_name
            torch.save(FC_Agent.eval_net.state_dict(), f"{base_path}/{net_name_base}_FC.pth")
            torch.save(Bat_Agent.eval_net.state_dict(), f"{base_path}/{net_name_base}_BAT.pth")
            torch.save(SC_Agent.eval_net.state_dict(), f"{base_path}/{net_name_base}_SC.pth")
            print(f"\n--- New Max Reward: {reward_max:.2f} | Models saved: {net_name_base} ---")
        else:
            reward_not_improve_episodes += 1

        # å­¦ä¹ ç‡è°ƒåº¦
        for agent in all_agents:
            agent.scheduler.step(ep_r)

        # æ—©åœæ£€æŸ¥
        if reward_not_improve_episodes >= EARLY_STOP_PATIENCE:
            print(f"\n--- Early Stopping Triggered! ---")
            training_done = True

    # æœ€ç»ˆå¤„ç†
    final_episode = i_episode + 1 if not training_done else i_episode
    final_net_name_base = (f"{base_path}/final_bs{BATCH_SIZE}_lr{int(LR*10000)}_ep_{final_episode}_pool{POOL_SIZE}"
                         f"_freq{LEARN_FREQUENCY}_MARL_{remark}_FINAL")
    torch.save(FC_Agent.eval_net.state_dict(), f"{final_net_name_base}_FC.pth")
    torch.save(Bat_Agent.eval_net.state_dict(), f"{final_net_name_base}_BAT.pth")
    torch.save(SC_Agent.eval_net.state_dict(), f"{final_net_name_base}_SC.pth")
    print(f"\nFinal models saved: {final_net_name_base}")

    # æ•´ç†è®­ç»ƒæœ€ç»ˆæŒ‡æ ‡
    final_metrics = {
        "max_reward": round(reward_max, 4),
        "final_reward": round(y[-1], 4) if y else 0,
        "average_reward": round(np.mean(y) if y else 0, 4),
        "total_episodes_completed": final_episode,
        "early_stopped": training_done,
        "final_learning_rate": round(FC_Agent.optimizer.param_groups[0]["lr"], 6),
        "reward_not_improve_episodes": reward_not_improve_episodes,
        "best_model_reward": round(reward_max, 4)
    }

    # ä¿å­˜è¶…å‚æ•°ï¼ˆåŒ…å«å‘½ä»¤è¡Œå‚æ•°è®°å½•ï¼‰
    save_hyperparameters(base_path, final_metrics)

    # å¯è§†åŒ–ä¸ä¿å­˜
    writer.flush()
    writer.close()
    plt.figure()
    plt.plot(x, y)
    plt.xlabel('Episode')
    plt.ylabel('Episode Reward')
    plt.title(f'Training Curve (MARL_IQL, Ep={final_episode})')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.savefig(f"{base_path}/train_curve_MARL_IQL_bs{BATCH_SIZE}_lr{int(LR*10000)}_ep{final_episode}.svg")
    if REAL_TIME_DRAW:
        plt.ioff()
        plt.show()

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {base_path}")
    if best_model_base_name:
        print(f"\nğŸ“‹ æœ€ä¼˜æ¨¡å‹æ–‡ä»¶åå‰ç¼€ï¼ˆç›´æ¥å¤åˆ¶å³å¯ï¼‰ï¼š")
        print(f"{best_model_base_name}")


    # æ‰§è¡Œæµ‹è¯•
    test_script_path = os.path.join(project_root, "Scripts", "Chapter3", "test.py")
    # æ„é€ æµ‹è¯•å‘½ä»¤å‚æ•°
    test_cmd = [
        str(sys.executable),                # Pythonè§£é‡Šå™¨ï¼ˆå¼ºåˆ¶è½¬å­—ç¬¦ä¸²ï¼‰
        str(test_script_path),              # æµ‹è¯•è„šæœ¬è·¯å¾„ï¼ˆå¼ºåˆ¶è½¬å­—ç¬¦ä¸²ï¼‰
        "--net-date", str(execute_date),    # æ—¥æœŸï¼ˆå¼ºåˆ¶è½¬å­—ç¬¦ä¸²ï¼‰
        "--train-id", str(train_id),        # train_idï¼ˆå¼ºåˆ¶è½¬å­—ç¬¦ä¸²ï¼‰
        "--model-prefix", str(best_model_base_name)  # æ¨¡å‹å‰ç¼€ï¼ˆå¼ºåˆ¶è½¬å­—ç¬¦ä¸²ï¼‰
    ]
    # æ‰§è¡Œæµ‹è¯•è„šæœ¬
    print("\nğŸš€ å¼€å§‹æ‰§è¡Œæµ‹è¯•è„šæœ¬...")
    subprocess.run(test_cmd, check=True)

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {base_path}")
    if best_model_base_name:
        print(f"\nğŸ“‹ æœ€ä¼˜æ¨¡å‹æ–‡ä»¶åå‰ç¼€ï¼ˆç›´æ¥å¤åˆ¶å³å¯ï¼‰ï¼š")
        print(f"{best_model_base_name}")