import os
import time
import json
import subprocess
import sys
import argparse  # æ–°å¢ï¼šå¯¼å…¥å‚æ•°è§£ææ¨¡å—
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import torch

# å¯¼å…¥å…¬å…±æ¨¡å—
from MARL_Engine import (
    setup_project_root, device, 
    IndependentDQN, get_max_folder_name
)
project_root = setup_project_root()
from Scripts.Env import Envs

from Scripts.utils.global_utils import *
# è·å–å­—ä½“ï¼ˆä¼˜å…ˆå®‹ä½“+Times New Romanï¼Œè§£å†³ä¸­æ–‡/è´Ÿå·æ˜¾ç¤ºï¼‰
font_get()

# ====================== æ–°å¢ï¼šå‘½ä»¤è¡Œå‚æ•°è§£æ ======================
def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='MARLè®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒä»å¤´è®­ç»ƒ/ç»§ç»­è®­ç»ƒï¼‰')
    
    # æ ¸å¿ƒè®­ç»ƒæ¨¡å¼å‚æ•°
    parser.add_argument('--resume-training', action='store_true', 
                        help='æ˜¯å¦åŸºäºå·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆé»˜è®¤ï¼šä»å¤´è®­ç»ƒï¼‰')
    parser.add_argument('--pretrain-date', type=str, default="1217",
                        help='é¢„è®­ç»ƒæ¨¡å‹çš„æ—¥æœŸæ–‡ä»¶å¤¹ï¼ˆä»…resume-training=Trueæ—¶ç”Ÿæ•ˆï¼‰')
    parser.add_argument('--pretrain-train-id', type=str, default="37",
                        help='é¢„è®­ç»ƒæ¨¡å‹çš„train_idï¼ˆä»…resume-training=Trueæ—¶ç”Ÿæ•ˆï¼‰')
    parser.add_argument('--pretrain-model-prefix', type=str, 
                        default="MARL_Model",  # ç®€åŒ–é¢„è®­ç»ƒæ¨¡å‹å‰ç¼€
                        help='é¢„è®­ç»ƒæ¨¡å‹å‰ç¼€ï¼ˆä»…resume-training=Trueæ—¶ç”Ÿæ•ˆï¼‰')
    
    # ç»§ç»­è®­ç»ƒç¤ºä¾‹ä»£ç ï¼š--resume-training --pretrain-date 1218 --pretrain-train-id 2

    # è®­ç»ƒè¶…å‚æ•°ï¼ˆå¯é€‰ï¼Œæ”¯æŒå‘½ä»¤è¡Œè¦†ç›–é»˜è®¤å€¼ï¼‰
    parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹å¤§å°ï¼ˆé»˜è®¤ï¼š32ï¼‰')
    parser.add_argument('--lr', type=float, default=1e-5, help='å­¦ä¹ ç‡ï¼ˆé»˜è®¤ï¼š1e-5ï¼‰')
    parser.add_argument('--epsilon', type=float, default=0.9, help='æ¢ç´¢ç‡ï¼ˆé»˜è®¤ï¼š0.9ï¼‰')
    parser.add_argument('--gamma', type=float, default=0.95, help='æŠ˜æ‰£å› å­ï¼ˆé»˜è®¤ï¼š0.95ï¼‰')
    parser.add_argument('--pool-size', type=int, default=100, help='æ± å¤§å°ï¼ˆé»˜è®¤ï¼š50ï¼‰')
    parser.add_argument('--episode', type=int, default=2000, help='è®­ç»ƒå›åˆæ•°ï¼ˆé»˜è®¤ï¼š1000ï¼‰')
    parser.add_argument('--learn-frequency', type=int, default=5, help='å­¦ä¹ é¢‘ç‡ï¼ˆé»˜è®¤ï¼š50ï¼‰')
    parser.add_argument('--remark', type=str, default="", help='å¤‡æ³¨')
    
    # è·¯å¾„å‚æ•°ï¼ˆå¯é€‰ï¼‰
    parser.add_argument('--log-dir', type=str, default=None, help='TensorBoardæ—¥å¿—ç›®å½•ï¼ˆé»˜è®¤ï¼šè‡ªåŠ¨ç”Ÿæˆï¼‰')
    
    return parser.parse_args()

# è§£æå‚æ•°
args = parse_args()
# =====================================================================

# å…¨å±€è®¾ç½®ä¸è¶…å‚æ•°
env = Envs()
writer = SummaryWriter(log_dir=args.log_dir)  # ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æ—¥å¿—ç›®å½•
torch.set_default_dtype(torch.float32)

# ====================== åŠ¨æ€é…ç½®è¶…å‚æ•°ï¼ˆä»å‘½ä»¤è¡Œå‚æ•°è¯»å–ï¼‰ ======================
# æ ¸å¿ƒè¶…å‚æ•°ï¼ˆæ”¯æŒå‘½ä»¤è¡Œè¦†ç›–ï¼‰
BATCH_SIZE = args.batch_size
LR = args.lr
EPSILON = args.epsilon
GAMMA = args.gamma
TARGET_REPLACE_ITER = 100
POOL_SIZE = args.pool_size
EPISODE = args.episode
LEARN_FREQUENCY = args.learn_frequency
REAL_TIME_DRAW = False

# ç»§ç»­è®­ç»ƒé…ç½®ï¼ˆä»å‘½ä»¤è¡Œå‚æ•°è¯»å–ï¼‰
RESUME_TRAINING = args.resume_training
PRETRAIN_DATE = args.pretrain_date
PRETRAIN_TRAIN_ID = args.pretrain_train_id
PRETRAIN_MODEL_PREFIX = args.pretrain_model_prefix
GLOBAL_SEED = 42

# å­¦ä¹ ç‡è°ƒåº¦ä¸æ—©åœå‚æ•°
LR_PATIENCE = 50
LR_FACTOR = 0.5
EARLY_STOP_PATIENCE = 1000
REWARD_THRESHOLD = 0.001

# ç¯å¢ƒå‚æ•°
N_STATES = env.observation_space.shape[0]
N_TOTAL_ACTIONS = env.N_ACTIONS
N_FC_ACTIONS = 32
N_BAT_ACTIONS = 40
N_SC_ACTIONS = 2

# å†…å­˜é…ç½®
MEMORY_CAPACITY = env.step_length * POOL_SIZE
current_timestamp = time.time()
local_time = time.localtime(current_timestamp)
execute_date = time.strftime("%m%d", local_time)
execute_time = time.strftime("%H%M%S", local_time)  # æ–°å¢ï¼šè®°å½•å…·ä½“æ—¶é—´

# ====================== å…ˆåˆå§‹åŒ–remarkï¼ˆåç»­åœ¨mainä¸­æ›´æ–°ï¼‰ ======================
remark = args.remark
# =====================================================================

torch.manual_seed(GLOBAL_SEED)

# æ–°å¢ï¼šå…¨å±€å˜é‡å­˜å‚¨æœ€ä¼˜æ¨¡å‹æ–‡ä»¶åï¼ˆç®€åŒ–ä¸ºå›ºå®šå‰ç¼€ï¼‰
best_model_base_name = "MARL_Model"

# éªŒè¯åŠ¨ä½œåˆ†è§£
N_EXPECTED_ACTIONS = N_FC_ACTIONS * N_BAT_ACTIONS * N_SC_ACTIONS
if N_EXPECTED_ACTIONS != N_TOTAL_ACTIONS:
    print(f"è­¦å‘Šï¼šåŠ¨ä½œåˆ†è§£ {N_EXPECTED_ACTIONS} ä¸ç¯å¢ƒ N_TOTAL_ACTIONS({N_TOTAL_ACTIONS}) ä¸åŒ¹é…")

# æ–°å¢ï¼šå®šä¹‰ä¿å­˜è¶…å‚æ•°çš„å‡½æ•°ï¼ˆé€‚é…ç®€åŒ–çš„æ¨¡å‹åç§°ï¼‰
def save_hyperparameters(save_path, final_metrics=None):
    """
    ä¿å­˜è¶…å‚æ•°åˆ°æŒ‡å®šè·¯å¾„ï¼ˆtxtå’Œjsonæ ¼å¼ï¼‰
    :param save_path: ä¿å­˜ç›®å½•
    :param final_metrics: è®­ç»ƒæœ€ç»ˆæŒ‡æ ‡ï¼ˆå¦‚æœ€å¤§å¥–åŠ±ã€æœ€ç»ˆå¥–åŠ±ç­‰ï¼‰
    """
    # æ•´ç†è¶…å‚æ•°å­—å…¸ï¼ˆæ–°å¢å‘½ä»¤è¡Œå‚æ•°è®°å½•ï¼‰
    hyperparams = {
        # åŸºç¡€ä¿¡æ¯
        "train_info": {
            "execute_date": execute_date,
            "execute_time": execute_time,
            "train_id": os.path.basename(save_path),
            "remark": remark,
            "device": str(device),
            "seed": GLOBAL_SEED,
            "total_training_time_s": round(time.time() - start_time_total, 2) if 'start_time_total' in globals() else 0,
            "best_model_base_name": best_model_base_name,
            "best_model_full_path": os.path.join(save_path, best_model_base_name) if best_model_base_name else "",
            "resume_training": RESUME_TRAINING,
            "command_line_args": vars(args),  # æ–°å¢ï¼šè®°å½•æ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°
            "pretrain_model_info": {
                "pretrain_date": PRETRAIN_DATE if RESUME_TRAINING else "",
                "pretrain_train_id": PRETRAIN_TRAIN_ID if RESUME_TRAINING else "",
                "pretrain_model_prefix": PRETRAIN_MODEL_PREFIX if RESUME_TRAINING else ""
            }
        },
        # æ ¸å¿ƒè¶…å‚æ•°
        "core_hyperparams": {
            "BATCH_SIZE": BATCH_SIZE,
            "LR": LR,
            "EPSILON": EPSILON,
            "GAMMA": GAMMA,
            "TARGET_REPLACE_ITER": TARGET_REPLACE_ITER,
            "POOL_SIZE": POOL_SIZE,
            "EPISODE": EPISODE,
            "LEARN_FREQUENCY": LEARN_FREQUENCY,
            "MEMORY_CAPACITY": MEMORY_CAPACITY,
            "REAL_TIME_DRAW": REAL_TIME_DRAW
        },
        # å­¦ä¹ ç‡è°ƒåº¦ä¸æ—©åœå‚æ•°
        "lr_earlystop_params": {
            "LR_PATIENCE": LR_PATIENCE,
            "LR_FACTOR": LR_FACTOR,
            "EARLY_STOP_PATIENCE": EARLY_STOP_PATIENCE,
            "REWARD_THRESHOLD": REWARD_THRESHOLD
        },
        # ç¯å¢ƒå‚æ•°
        "env_params": {
            "N_STATES": N_STATES,
            "N_TOTAL_ACTIONS": N_TOTAL_ACTIONS,
            "N_FC_ACTIONS": N_FC_ACTIONS,
            "N_BAT_ACTIONS": N_BAT_ACTIONS,
            "N_SC_ACTIONS": N_SC_ACTIONS,
            "N_EXPECTED_ACTIONS": N_EXPECTED_ACTIONS,
            "step_length": env.step_length if hasattr(env, 'step_length') else "unknown"
        },
        # è®­ç»ƒç»“æœæŒ‡æ ‡
        "training_metrics": final_metrics or {}
    }

    # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆä¾¿äºç¨‹åºè§£æï¼‰
    json_path = os.path.join(save_path, "hyperparameters.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(hyperparams, f, indent=4, ensure_ascii=False)

    # ä¿å­˜ä¸ºTXTæ ¼å¼ï¼ˆä¾¿äºäººå·¥é˜…è¯»ï¼‰
    txt_path = os.path.join(save_path, "hyperparameters.txt")
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("                è®­ç»ƒè¶…å‚æ•°æ±‡æ€»                \n")
        f.write("=" * 80 + "\n\n")
        
        for section, params in hyperparams.items():
            f.write(f"ã€{section.upper()}ã€‘\n")
            f.write("-" * 60 + "\n")
            for key, value in params.items():
                # å¯¹å…³é”®ä¿¡æ¯é«˜äº®æ˜¾ç¤º
                if key in ["best_model_base_name", "best_model_full_path", "resume_training", "pretrain_model_prefix", "command_line_args"]:
                    f.write(f"{key:<30}: \033[1;32m{value}\033[0m\n")  # ç»¿è‰²é«˜äº®
                else:
                    f.write(f"{key:<30}: {value}\n")
            f.write("\n")

    # å•ç‹¬æ‰“å°æœ€ä¼˜æ¨¡å‹åç§°ï¼ˆæ–¹ä¾¿ç›´æ¥å¤åˆ¶ï¼‰
    if best_model_base_name:
        print(f"\nğŸ¯ æœ€ä¼˜æ¨¡å‹æ–‡ä»¶åå‰ç¼€ï¼ˆå¯ç›´æ¥å¤åˆ¶ï¼‰ï¼š")
        print(f"   {best_model_base_name}")
    print(f"\nâœ… è¶…å‚æ•°å·²ä¿å­˜åˆ°ï¼š")
    print(f"   JSONæ ¼å¼: {json_path}")
    print(f"   TXTæ ¼å¼: {txt_path}")

# æ—¶é—´åˆ†è§£æ‰“å°å‡½æ•°
def print_time_breakdown(episode, episode_times):
    total_time = sum(episode_times.values())
    if total_time < 1e-6:
        print(f"å›åˆ {episode} è€—æ—¶è¿‡çŸ­ï¼Œè·³è¿‡è€—æ—¶åˆ†æã€‚")
        return

    print("\n" + "=" * 45)
    print(f"ğŸš€ å›åˆ {episode} è€—æ—¶åˆ†è§£ (æ€»è€—æ—¶: {total_time:.4f} s)")
    print("-" * 45)
    for name, time_val in sorted(episode_times.items(), key=lambda x: x[1], reverse=True):
        percentage = (time_val / total_time) * 100
        print(f"| {name.ljust(15)} | {time_val:9.4f} s | {percentage:6.2f} % |")
    print("=" * 45)

# ====================== åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å‡½æ•°ï¼ˆå®Œå…¨ä¿®å¤ç‰ˆï¼‰ ======================
def load_pretrained_models(agents, pretrain_date, pretrain_train_id, model_prefix, shared_memory, memory_counter):
    """
    åŠ è½½é¢„è®­ç»ƒæ¨¡å‹åˆ°æ™ºèƒ½ä½“ï¼ˆæ”¯æŒé€ä¸ªæ£€æŸ¥ï¼Œäº¤äº’ç¡®è®¤é‡æ–°åˆå§‹åŒ–/ç»ˆæ­¢è®­ç»ƒï¼‰
    è¿”å›ï¼šæ›´æ–°åçš„ç‹¬ç«‹æ™ºèƒ½ä½“å®ä¾‹ + åˆ—è¡¨
    """
    pretrain_base_dir = os.path.join(project_root, "nets", "Chap3", pretrain_date, pretrain_train_id)
    model_paths = {
        "FC_Agent": os.path.join(pretrain_base_dir, f"{model_prefix}_FC.pth"),
        "Bat_Agent": os.path.join(pretrain_base_dir, f"{model_prefix}_BAT.pth"),
        "SC_Agent": os.path.join(pretrain_base_dir, f"{model_prefix}_SC.pth")
    }

    # è®°å½•ç¼ºå¤±çš„æ™ºèƒ½ä½“åç§°
    missing_agent_names = []
    existing_agents = []

    # ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥æ‰€æœ‰æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for agent in agents:
        model_path = model_paths[agent.agent_name]
        if not os.path.exists(model_path):
            missing_agent_names.append(agent.agent_name)
        else:
            existing_agents.append((agent, model_path))

    # ç¬¬äºŒæ­¥ï¼šåŠ è½½å­˜åœ¨çš„æ¨¡å‹
    for agent, model_path in existing_agents:
        try:
            agent.eval_net.load_state_dict(torch.load(model_path, map_location=device))
            agent.target_net.load_state_dict(agent.eval_net.state_dict())
            print(f"âœ… æˆåŠŸåŠ è½½{agent.agent_name}é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½{agent.agent_name}æ¨¡å‹å¤±è´¥: {e}")

    # ç¬¬ä¸‰æ­¥ï¼šå¤„ç†ç¼ºå¤±çš„æ¨¡å‹ï¼ˆäº¤äº’ç¡®è®¤ + é‡æ–°åˆå§‹åŒ–ï¼‰
    if missing_agent_names:
        print("\nâŒ ä»¥ä¸‹æ™ºèƒ½ä½“æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼š")
        for idx, agent_name in enumerate(missing_agent_names):
            print(f"   {idx+1}. {agent_name}: {model_paths[agent_name]}")
        
        # å‘½ä»¤è¡Œäº¤äº’ç¡®è®¤
        while True:
            user_input = input("\nğŸ“Œ æ˜¯å¦é‡æ–°åˆå§‹åŒ–è¿™äº›ç¼ºå¤±çš„æ™ºèƒ½ä½“ï¼Ÿ(y/n): ").strip().lower()
            if user_input in ['y', 'yes']:
                # é‡æ–°åˆå§‹åŒ–ç¼ºå¤±çš„æ™ºèƒ½ä½“ï¼ˆå®Œå…¨å¤ç”¨åŸæœ‰åˆå§‹åŒ–é€»è¾‘ï¼‰
                for idx, agent in enumerate(agents):
                    if agent.agent_name in missing_agent_names:
                        print(f"\nğŸ”„ é‡æ–°åˆå§‹åŒ–{agent.agent_name}ï¼ˆä»0å¼€å§‹ï¼‰...")
                        # æ ¸å¿ƒï¼šå’ŒåŸæœ‰åˆå§‹åŒ–ä»£ç å®Œå…¨ä¸€è‡´
                        if agent.agent_name == "FC_Agent":
                            new_agent = IndependentDQN(
                                "FC_Agent", N_STATES, N_FC_ACTIONS,
                                shared_memory, memory_counter
                            )
                        elif agent.agent_name == "Bat_Agent":
                            new_agent = IndependentDQN(
                                "Bat_Agent", N_STATES, N_BAT_ACTIONS,
                                shared_memory, memory_counter
                            )
                        elif agent.agent_name == "SC_Agent":
                            new_agent = IndependentDQN(
                                "SC_Agent", N_STATES, N_SC_ACTIONS,
                                shared_memory, memory_counter
                            )
                        # å…³é”®ï¼šåˆå§‹åŒ–ä¼˜åŒ–å™¨
                        new_agent.setup_optimizer(LR, LR_FACTOR, LR_PATIENCE)
                        # æ›¿æ¢åˆ—è¡¨ä¸­çš„å®ä¾‹
                        agents[idx] = new_agent
                        print(f"âœ… {agent.agent_name}å·²é‡æ–°åˆå§‹åŒ–å®Œæˆï¼ˆå«ä¼˜åŒ–å™¨ï¼‰")
                break
            elif user_input in ['n', 'no']:
                # ç»ˆæ­¢è®­ç»ƒ
                print("\nğŸ›‘ ç”¨æˆ·é€‰æ‹©ç»ˆæ­¢è®­ç»ƒï¼Œé€€å‡ºç¨‹åº...")
                sys.exit(0)
            else:
                print("âš ï¸ è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ y/yes æˆ– n/noï¼")

    # æå–ç‹¬ç«‹æ™ºèƒ½ä½“å®ä¾‹è¿”å›
    fc_agent = next(a for a in agents if a.agent_name == "FC_Agent")
    bat_agent = next(a for a in agents if a.agent_name == "Bat_Agent")
    sc_agent = next(a for a in agents if a.agent_name == "SC_Agent")
    
    print("\nğŸ‰ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½/åˆå§‹åŒ–å®Œæˆï¼")
    return fc_agent, bat_agent, sc_agent, agents
# =====================================================================

if __name__ == '__main__':
    # æ‰“å°å‘½ä»¤è¡Œå‚æ•°ï¼ˆä¾¿äºç¡®è®¤é…ç½®ï¼‰
    print("=" * 80)
    print("                    è®­ç»ƒé…ç½®ç¡®è®¤                  ")
    print("=" * 80)
    print(f"è®­ç»ƒæ¨¡å¼: {'ç»§ç»­è®­ç»ƒï¼ˆåŸºäºå·²æœ‰æ¨¡å‹ï¼‰' if RESUME_TRAINING else 'ä»å¤´è®­ç»ƒ'}")
    if RESUME_TRAINING:
        print(f"é¢„è®­ç»ƒæ¨¡å‹é…ç½®:")
        print(f"  - æ—¥æœŸæ–‡ä»¶å¤¹: {PRETRAIN_DATE}")
        print(f"  - Train ID: {PRETRAIN_TRAIN_ID}")
        print(f"  - æ¨¡å‹å‰ç¼€: {PRETRAIN_MODEL_PREFIX}")
    print(f"æ ¸å¿ƒè¶…å‚æ•°:")
    print(f"  - æ‰¹å¤§å°: {BATCH_SIZE}")
    print(f"  - å­¦ä¹ ç‡: {LR:.6f}")
    print(f"  - æ¢ç´¢ç‡: {EPSILON}")
    print(f"  - è®­ç»ƒå›åˆæ•°: {EPISODE}")
    print("=" * 80 + "\n")

    # è·¯å¾„è®¾ç½®
    TARGET_BASE_DIR = os.path.join(project_root, "nets", "Chap3", execute_date)
    os.makedirs(TARGET_BASE_DIR, exist_ok=True)
    train_id = get_max_folder_name(TARGET_BASE_DIR)
    base_path = f"{TARGET_BASE_DIR}/{train_id}"
    os.makedirs(base_path)

    # æ›´æ–°remark
    if RESUME_TRAINING:
        remark = f"RESUME_{execute_date}_{train_id}"
    else:
        remark = f"MARL_{execute_date}_{train_id}"

    # å…±äº«å†…å­˜åˆå§‹åŒ–
    MEMORY_WIDTH = N_STATES * 2 + 4
    shared_memory = np.zeros((MEMORY_CAPACITY, MEMORY_WIDTH))
    memory_counter = [0]

    # åˆå§‹åŒ–æ™ºèƒ½ä½“
    FC_Agent = IndependentDQN(
        "FC_Agent", N_STATES, N_FC_ACTIONS, 
        shared_memory, memory_counter
    )
    Bat_Agent = IndependentDQN(
        "Bat_Agent", N_STATES, N_BAT_ACTIONS, 
        shared_memory, memory_counter
    )
    SC_Agent = IndependentDQN(
        "SC_Agent", N_STATES, N_SC_ACTIONS, 
        shared_memory, memory_counter
    )
    all_agents = [FC_Agent, Bat_Agent, SC_Agent]

    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¿®å¤ï¼šåŒæ­¥å…¨å±€å˜é‡ï¼‰
    if RESUME_TRAINING:
        print("\nğŸ“Œ å¼€å§‹åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        FC_Agent, Bat_Agent, SC_Agent, all_agents = load_pretrained_models(
            all_agents, PRETRAIN_DATE, PRETRAIN_TRAIN_ID, PRETRAIN_MODEL_PREFIX,
            shared_memory, memory_counter
        )

    # è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆé¿å…é‡å¤åˆå§‹åŒ–ï¼‰
    for agent in all_agents:
        if not hasattr(agent, 'optimizer') or agent.optimizer is None:
            agent.setup_optimizer(LR, LR_FACTOR, LR_PATIENCE)

    # éªŒè¯ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆè°ƒè¯•ç”¨ï¼‰
    print("\nğŸ” æ™ºèƒ½ä½“ä¼˜åŒ–å™¨çŠ¶æ€éªŒè¯:")
    print(f"FC_Agent: {'âœ…' if FC_Agent.optimizer else 'âŒ'}")
    print(f"Bat_Agent: {'âœ…' if Bat_Agent.optimizer else 'âŒ'}")
    print(f"SC_Agent: {'âœ…' if SC_Agent.optimizer else 'âŒ'}")

    # è®­ç»ƒè¿‡ç¨‹
    print('\nCollecting experience and learning (I-DQN, 3-Agent)...')
    start_time_total = time.time()
    reward_max = -float('inf')
    reward_not_improve_episodes = 0
    training_done = False
    x, y = [], []
    loss_records = []

    if REAL_TIME_DRAW:
        plt.ion()
        fig, ax = plt.subplots()
        line, = ax.plot(x, y)

    episode_pbar = tqdm(range(EPISODE), desc=f"RL Training")
    for i_episode in episode_pbar:
        if training_done:
            break

        s = env.reset()
        ep_r = 0
        episode_times = {
            'Action_Select': 0.0,
            'Env_Step': 0.0,
            'Data_Store': 0.0,
            'DQN_Learn': 0.0
        }
        step_count = 0
        current_loss = 0.0

        while True:
            # åŠ¨ä½œé€‰æ‹©
            time_start_action = time.time()
            a_fc = FC_Agent.choose_action(s, train=True, epsilon=EPSILON)
            a_bat = Bat_Agent.choose_action(s, train=True, epsilon=EPSILON)
            a_sc = SC_Agent.choose_action(s, train=True, epsilon=EPSILON)
            episode_times['Action_Select'] += (time.time() - time_start_action)

            # ç¯å¢ƒäº¤äº’
            action_list = [a_fc, a_bat, a_sc]
            time_start_step = time.time()
            s_, r, done, _ = env.step(action_list)
            episode_times['Env_Step'] += (time.time() - time_start_step)

            # å­˜å‚¨è½¬æ¢
            time_start_store = time.time()
            transition = np.hstack((s, a_fc, a_bat, a_sc, r, s_))
            index = memory_counter[0] % MEMORY_CAPACITY
            if transition.shape[0] != MEMORY_WIDTH:
                raise RuntimeError(f"å­˜å‚¨è½¬æ¢é•¿åº¦é”™è¯¯: æœŸæœ› {MEMORY_WIDTH}, å®é™… {transition.shape[0]}")
            shared_memory[index, :] = transition
            memory_counter[0] += 1
            episode_times['Data_Store'] += (time.time() - time_start_store)

            ep_r += r
            step_count += 1

            # å­¦ä¹ è¿‡ç¨‹
            if memory_counter[0] > MEMORY_CAPACITY and memory_counter[0] % LEARN_FREQUENCY == 0:
                time_start_learn = time.time()
                fc_loss = FC_Agent.learn(0, N_STATES, GAMMA, TARGET_REPLACE_ITER, BATCH_SIZE) or 0.0
                bat_loss = Bat_Agent.learn(1, N_STATES, GAMMA, TARGET_REPLACE_ITER, BATCH_SIZE) or 0.0
                # ä¸å­¦ä¹ ç‡ƒæ–™ç”µæ± é”‚ç”µæ± 
                # fc_loss =  0.0
                # bat_loss =  0.0
                sc_loss = SC_Agent.learn(2, N_STATES, GAMMA, TARGET_REPLACE_ITER, BATCH_SIZE) or 0.0
                current_loss = (fc_loss + bat_loss + sc_loss) / 3.0
                episode_times['DQN_Learn'] += (time.time() - time_start_learn)

            if done:
                writer.add_scalar("Ep_r/Ep", ep_r, i_episode)
                using_time_total = time.time() - start_time_total
                current_lr = FC_Agent.optimizer.param_groups[0]["lr"]
                episode_pbar.set_postfix({
                    'Ep_r': f'{ep_r:.2f}',
                    'LR': f'{current_lr:.2e}',
                    'Total_Time': f'{using_time_total:.2f}s',
                    'Loss': f'{current_loss:.4f}'
                })

                loss_records.append(current_loss)

                if i_episode < 2 or (i_episode + 1) % 500 == 0:
                    print_time_breakdown(i_episode + 1, episode_times)
                break

            s = s_

        x.append(i_episode)
        y.append(ep_r)

        # æ¨¡å‹ä¿å­˜ä¸æ—©åœé€»è¾‘
        if ep_r > reward_max + REWARD_THRESHOLD:
            reward_max = ep_r
            reward_not_improve_episodes = 0
            torch.save(FC_Agent.eval_net.state_dict(), f"{base_path}/{best_model_base_name}_FC.pth")
            torch.save(Bat_Agent.eval_net.state_dict(), f"{base_path}/{best_model_base_name}_BAT.pth")
            torch.save(SC_Agent.eval_net.state_dict(), f"{base_path}/{best_model_base_name}_SC.pth")
            print(f"\n--- New Max Reward: {reward_max:.2f} ---")
        else:
            reward_not_improve_episodes += 1

        # å­¦ä¹ ç‡è°ƒåº¦
        for agent in all_agents:
            agent.scheduler.step(ep_r)

        # æ—©åœæ£€æŸ¥
        if reward_not_improve_episodes >= EARLY_STOP_PATIENCE:
            print(f"\n--- Early Stopping Triggered! ---")
            training_done = True

    # æœ€ç»ˆå¤„ç†
    final_episode = i_episode + 1 if not training_done else i_episode
    final_model_name = f"{base_path}/{best_model_base_name}_FINAL"
    torch.save(FC_Agent.eval_net.state_dict(), f"{final_model_name}_FC.pth")
    torch.save(Bat_Agent.eval_net.state_dict(), f"{final_model_name}_BAT.pth")
    torch.save(SC_Agent.eval_net.state_dict(), f"{final_model_name}_SC.pth")
    print(f"\nFinal models saved: {final_model_name}")

    # æ•´ç†è®­ç»ƒæœ€ç»ˆæŒ‡æ ‡
    final_metrics = {
        "max_reward": round(reward_max, 4),
        "final_reward": round(y[-1], 4) if y else 0,
        "average_reward": round(np.mean(y[POOL_SIZE:]) if len(y) > POOL_SIZE else 0, 4),
        "total_episodes_completed": final_episode,
        "early_stopped": training_done,
        "final_learning_rate": round(FC_Agent.optimizer.param_groups[0]["lr"], 6),
        "reward_not_improve_episodes": reward_not_improve_episodes,
        "best_model_reward": round(reward_max, 4),
        "excluded_episodes": POOL_SIZE
    }

    # ä¿å­˜è¶…å‚æ•°
    save_hyperparameters(base_path, final_metrics)

    # ä¿å­˜è®­ç»ƒè®°å½•åˆ°CSV
    csv_path = os.path.join(base_path, "training_records.csv")
    with open(csv_path, 'w', encoding='utf-8') as f:
        f.write("episode,reward,loss\n")
        for ep, r, l in zip(x, y, loss_records):
            f.write(f"{ep},{r:.4f},{l:.4f}\n")
    print(f"âœ… è®­ç»ƒè®°å½•ï¼ˆå«lossï¼‰å·²ä¿å­˜åˆ°CSV: {csv_path}")

    # å¯è§†åŒ–ä¸ä¿å­˜
    writer.flush()
    writer.close()
    plt.figure()
    x_filtered = x[POOL_SIZE:]
    y_filtered = y[POOL_SIZE:]
    plt.plot(x_filtered, y_filtered)
    plt.xlabel('Episode')
    plt.ylabel('Episode Reward')
    plt.title(f'Training Curve (MARL_IQL, Ep={final_episode}, Exclude First {POOL_SIZE} Episodes)')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.savefig(f"{base_path}/train_curve_MARL_Model.svg")
    if REAL_TIME_DRAW:
        plt.ioff()
        plt.show()

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {base_path}")
    if best_model_base_name:
        print(f"\nğŸ“‹ æœ€ä¼˜æ¨¡å‹æ–‡ä»¶åå‰ç¼€ï¼š{best_model_base_name}")

    # æ‰§è¡Œæµ‹è¯•
    test_script_path = os.path.join(project_root, "Scripts", "Chapter3", "test.py")
    test_cmd = [
        str(sys.executable),
        str(test_script_path),
        "--net-date", str(execute_date),
        "--train-id", str(train_id),
        "--model-prefix", str(best_model_base_name)
    ]
    print("\nğŸš€ å¼€å§‹æ‰§è¡Œæµ‹è¯•è„šæœ¬...")
    print(test_cmd)
    subprocess.run(test_cmd, check=True)

    print(f"\nğŸ‰ æ‰€æœ‰æµç¨‹å®Œæˆï¼æ–‡ä»¶ä¿å­˜è·¯å¾„: {base_path}")