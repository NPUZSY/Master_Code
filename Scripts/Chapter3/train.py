import os
import time
import json
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import torch

# å¯¼å…¥å…¬å…±æ¨¡å—
from MARL_Engine import (
    setup_project_root, device, 
    IndependentDQN, get_max_folder_name,
    font_get
)
project_root = setup_project_root()
from Scripts.Env import Envs

# è·å–æ–°ç½—é©¬
font_get()

# å…¨å±€è®¾ç½®ä¸è¶…å‚æ•°
env = Envs()
writer = SummaryWriter()
torch.set_default_dtype(torch.float32)

# è¶…å‚æ•°
BATCH_SIZE = 32
LR = 0.002
EPSILON = 0.9
GAMMA = 0.9
TARGET_REPLACE_ITER = 100
POOL_SIZE = 100
EPISODE = 1000
LEARN_FREQUENCY = 10
REAL_TIME_DRAW = False

# å­¦ä¹ ç‡è°ƒåº¦ä¸æ—©åœå‚æ•°
LR_PATIENCE = 50
LR_FACTOR = 0.5
EARLY_STOP_PATIENCE = 100
REWARD_THRESHOLD = 0.001

# ç¯å¢ƒå‚æ•°
N_STATES = env.observation_space.shape[0]
N_TOTAL_ACTIONS = env.N_ACTIONS
N_FC_ACTIONS = 32
N_BAT_ACTIONS = 20
N_SC_ACTIONS = 2

# å†…å­˜é…ç½®
MEMORY_CAPACITY = env.step_length * POOL_SIZE
current_timestamp = time.time()
local_time = time.localtime(current_timestamp)
execute_date = time.strftime("%m%d", local_time)
execute_time = time.strftime("%H%M%S", local_time)  # æ–°å¢ï¼šè®°å½•å…·ä½“æ—¶é—´
remark = "MARL_IQL_32x20x2"

# æ–°å¢ï¼šå…¨å±€å˜é‡å­˜å‚¨æœ€ä¼˜æ¨¡å‹æ–‡ä»¶å
best_model_base_name = ""

# éªŒè¯åŠ¨ä½œåˆ†è§£
N_EXPECTED_ACTIONS = N_FC_ACTIONS * N_BAT_ACTIONS * N_SC_ACTIONS
if N_EXPECTED_ACTIONS != N_TOTAL_ACTIONS:
    print(f"è­¦å‘Šï¼šåŠ¨ä½œåˆ†è§£ {N_EXPECTED_ACTIONS} ä¸ç¯å¢ƒ N_TOTAL_ACTIONS({N_TOTAL_ACTIONS}) ä¸åŒ¹é…")

# æ–°å¢ï¼šå®šä¹‰ä¿å­˜è¶…å‚æ•°çš„å‡½æ•°
def save_hyperparameters(save_path, final_metrics=None):
    """
    ä¿å­˜è¶…å‚æ•°åˆ°æŒ‡å®šè·¯å¾„ï¼ˆtxtå’Œjsonæ ¼å¼ï¼‰
    :param save_path: ä¿å­˜ç›®å½•
    :param final_metrics: è®­ç»ƒæœ€ç»ˆæŒ‡æ ‡ï¼ˆå¦‚æœ€å¤§å¥–åŠ±ã€æœ€ç»ˆå¥–åŠ±ç­‰ï¼‰
    """
    # æ•´ç†è¶…å‚æ•°å­—å…¸
    hyperparams = {
        # åŸºç¡€ä¿¡æ¯
        "train_info": {
            "execute_date": execute_date,
            "execute_time": execute_time,
            "train_id": os.path.basename(save_path),
            "remark": remark,
            "device": str(device),
            "total_training_time_s": round(time.time() - start_time_total, 2) if 'start_time_total' in globals() else 0,
            "best_model_base_name": best_model_base_name,  # æ–°å¢ï¼šæœ€ä¼˜æ¨¡å‹æ–‡ä»¶åå‰ç¼€
            "best_model_full_path": os.path.join(save_path, best_model_base_name) if best_model_base_name else ""  # æ–°å¢ï¼šæœ€ä¼˜æ¨¡å‹å®Œæ•´è·¯å¾„
        },
        # æ ¸å¿ƒè¶…å‚æ•°
        "core_hyperparams": {
            "BATCH_SIZE": BATCH_SIZE,
            "LR": LR,
            "EPSILON": EPSILON,
            "GAMMA": GAMMA,
            "TARGET_REPLACE_ITER": TARGET_REPLACE_ITER,
            "POOL_SIZE": POOL_SIZE,
            "EPISODE": EPISODE,
            "LEARN_FREQUENCY": LEARN_FREQUENCY,
            "MEMORY_CAPACITY": MEMORY_CAPACITY,
            "REAL_TIME_DRAW": REAL_TIME_DRAW
        },
        # å­¦ä¹ ç‡è°ƒåº¦ä¸æ—©åœå‚æ•°
        "lr_earlystop_params": {
            "LR_PATIENCE": LR_PATIENCE,
            "LR_FACTOR": LR_FACTOR,
            "EARLY_STOP_PATIENCE": EARLY_STOP_PATIENCE,
            "REWARD_THRESHOLD": REWARD_THRESHOLD
        },
        # ç¯å¢ƒå‚æ•°
        "env_params": {
            "N_STATES": N_STATES,
            "N_TOTAL_ACTIONS": N_TOTAL_ACTIONS,
            "N_FC_ACTIONS": N_FC_ACTIONS,
            "N_BAT_ACTIONS": N_BAT_ACTIONS,
            "N_SC_ACTIONS": N_SC_ACTIONS,
            "N_EXPECTED_ACTIONS": N_EXPECTED_ACTIONS,
            "step_length": env.step_length if hasattr(env, 'step_length') else "unknown"
        },
        # è®­ç»ƒç»“æœæŒ‡æ ‡
        "training_metrics": final_metrics or {}
    }

    # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆä¾¿äºç¨‹åºè§£æï¼‰
    json_path = os.path.join(save_path, "hyperparameters.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(hyperparams, f, indent=4, ensure_ascii=False)

    # ä¿å­˜ä¸ºTXTæ ¼å¼ï¼ˆä¾¿äºäººå·¥é˜…è¯»ï¼‰
    txt_path = os.path.join(save_path, "hyperparameters.txt")
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("                è®­ç»ƒè¶…å‚æ•°æ±‡æ€»                \n")
        f.write("=" * 80 + "\n\n")
        
        for section, params in hyperparams.items():
            f.write(f"ã€{section.upper()}ã€‘\n")
            f.write("-" * 60 + "\n")
            for key, value in params.items():
                # å¯¹æœ€ä¼˜æ¨¡å‹åç§°å•ç‹¬é«˜äº®æ˜¾ç¤º
                if key in ["best_model_base_name", "best_model_full_path"]:
                    f.write(f"{key:<30}: \033[1;32m{value}\033[0m\n")  # ç»¿è‰²é«˜äº®
                else:
                    f.write(f"{key:<30}: {value}\n")
            f.write("\n")

    # å•ç‹¬æ‰“å°æœ€ä¼˜æ¨¡å‹åç§°ï¼ˆæ–¹ä¾¿ç›´æ¥å¤åˆ¶ï¼‰
    if best_model_base_name:
        print(f"\nğŸ¯ æœ€ä¼˜æ¨¡å‹æ–‡ä»¶åå‰ç¼€ï¼ˆå¯ç›´æ¥å¤åˆ¶ï¼‰ï¼š")
        print(f"   {best_model_base_name}")
    print(f"\nâœ… è¶…å‚æ•°å·²ä¿å­˜åˆ°ï¼š")
    print(f"   JSONæ ¼å¼: {json_path}")
    print(f"   TXTæ ¼å¼: {txt_path}")

# æ—¶é—´åˆ†è§£æ‰“å°å‡½æ•°
def print_time_breakdown(episode, episode_times):
    total_time = sum(episode_times.values())
    if total_time < 1e-6:
        print(f"å›åˆ {episode} è€—æ—¶è¿‡çŸ­ï¼Œè·³è¿‡è€—æ—¶åˆ†æã€‚")
        return

    print("\n" + "=" * 45)
    print(f"ğŸš€ å›åˆ {episode} è€—æ—¶åˆ†è§£ (æ€»è€—æ—¶: {total_time:.4f} s)")
    print("-" * 45)
    for name, time_val in sorted(episode_times.items(), key=lambda x: x[1], reverse=True):
        percentage = (time_val / total_time) * 100
        print(f"| {name.ljust(15)} | {time_val:9.4f} s | {percentage:6.2f} % |")
    print("=" * 45)

if __name__ == '__main__':
    # è·¯å¾„è®¾ç½®
    TARGET_BASE_DIR = os.path.join(project_root, "nets", "Chap3", execute_date)
    os.makedirs(TARGET_BASE_DIR, exist_ok=True)
    train_id = get_max_folder_name(TARGET_BASE_DIR)
    base_path = f"{TARGET_BASE_DIR}/{train_id}"
    os.makedirs(base_path)

    # å…±äº«å†…å­˜åˆå§‹åŒ–
    MEMORY_WIDTH = N_STATES * 2 + 4
    shared_memory = np.zeros((MEMORY_CAPACITY, MEMORY_WIDTH))
    memory_counter = [0]

    # åˆå§‹åŒ–æ™ºèƒ½ä½“
    FC_Agent = IndependentDQN(
        "FC_Agent", N_STATES, N_FC_ACTIONS, 
        shared_memory, memory_counter
    )
    Bat_Agent = IndependentDQN(
        "Bat_Agent", N_STATES, N_BAT_ACTIONS, 
        shared_memory, memory_counter
    )
    SC_Agent = IndependentDQN(
        "SC_Agent", N_STATES, N_SC_ACTIONS, 
        shared_memory, memory_counter
    )
    all_agents = [FC_Agent, Bat_Agent, SC_Agent]

    # è®¾ç½®ä¼˜åŒ–å™¨
    for agent in all_agents:
        agent.setup_optimizer(LR, LR_FACTOR, LR_PATIENCE)

    # è®­ç»ƒè¿‡ç¨‹
    print('\nCollecting experience and learning (I-DQN, 3-Agent)...')
    start_time_total = time.time()
    reward_max = -float('inf')
    reward_not_improve_episodes = 0
    training_done = False
    x, y = [], []

    if REAL_TIME_DRAW:
        plt.ion()
        fig, ax = plt.subplots()
        line, = ax.plot(x, y)

    episode_pbar = tqdm(range(EPISODE), desc=f"RL Training ({remark})")
    for i_episode in episode_pbar:
        if training_done:
            break

        s = env.reset()
        ep_r = 0
        episode_times = {
            'Action_Select': 0.0,
            'Env_Step': 0.0,
            'Data_Store': 0.0,
            'DQN_Learn': 0.0
        }
        step_count = 0

        while True:
            # åŠ¨ä½œé€‰æ‹©
            time_start_action = time.time()
            a_fc = FC_Agent.choose_action(s, train=True, epsilon=EPSILON)
            a_bat = Bat_Agent.choose_action(s, train=True, epsilon=EPSILON)
            a_sc = SC_Agent.choose_action(s, train=True, epsilon=EPSILON)
            episode_times['Action_Select'] += (time.time() - time_start_action)

            # ç¯å¢ƒäº¤äº’
            action_list = [a_fc, a_bat, a_sc]
            time_start_step = time.time()
            s_, r, done, _ = env.step(action_list)
            episode_times['Env_Step'] += (time.time() - time_start_step)

            # å­˜å‚¨è½¬æ¢
            time_start_store = time.time()
            transition = np.hstack((s, a_fc, a_bat, a_sc, r, s_))
            index = memory_counter[0] % MEMORY_CAPACITY
            if transition.shape[0] != MEMORY_WIDTH:
                raise RuntimeError(f"å­˜å‚¨è½¬æ¢é•¿åº¦é”™è¯¯: æœŸæœ› {MEMORY_WIDTH}, å®é™… {transition.shape[0]}")
            shared_memory[index, :] = transition
            memory_counter[0] += 1
            episode_times['Data_Store'] += (time.time() - time_start_store)

            ep_r += r
            step_count += 1

            # å­¦ä¹ è¿‡ç¨‹
            if memory_counter[0] > MEMORY_CAPACITY and memory_counter[0] % LEARN_FREQUENCY == 0:
                time_start_learn = time.time()
                FC_Agent.learn(0, N_STATES, GAMMA, TARGET_REPLACE_ITER, BATCH_SIZE)
                Bat_Agent.learn(1, N_STATES, GAMMA, TARGET_REPLACE_ITER, BATCH_SIZE)
                SC_Agent.learn(2, N_STATES, GAMMA, TARGET_REPLACE_ITER, BATCH_SIZE)
                episode_times['DQN_Learn'] += (time.time() - time_start_learn)

            if done:
                writer.add_scalar("Ep_r/Ep", ep_r, i_episode)
                using_time_total = time.time() - start_time_total
                current_lr = FC_Agent.optimizer.param_groups[0]["lr"]
                episode_pbar.set_postfix({
                    'Ep_r': f'{ep_r:.2f}',
                    'LR': f'{current_lr:.2e}',
                    'Total_Time': f'{using_time_total:.2f}s',
                })

                if i_episode < 2 or (i_episode + 1) % 100 == 0:
                    print_time_breakdown(i_episode + 1, episode_times)
                break

            s = s_

        x.append(i_episode)
        y.append(ep_r)

        # æ¨¡å‹ä¿å­˜ä¸æ—©åœé€»è¾‘
        if ep_r > reward_max + REWARD_THRESHOLD:
            reward_max = ep_r
            reward_not_improve_episodes = 0
            best_model_base_name = (f"bs{BATCH_SIZE}_lr{int(LR*10000)}_ep_{i_episode+1}"  # æ–°å¢ï¼šæ›´æ–°å…¨å±€æœ€ä¼˜æ¨¡å‹åç§°
                                   f"_pool{POOL_SIZE}_freq{LEARN_FREQUENCY}_MARL_{remark}_MAX_R{int(reward_max)}")
            net_name_base = best_model_base_name  # ä¿æŒåŸæœ‰é€»è¾‘
            torch.save(FC_Agent.eval_net.state_dict(), f"{base_path}/{net_name_base}_FC.pth")
            torch.save(Bat_Agent.eval_net.state_dict(), f"{base_path}/{net_name_base}_BAT.pth")
            torch.save(SC_Agent.eval_net.state_dict(), f"{base_path}/{net_name_base}_SC.pth")
            print(f"\n--- New Max Reward: {reward_max:.2f} | Models saved: {net_name_base} ---")
        else:
            reward_not_improve_episodes += 1

        # å­¦ä¹ ç‡è°ƒåº¦
        for agent in all_agents:
            agent.scheduler.step(ep_r)

        # æ—©åœæ£€æŸ¥
        if reward_not_improve_episodes >= EARLY_STOP_PATIENCE:
            print(f"\n--- Early Stopping Triggered! ---")
            training_done = True

    # æœ€ç»ˆå¤„ç†
    final_episode = i_episode + 1 if not training_done else i_episode
    final_net_name_base = (f"{base_path}/final_bs{BATCH_SIZE}_lr{int(LR*10000)}_ep_{final_episode}_pool{POOL_SIZE}"
                         f"_freq{LEARN_FREQUENCY}_MARL_{remark}_FINAL")
    torch.save(FC_Agent.eval_net.state_dict(), f"{final_net_name_base}_FC.pth")
    torch.save(Bat_Agent.eval_net.state_dict(), f"{final_net_name_base}_BAT.pth")
    torch.save(SC_Agent.eval_net.state_dict(), f"{final_net_name_base}_SC.pth")
    print(f"\nFinal models saved: {final_net_name_base}")

    # æ–°å¢ï¼šæ•´ç†è®­ç»ƒæœ€ç»ˆæŒ‡æ ‡
    final_metrics = {
        "max_reward": round(reward_max, 4),
        "final_reward": round(y[-1], 4) if y else 0,
        "average_reward": round(np.mean(y) if y else 0, 4),
        "total_episodes_completed": final_episode,
        "early_stopped": training_done,
        "final_learning_rate": round(FC_Agent.optimizer.param_groups[0]["lr"], 6),
        "reward_not_improve_episodes": reward_not_improve_episodes,
        "best_model_reward": round(reward_max, 4)  # æ–°å¢ï¼šæœ€ä¼˜æ¨¡å‹å¯¹åº”çš„å¥–åŠ±
    }

    # æ–°å¢ï¼šä¿å­˜è¶…å‚æ•°
    save_hyperparameters(base_path, final_metrics)

    # å¯è§†åŒ–ä¸ä¿å­˜
    writer.flush()
    writer.close()
    plt.figure()
    plt.plot(x, y)
    plt.xlabel('Episode')
    plt.ylabel('Episode Reward')
    plt.title(f'Training Curve (MARL_IQL, Ep={final_episode})')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.savefig(f"{base_path}/train_curve_MARL_IQL_bs{BATCH_SIZE}_lr{int(LR*10000)}_ep{final_episode}.svg", 
                bbox_inches='tight', dpi=300)
    if REAL_TIME_DRAW:
        plt.ioff()
        plt.show()

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {base_path}")
    # æœ€ç»ˆå†æ¬¡æ‰“å°æœ€ä¼˜æ¨¡å‹åç§°ï¼ˆæ–¹ä¾¿å¤åˆ¶ï¼‰
    if best_model_base_name:
        print(f"\nğŸ“‹ æœ€ä¼˜æ¨¡å‹æ–‡ä»¶åå‰ç¼€ï¼ˆç›´æ¥å¤åˆ¶å³å¯ï¼‰ï¼š")
        print(f"{best_model_base_name}")