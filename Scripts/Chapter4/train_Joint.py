import os
import time
import json
import subprocess
import sys
import argparse
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import torch

def setup_path():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(script_dir, '..', '..'))
    if project_root not in sys.path:
        sys.path.append(project_root)
    return project_root

project_root = setup_path()

# ----------------------------------------------------
# 1. ç¯å¢ƒä¸è·¯å¾„é…ç½®
# ----------------------------------------------------
from Scripts.Chapter3.MARL_Engine import (
    setup_project_root, device, 
    get_max_folder_name
)
project_root = setup_project_root()
from Scripts.Env import Envs
from Scripts.utils.global_utils import *

# ç¡®ä¿ Joint_Net å®šä¹‰æ­£ç¡®
from Scripts.Chapter4.Joint_Net import MultiTaskRNN, JointNet, JointDQN

font_get()

"""
ç¤ºä¾‹è®­ç»ƒè„šæœ¬

ä»å¤´è®­ç»ƒ

nohup python Scripts/Chapter4/train_Joint.py \
--episode 5000 \
--pool-size 200 > logs/1222_5.log 2>&1 &

ç»§ç»­è®­ç»ƒ

nohup python Scripts/Chapter4/train_Joint.py \
--resume-training \
--pretrain-date 1223 \
--pretrain-train-id 0 \
--epsilon 0.9 \
--lr 1e-5 \
--pretrain-model-prefix "Joint_Model" \
--episode 2000 > logs/1223_3.log 2>&1 &

"""

# ====================== æ–°å¢ï¼šå‘½ä»¤è¡Œå‚æ•°è§£æï¼ˆå¯¹é½train.pyï¼‰ ======================
def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆæ”¯æŒä»å¤´è®­ç»ƒ/ç»§ç»­è®­ç»ƒï¼‰"""
    parser = argparse.ArgumentParser(description='JointNetè®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒä»å¤´è®­ç»ƒ/ç»§ç»­è®­ç»ƒï¼‰')
    
    # æ ¸å¿ƒè®­ç»ƒæ¨¡å¼å‚æ•°
    parser.add_argument('--resume-training', action='store_true', 
                        help='æ˜¯å¦åŸºäºå·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆé»˜è®¤ï¼šä»å¤´è®­ç»ƒï¼‰')
    parser.add_argument('--pretrain-date', type=str, default="1219",
                        help='é¢„è®­ç»ƒæ¨¡å‹çš„æ—¥æœŸæ–‡ä»¶å¤¹ï¼ˆä»…resume-training=Trueæ—¶ç”Ÿæ•ˆï¼‰')
    parser.add_argument('--pretrain-train-id', type=str, default="1",
                        help='é¢„è®­ç»ƒæ¨¡å‹çš„train_idï¼ˆä»…resume-training=Trueæ—¶ç”Ÿæ•ˆï¼‰')
    parser.add_argument('--pretrain-model-prefix', type=str, 
                        default="Joint_Model",
                        help='é¢„è®­ç»ƒæ¨¡å‹å‰ç¼€ï¼ˆä»…resume-training=Trueæ—¶ç”Ÿæ•ˆï¼‰')
    
    # ç»§ç»­è®­ç»ƒç¤ºä¾‹ä»£ç ï¼š--resume-training --pretrain-date 1219 --pretrain-train-id 5

    # è®­ç»ƒè¶…å‚æ•°ï¼ˆå¯é€‰ï¼Œæ”¯æŒå‘½ä»¤è¡Œè¦†ç›–é»˜è®¤å€¼ï¼‰
    parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹å¤§å°ï¼ˆé»˜è®¤ï¼š32ï¼‰')
    parser.add_argument('--lr', type=float, default=1e-5, help='å­¦ä¹ ç‡ï¼ˆé»˜è®¤ï¼š1e-5ï¼‰')
    parser.add_argument('--epsilon', type=float, default=0.9, help='æ¢ç´¢ç‡ï¼ˆé»˜è®¤ï¼š0.9ï¼‰')
    parser.add_argument('--gamma', type=float, default=0.95, help='æŠ˜æ‰£å› å­ï¼ˆé»˜è®¤ï¼š0.95ï¼‰')
    parser.add_argument('--pool-size', type=int, default=100, help='æ± å¤§å°ï¼ˆé»˜è®¤ï¼š20ï¼‰')
    parser.add_argument('--episode', type=int, default=2000, help='è®­ç»ƒå›åˆæ•°ï¼ˆé»˜è®¤ï¼š2000ï¼‰')
    parser.add_argument('--learn-frequency', type=int, default=5, help='å­¦ä¹ é¢‘ç‡ï¼ˆé»˜è®¤ï¼š5ï¼‰')
    parser.add_argument('--remark', type=str, default="", help='å¤‡æ³¨')
    
    # è·¯å¾„å‚æ•°ï¼ˆå¯é€‰ï¼‰
    parser.add_argument('--log-dir', type=str, default=None, help='TensorBoardæ—¥å¿—ç›®å½•ï¼ˆé»˜è®¤ï¼šè‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--init-rnn-path', type=str, 
                        default=os.path.join(project_root, "nets/Chap4/RNN_Reg_Opt_MultiTask/1216/17/rnn_classifier_multitask.pth"),
                        help='ä»å¤´è®­ç»ƒæ—¶åˆå§‹åŒ–RNNçš„è·¯å¾„ï¼ˆresume-training=Trueæ—¶æ— æ•ˆï¼‰')
    
    return parser.parse_args()

args = parse_args()

# ====================== å…¨å±€é…ç½®ï¼ˆå¯¹é½train.pyï¼‰ ======================
try:
    env = Envs()
except Exception as e:
    print(f"âŒ ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
    sys.exit()

# åŠ¨æ€é…ç½®è¶…å‚æ•°ï¼ˆä»å‘½ä»¤è¡Œå‚æ•°è¯»å–ï¼‰
BATCH_SIZE = args.batch_size
LR = args.lr
EPSILON = args.epsilon
GAMMA = args.gamma
TARGET_REPLACE_ITER = 100
POOL_SIZE = args.pool_size
EPISODE = args.episode
LEARN_FREQUENCY = args.learn_frequency
REAL_TIME_DRAW = False

# ç»§ç»­è®­ç»ƒé…ç½®
RESUME_TRAINING = args.resume_training
PRETRAIN_DATE = args.pretrain_date
PRETRAIN_TRAIN_ID = args.pretrain_train_id
PRETRAIN_MODEL_PREFIX = args.pretrain_model_prefix
GLOBAL_SEED = 42

# å­¦ä¹ ç‡è°ƒåº¦ä¸æ—©åœå‚æ•°
LR_PATIENCE = 100
LR_FACTOR = 0.5
EARLY_STOP_PATIENCE = 1000
REWARD_THRESHOLD = 0.001

torch.set_default_dtype(torch.float32)
torch.manual_seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)

N_STATES = 7  # JointNetå›ºå®š7ç»´è¾“å…¥
# ç¡®ä¿ MEMORY_CAPACITY è‡³å°‘å¤§äº batch_size
MEMORY_CAPACITY = max(env.step_length * POOL_SIZE, BATCH_SIZE * 2)
execute_date = time.strftime("%m%d", time.localtime())
execute_time = time.strftime("%H%M%S", time.localtime())  # æ–°å¢ï¼šè®°å½•å…·ä½“æ—¶é—´

# å…¨å±€å˜é‡å­˜å‚¨æœ€ä¼˜æ¨¡å‹æ–‡ä»¶å
best_model_base_name = "Joint_Model"
remark = args.remark  # åˆå§‹åŒ–remark

# ====================== æ–°å¢ï¼šä¿å­˜è¶…å‚æ•°å‡½æ•°ï¼ˆå¯¹é½train.pyï¼‰ ======================
def save_hyperparameters(save_path, final_metrics=None):
    """
    ä¿å­˜è¶…å‚æ•°åˆ°æŒ‡å®šè·¯å¾„ï¼ˆtxtå’Œjsonæ ¼å¼ï¼‰
    :param save_path: ä¿å­˜ç›®å½•
    :param final_metrics: è®­ç»ƒæœ€ç»ˆæŒ‡æ ‡ï¼ˆå¦‚æœ€å¤§å¥–åŠ±ã€æœ€ç»ˆå¥–åŠ±ç­‰ï¼‰
    """
    # æ•´ç†è¶…å‚æ•°å­—å…¸
    hyperparams = {
        # åŸºç¡€ä¿¡æ¯
        "train_info": {
            "execute_date": execute_date,
            "execute_time": execute_time,
            "train_id": os.path.basename(save_path),
            "remark": remark,
            "device": str(device),
            "seed": GLOBAL_SEED,
            "total_training_time_s": round(time.time() - start_time_total, 2) if 'start_time_total' in globals() else 0,
            "best_model_base_name": best_model_base_name,
            "best_model_full_path": os.path.join(save_path, best_model_base_name) if best_model_base_name else "",
            "resume_training": RESUME_TRAINING,
            "command_line_args": vars(args),
            "pretrain_model_info": {
                "pretrain_date": PRETRAIN_DATE if RESUME_TRAINING else "",
                "pretrain_train_id": PRETRAIN_TRAIN_ID if RESUME_TRAINING else "",
                "pretrain_model_prefix": PRETRAIN_MODEL_PREFIX if RESUME_TRAINING else "",
                "init_rnn_path": args.init_rnn_path if not RESUME_TRAINING else "NOT_USED"
            }
        },
        # æ ¸å¿ƒè¶…å‚æ•°
        "core_hyperparams": {
            "BATCH_SIZE": BATCH_SIZE,
            "LR": LR,
            "EPSILON": EPSILON,
            "GAMMA": GAMMA,
            "TARGET_REPLACE_ITER": TARGET_REPLACE_ITER,
            "POOL_SIZE": POOL_SIZE,
            "EPISODE": EPISODE,
            "LEARN_FREQUENCY": LEARN_FREQUENCY,
            "MEMORY_CAPACITY": MEMORY_CAPACITY,
            "REAL_TIME_DRAW": REAL_TIME_DRAW
        },
        # å­¦ä¹ ç‡è°ƒåº¦ä¸æ—©åœå‚æ•°
        "lr_earlystop_params": {
            "LR_PATIENCE": LR_PATIENCE,
            "LR_FACTOR": LR_FACTOR,
            "EARLY_STOP_PATIENCE": EARLY_STOP_PATIENCE,
            "REWARD_THRESHOLD": REWARD_THRESHOLD
        },
        # ç¯å¢ƒå‚æ•°
        "env_params": {
            "N_STATES": N_STATES,
            "MEMORY_WIDTH": MEMORY_WIDTH if 'MEMORY_WIDTH' in globals() else 0,
            "step_length": env.step_length if hasattr(env, 'step_length') else "unknown"
        },
        # è®­ç»ƒç»“æœæŒ‡æ ‡
        "training_metrics": final_metrics or {}
    }

    # ä¿å­˜ä¸ºJSONæ ¼å¼
    json_path = os.path.join(save_path, "hyperparameters.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(hyperparams, f, indent=4, ensure_ascii=False)

    # ä¿å­˜ä¸ºTXTæ ¼å¼
    txt_path = os.path.join(save_path, "hyperparameters.txt")
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("                JointNetè®­ç»ƒè¶…å‚æ•°æ±‡æ€»                \n")
        f.write("=" * 80 + "\n\n")
        
        for section, params in hyperparams.items():
            f.write(f"ã€{section.upper()}ã€‘\n")
            f.write("-" * 60 + "\n")
            for key, value in params.items():
                if key in ["best_model_base_name", "best_model_full_path", "resume_training", "init_rnn_path"]:
                    f.write(f"{key:<30}: \033[1;32m{value}\033[0m\n")
                else:
                    f.write(f"{key:<30}: {value}\n")
            f.write("\n")

    print(f"\nâœ… è¶…å‚æ•°å·²ä¿å­˜åˆ°ï¼š")
    print(f"   JSONæ ¼å¼: {json_path}")
    print(f"   TXTæ ¼å¼: {txt_path}")

# ====================== æ—¶é—´åˆ†è§£æ‰“å°å‡½æ•°ï¼ˆå¯¹é½train.pyï¼‰ ======================
def print_time_breakdown(episode, episode_times):
    total_time = sum(episode_times.values())
    if total_time < 1e-6:
        print(f"å›åˆ {episode} è€—æ—¶è¿‡çŸ­ï¼Œè·³è¿‡è€—æ—¶åˆ†æã€‚")
        return

    print("\n" + "=" * 45)
    print(f"ğŸš€ å›åˆ {episode} è€—æ—¶åˆ†è§£ (æ€»è€—æ—¶: {total_time:.4f} s)")
    print("-" * 45)
    for name, time_val in sorted(episode_times.items(), key=lambda x: x[1], reverse=True):
        percentage = (time_val / total_time) * 100
        print(f"| {name.ljust(15)} | {time_val:9.4f} s | {percentage:6.2f} % |")
    print("=" * 45)

# ====================== åŠ è½½å®Œæ•´Jointæ¨¡å‹å‡½æ•°ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰ ======================
def load_full_joint_agents(pretrain_date, pretrain_id, prefix):
    """åŠ è½½åŒ…å«RNNçš„å®Œæ•´JointNetæ™ºèƒ½ä½“"""
    pretrain_dir = os.path.join(project_root, "nets", "Chap4", "Joint_Net", pretrain_date, pretrain_id)
    agents = {}
    names = ["FC", "BAT", "SC"]
    rnn_model = None
    
    if not os.path.exists(pretrain_dir):
        raise FileNotFoundError(f"é¢„è®­ç»ƒç›®å½•ä¸å­˜åœ¨: {pretrain_dir}")

    # ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
    missing_agent_names = []
    existing_paths = {}
    
    for name in names:
        path = os.path.join(pretrain_dir, f"{prefix}_{name}.pth")
        if not os.path.exists(path):
            path = os.path.join(pretrain_dir, f"Joint_Model_{name}.pth")
            if not os.path.exists(path):
                missing_agent_names.append(name)
                continue
        existing_paths[name] = path

    # ç¬¬äºŒæ­¥ï¼šå¤„ç†ç¼ºå¤±æ¨¡å‹
    if missing_agent_names:
        print("\nâŒ ä»¥ä¸‹JointNetæ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼š")
        for name in missing_agent_names:
            print(f"   - {name} Agent")
        
        # äº¤äº’ç¡®è®¤æ˜¯å¦é‡æ–°åˆå§‹åŒ–
        while True:
            user_input = input("\nğŸ“Œ æ˜¯å¦é‡æ–°åˆå§‹åŒ–è¿™äº›ç¼ºå¤±çš„æ™ºèƒ½ä½“ï¼Ÿ(y/n): ").strip().lower()
            if user_input in ['y', 'yes']:
                # é‡æ–°åˆå§‹åŒ–ç¼ºå¤±çš„æ™ºèƒ½ä½“ï¼ˆéœ€è¦å…ˆåˆå§‹åŒ–RNNï¼‰
                print("\nğŸ”„ é‡æ–°åˆå§‹åŒ–RNNæ¨¡å‹ï¼ˆä½¿ç”¨é»˜è®¤åˆå§‹è·¯å¾„ï¼‰...")
                rnn_model = MultiTaskRNN().to(device)
                rnn_model.load_state_dict(torch.load(args.init_rnn_path, map_location=device))
                rnn_model.train()
                
                action_dims = {"FC": 32, "BAT": 40, "SC": 2}  # é»˜è®¤åŠ¨ä½œç»´åº¦
                for name in missing_agent_names:
                    print(f"\nğŸ”„ é‡æ–°åˆå§‹åŒ–{name} Agentï¼ˆä»0å¼€å§‹ï¼‰...")
                    agent = JointDQN(name, rnn_model, action_dims[name])
                    agent.setup_optimizer(LR, LR_FACTOR, LR_PATIENCE)
                    agents[name] = agent
                    print(f"âœ… {name} Agentå·²é‡æ–°åˆå§‹åŒ–å®Œæˆ")
                break
            elif user_input in ['n', 'no']:
                print("\nğŸ›‘ ç”¨æˆ·é€‰æ‹©ç»ˆæ­¢è®­ç»ƒï¼Œé€€å‡ºç¨‹åº...")
                sys.exit(0)
            else:
                print("âš ï¸ è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ y/yes æˆ– n/noï¼")

    # ç¬¬ä¸‰æ­¥ï¼šåŠ è½½å­˜åœ¨çš„å®Œæ•´Jointæ¨¡å‹
    for name in existing_paths:
        path = existing_paths[name]
        try:
            ckpt = torch.load(path, map_location=device)
            
            # åˆ¤æ–­æ˜¯å¦æ˜¯åŒ…å«RNNçš„å®Œæ•´æ¨¡å‹
            has_rnn_params = any(key.startswith('rnn_part.') for key in ckpt.keys())
            if has_rnn_params:
                print(f"\nğŸ“Œ æ£€æµ‹åˆ°{name}æ¨¡å‹åŒ…å«RNNå‚æ•°ï¼ŒåŠ è½½å®Œæ•´Jointæ¨¡å‹...")
                # åˆå§‹åŒ–RNNï¼ˆç¬¬ä¸€æ¬¡åŠ è½½æ—¶ï¼‰
                if rnn_model is None:
                    rnn_model = MultiTaskRNN().to(device)
                
                # è·å–åŠ¨ä½œç»´åº¦
                try:
                    n_act = ckpt['marl_part.output.weight'].shape[0]
                except KeyError:
                    n_act = ckpt['output.weight'].shape[0]
                
                # åˆå§‹åŒ–Agentå¹¶åŠ è½½å®Œæ•´å‚æ•°ï¼ˆåŒ…å«RNNï¼‰
                agent = JointDQN(name, rnn_model, n_act)
                agent.eval_net.load_state_dict(ckpt)
                agent.target_net.load_state_dict(ckpt)
                agent.setup_optimizer(LR, LR_FACTOR, LR_PATIENCE)
                agents[name] = agent
                
                # æ›´æ–°RNNæ¨¡å‹ï¼ˆæ‰€æœ‰Agentå…±äº«åŒä¸€ä¸ªRNNï¼‰
                rnn_model = agent.eval_net.rnn_part
                rnn_model.train()
                
                print(f"âœ… æˆåŠŸåŠ è½½åŒ…å«RNNçš„{name}å®Œæ•´Jointæ¨¡å‹: {path}")
            else:
                print(f"\nğŸ“Œ {name}æ¨¡å‹ä¸åŒ…å«RNNå‚æ•°ï¼ŒåŠ è½½ä¼ ç»ŸMARLæ¨¡å‹...")
                # å…¼å®¹æ—§æ¨¡å‹ï¼Œéœ€è¦åˆå§‹åŒ–RNN
                if rnn_model is None:
                    rnn_model = MultiTaskRNN().to(device)
                    rnn_model.load_state_dict(torch.load(args.init_rnn_path, map_location=device))
                    rnn_model.train()
                
                # è·å–åŠ¨ä½œç»´åº¦
                try:
                    n_act = ckpt['output.weight'].shape[0]
                except KeyError:
                    n_act = 32 if name == "FC" else 40 if name == "BAT" else 2
                
                agent = JointDQN(name, rnn_model, n_act)
                agent.eval_net.marl_part.load_state_dict(ckpt)
                agent.target_net.marl_part.load_state_dict(ckpt)
                agent.setup_optimizer(LR, LR_FACTOR, LR_PATIENCE)
                agents[name] = agent
                print(f"âœ… æˆåŠŸåŠ è½½{name} MARLæ¨¡å‹ï¼ˆä½¿ç”¨åˆå§‹RNNï¼‰: {path}")
                
        except Exception as e:
            raise RuntimeError(f"åŠ è½½{name} Agentå¤±è´¥: {e}")
    
    # ç¡®ä¿æ‰€æœ‰Agentå…±äº«åŒä¸€ä¸ªRNNæ¨¡å‹
    if rnn_model is None:
        raise RuntimeError("æœªèƒ½åˆå§‹åŒ–/åŠ è½½RNNæ¨¡å‹")
    
    for name in agents:
        agents[name].eval_net.rnn_part = rnn_model
        agents[name].target_net.rnn_part = rnn_model
    
    return agents["FC"], agents["BAT"], agents["SC"], rnn_model

# ====================== Main è®­ç»ƒé€»è¾‘ï¼ˆå®Œæ•´å¢å¼ºç‰ˆï¼‰ ======================
if __name__ == '__main__':
    # æ‰“å°é…ç½®ç¡®è®¤ä¿¡æ¯
    print("=" * 80)
    print("                    JointNetè®­ç»ƒé…ç½®ç¡®è®¤                  ")
    print("=" * 80)
    print(f"è®­ç»ƒæ¨¡å¼: {'ç»§ç»­è®­ç»ƒï¼ˆåŸºäºå·²æœ‰æ¨¡å‹ï¼‰' if RESUME_TRAINING else 'ä»å¤´è®­ç»ƒ'}")
    if RESUME_TRAINING:
        print(f"é¢„è®­ç»ƒæ¨¡å‹é…ç½®:")
        print(f"  - æ—¥æœŸæ–‡ä»¶å¤¹: {PRETRAIN_DATE}")
        print(f"  - Train ID: {PRETRAIN_TRAIN_ID}")
        print(f"  - æ¨¡å‹å‰ç¼€: {PRETRAIN_MODEL_PREFIX}")
        print(f"  - åˆå§‹RNNè·¯å¾„: ã€ç»§ç»­è®­ç»ƒæ¨¡å¼ï¼Œä¸ä½¿ç”¨ã€‘")
    else:
        print(f"  - åˆå§‹RNNè·¯å¾„: {args.init_rnn_path}")
    print(f"æ ¸å¿ƒè¶…å‚æ•°:")
    print(f"  - æ‰¹å¤§å°: {BATCH_SIZE}")
    print(f"  - å­¦ä¹ ç‡: {LR:.6f}")
    print(f"  - æ¢ç´¢ç‡: {EPSILON}")
    print(f"  - è®­ç»ƒå›åˆæ•°: {EPISODE}")
    print("=" * 80 + "\n")

    # 1. å‡†å¤‡ç›®å½•
    TARGET_BASE_DIR = os.path.join(project_root, "nets", "Chap4", "Joint_Net", execute_date)
    os.makedirs(TARGET_BASE_DIR, exist_ok=True)
    train_id = get_max_folder_name(TARGET_BASE_DIR)
    base_path = os.path.join(TARGET_BASE_DIR, str(train_id))
    os.makedirs(base_path, exist_ok=True)
    
    # æ›´æ–°remark
    if RESUME_TRAINING:
        remark = f"RESUME_JOINT_{execute_date}_{train_id}"
    else:
        remark = f"JOINT_{execute_date}_{train_id}"

    # 2. TensorBoardæ—¥å¿—
    log_dir = args.log_dir if args.log_dir else os.path.join(base_path, "logs")
    writer = SummaryWriter(log_dir=log_dir)

    # 3. åˆå§‹åŒ–/åŠ è½½æ¨¡å‹ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
    rnn_model = None
    FC_Agent, Bat_Agent, SC_Agent = None, None, None
    
    if RESUME_TRAINING:
        print("\nğŸ“Œ å¼€å§‹åŠ è½½åŒ…å«RNNçš„å®Œæ•´é¢„è®­ç»ƒJointNetæ¨¡å‹...")
        try:
            FC_Agent, Bat_Agent, SC_Agent, rnn_model = load_full_joint_agents(
                PRETRAIN_DATE, PRETRAIN_TRAIN_ID, PRETRAIN_MODEL_PREFIX
            )
            print(f"âœ… æˆåŠŸåŠ è½½æ‰€æœ‰åŒ…å«RNNçš„å®Œæ•´JointNetæ™ºèƒ½ä½“")
        except Exception as e:
            print(f"âŒ åŠ è½½å®Œæ•´Jointæ¨¡å‹å¤±è´¥: {e}")
            raise
    else:
        print("\nğŸ“Œ ä»å¤´åˆå§‹åŒ–JointNetæ™ºèƒ½ä½“ï¼ˆåŒ…å«RNNï¼‰...")
        # åˆå§‹åŒ–åŸºç¡€ RNN
        try:
            rnn_model = MultiTaskRNN().to(device)
            rnn_model.load_state_dict(torch.load(args.init_rnn_path, map_location=device))
            rnn_model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œå…è®¸åå‘ä¼ æ’­
        except FileNotFoundError as e:
            print(f"âŒ åˆå§‹RNNæ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
            raise
        except Exception as e:
            print(f"âŒ åˆå§‹RNNæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
        
        # ä»å¤´åˆå§‹åŒ–æ™ºèƒ½ä½“
        FC_Agent = JointDQN("FC", rnn_model, 32)
        Bat_Agent = JointDQN("BAT", rnn_model, 40)
        SC_Agent = JointDQN("SC", rnn_model, 2)
        # è®¾ç½®ä¼˜åŒ–å™¨
        FC_Agent.setup_optimizer(LR, LR_FACTOR, LR_PATIENCE)
        Bat_Agent.setup_optimizer(LR, LR_FACTOR, LR_PATIENCE)
        SC_Agent.setup_optimizer(LR, LR_FACTOR, LR_PATIENCE)
        print(f"âœ… æˆåŠŸåˆå§‹åŒ–æ‰€æœ‰JointNetæ™ºèƒ½ä½“ï¼ˆåŒ…å«RNNï¼‰")

    all_agents = [FC_Agent, Bat_Agent, SC_Agent]

    # 5. å…±äº«å†…å­˜åˆå§‹åŒ– (å…³é”®ä¿®å¤)
    MEMORY_WIDTH = N_STATES * 2 + 3 + 1  # s(7), a1, a2, a3, r(1), s_(7) = 18
    shared_memory = np.zeros((MEMORY_CAPACITY, MEMORY_WIDTH))
    memory_counter = [0]
    
    # ç»‘å®šå…±äº«å†…å­˜åˆ°æ™ºèƒ½ä½“
    for a in all_agents:
        a.shared_memory = shared_memory
        a.memory_counter = memory_counter

    # 6. è®­ç»ƒå¾ªç¯ï¼ˆå®Œæ•´å¢å¼ºç‰ˆï¼‰
    print(f'\nğŸš€ JointNetè®­ç»ƒå¼€å§‹ [ID: {train_id}] [Device: {device}]')
    start_time_total = time.time()
    reward_max = -float('inf')
    reward_not_improve_episodes = 0
    training_done = False
    x_episodes, y_rewards, loss_records = [], [], []

    if REAL_TIME_DRAW:
        plt.ion()
        fig, ax = plt.subplots()
        line, = ax.plot(x_episodes, y_rewards)

    episode_pbar = tqdm(range(EPISODE), desc=f"JointNet Training")
    for i_episode in episode_pbar:
        if training_done:
            break

        # ç¡®ä¿RNNå¤„äºè®­ç»ƒæ¨¡å¼
        if rnn_model is not None:
            rnn_model.train()
        s = env.reset()
        ep_r = 0
        episode_times = {
            'Action_Select': 0.0,
            'Env_Step': 0.0,
            'Data_Store': 0.0,
            'DQN_Learn': 0.0
        }
        step_count = 0
        step_loss = []

        while True:
            # åŠ¨ä½œé€‰æ‹©
            time_start_action = time.time()
            a_fc = FC_Agent.choose_action(s, train=True, epsilon=EPSILON)
            a_bat = Bat_Agent.choose_action(s, train=True, epsilon=EPSILON)
            a_sc = SC_Agent.choose_action(s, train=True, epsilon=EPSILON)
            episode_times['Action_Select'] += (time.time() - time_start_action)

            # ç¯å¢ƒäº¤äº’
            action_list = [a_fc, a_bat, a_sc]
            time_start_step = time.time()
            s_, r, done, _ = env.step(action_list)
            episode_times['Env_Step'] += (time.time() - time_start_step)

            # å­˜å‚¨è½¬æ¢æ•°æ® (æ˜¾å¼æ‹¼æ¥ç¡®ä¿ç»´åº¦æ­£ç¡®)
            time_start_store = time.time()
            transition = np.zeros(MEMORY_WIDTH)
            transition[0:7] = s
            transition[7:10] = [a_fc, a_bat, a_sc]
            transition[10] = r
            transition[11:18] = s_
            
            index = memory_counter[0] % MEMORY_CAPACITY
            shared_memory[index, :] = transition
            memory_counter[0] += 1
            episode_times['Data_Store'] += (time.time() - time_start_store)

            ep_r += r
            step_count += 1

            # å­¦ä¹ è¿‡ç¨‹
            if memory_counter[0] > BATCH_SIZE and memory_counter[0] % LEARN_FREQUENCY == 0:
                time_start_learn = time.time()
                try:
                    l1 = FC_Agent.learn(0, N_STATES, GAMMA, TARGET_REPLACE_ITER, BATCH_SIZE) or 0.0
                    l2 = Bat_Agent.learn(1, N_STATES, GAMMA, TARGET_REPLACE_ITER, BATCH_SIZE) or 0.0
                    l3 = SC_Agent.learn(2, N_STATES, GAMMA, TARGET_REPLACE_ITER, BATCH_SIZE) or 0.0
                    step_loss.append((l1 + l2 + l3) / 3.0)
                except Exception as e:
                    print(f"\nâš ï¸ å­¦ä¹ è¿‡ç¨‹å¼‚å¸¸ (Episode {i_episode}): {e}")
                    step_loss.append(0.0)
                episode_times['DQN_Learn'] += (time.time() - time_start_learn)

            if done:
                # è®°å½•è®­ç»ƒæŒ‡æ ‡
                avg_loss = np.mean(step_loss) if step_loss else 0.0
                writer.add_scalar("Reward/Episode", ep_r, i_episode)
                writer.add_scalar("Loss/Average", avg_loss, i_episode)
                
                loss_records.append(avg_loss)
                using_time_total = time.time() - start_time_total
                current_lr = FC_Agent.optimizer.param_groups[0]["lr"]
                
                # æ›´æ–°è¿›åº¦æ¡
                episode_pbar.set_postfix({
                    'Rew': f'{ep_r:.2f}',
                    'LR': f'{current_lr:.2e}',
                    'Mem': f'{min(memory_counter[0], MEMORY_CAPACITY)}',
                    'Loss': f'{avg_loss:.4f}',
                    'Time': f'{using_time_total:.2f}s'
                })

                # æ‰“å°è€—æ—¶åˆ†è§£ï¼ˆæ¯500å›åˆï¼‰
                if i_episode < 2 or (i_episode + 1) % 500 == 0:
                    print_time_breakdown(i_episode + 1, episode_times)
                break

            s = s_

        x_episodes.append(i_episode)
        y_rewards.append(ep_r)

        # æ¨¡å‹ä¿å­˜ä¸æ—©åœé€»è¾‘ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šä¿å­˜åŒ…å«RNNçš„å®Œæ•´æ¨¡å‹ï¼‰
        if ep_r > reward_max + REWARD_THRESHOLD:
            reward_max = ep_r
            reward_not_improve_episodes = 0
            # ä¿å­˜åŒ…å«RNNçš„å®Œæ•´æœ€ä¼˜æ¨¡å‹
            torch.save(FC_Agent.eval_net.state_dict(), os.path.join(base_path, f"{best_model_base_name}_FC.pth"))
            torch.save(Bat_Agent.eval_net.state_dict(), os.path.join(base_path, f"{best_model_base_name}_BAT.pth"))
            torch.save(SC_Agent.eval_net.state_dict(), os.path.join(base_path, f"{best_model_base_name}_SC.pth"))
            # é¢å¤–ä¿å­˜ç‹¬ç«‹çš„RNNæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
            torch.save(rnn_model.state_dict(), os.path.join(base_path, f"{best_model_base_name}_RNN.pth"))
            print(f"\n--- New Max Reward: {reward_max:.2f} ---")
            print(f"--- å·²ä¿å­˜åŒ…å«RNNçš„å®Œæ•´Jointæ¨¡å‹åˆ°: {base_path} ---")
        else:
            reward_not_improve_episodes += 1

        # å­¦ä¹ ç‡è°ƒåº¦
        for agent in all_agents:
            agent.scheduler.step(ep_r)

        # æ—©åœæ£€æŸ¥
        if reward_not_improve_episodes >= EARLY_STOP_PATIENCE:
            print(f"\n--- Early Stopping Triggered! (No improvement for {EARLY_STOP_PATIENCE} episodes) ---")
            training_done = True

    # 7. æœ€ç»ˆå¤„ç†ï¼ˆå¯¹é½train.pyï¼‰
    final_episode = i_episode + 1 if not training_done else i_episode
    final_model_name = os.path.join(base_path, f"{best_model_base_name}_FINAL")
    
    # ä¿å­˜åŒ…å«RNNçš„æœ€ç»ˆå®Œæ•´æ¨¡å‹
    torch.save(FC_Agent.eval_net.state_dict(), f"{final_model_name}_FC.pth")
    torch.save(Bat_Agent.eval_net.state_dict(), f"{final_model_name}_BAT.pth")
    torch.save(SC_Agent.eval_net.state_dict(), f"{final_model_name}_SC.pth")
    torch.save(rnn_model.state_dict(), f"{final_model_name}_RNN.pth")
    print(f"\nFinal models saved (åŒ…å«RNN): {final_model_name}")

    # æ•´ç†è®­ç»ƒæœ€ç»ˆæŒ‡æ ‡
    final_metrics = {
        "max_reward": round(reward_max, 4),
        "final_reward": round(y_rewards[-1], 4) if y_rewards else 0,
        "average_reward": round(np.mean(y_rewards[POOL_SIZE:]) if len(y_rewards) > POOL_SIZE else 0, 4),
        "total_episodes_completed": final_episode,
        "early_stopped": training_done,
        "final_learning_rate": round(FC_Agent.optimizer.param_groups[0]["lr"], 6),
        "reward_not_improve_episodes": reward_not_improve_episodes,
        "best_model_reward": round(reward_max, 4),
        "excluded_episodes": POOL_SIZE
    }

    # ä¿å­˜è¶…å‚æ•°
    save_hyperparameters(base_path, final_metrics)

    # ä¿å­˜è®­ç»ƒè®°å½•åˆ°CSV
    csv_path = os.path.join(base_path, "training_records.csv")
    with open(csv_path, 'w', encoding='utf-8') as f:
        f.write("episode,reward,loss\n")
        for ep, r, l in zip(x_episodes, y_rewards, loss_records):
            f.write(f"{ep},{r:.4f},{l:.4f}\n")
    print(f"âœ… è®­ç»ƒè®°å½•ï¼ˆå«lossï¼‰å·²ä¿å­˜åˆ°CSV: {csv_path}")

    # å¯è§†åŒ–ä¸ä¿å­˜è®­ç»ƒæ›²çº¿
    writer.flush()
    writer.close()
    
    plt.figure(figsize=(12, 6))
    x_filtered = x_episodes[POOL_SIZE:]
    y_filtered = y_rewards[POOL_SIZE:]
    plt.plot(x_filtered, y_filtered, label='Episode Reward', color='#3570a8', linewidth=1.2)
    plt.axhline(y=reward_max, color='#c84343', linestyle='--', label=f'Best Reward: {reward_max:.2f}')
    plt.xlabel('Episode', fontsize=14)
    plt.ylabel('Episode Reward', fontsize=14)
    plt.title(f'JointNet Training Curve (Ep={final_episode}, Exclude First {POOL_SIZE} Episodes)', fontsize=16)
    plt.legend(fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.savefig(os.path.join(base_path, f"{best_model_base_name}_train_curve.svg"), dpi=1200, bbox_inches='tight')
    plt.savefig(os.path.join(base_path, f"{best_model_base_name}_train_curve.png"), dpi=300, bbox_inches='tight')
    
    if REAL_TIME_DRAW:
        plt.ioff()
        plt.show()
    else:
        plt.close()

    print(f"\nğŸ‰ JointNetè®­ç»ƒå®Œæˆï¼æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {base_path}")
    print(f"\nğŸ“‹ æœ€ä¼˜æ¨¡å‹æ–‡ä»¶åå‰ç¼€ï¼š{best_model_base_name}")
    print(f"ğŸ“‹ æ¨¡å‹åŒ…å«å®Œæ•´çš„RNN+MARLå‚æ•°ï¼Œåç»­å¯ç›´æ¥ç”¨--resume-trainingåŠ è½½")

    # 8. è‡ªåŠ¨æ‰§è¡Œæµ‹è¯•è„šæœ¬ï¼ˆå¯¹é½train.pyï¼Œä¿®æ”¹RNNè·¯å¾„ï¼‰
    test_script_path = os.path.join(project_root, "Scripts", "Chapter4", "test_Joint.py")
    if os.path.exists(test_script_path):
        test_cmd = [
            str(sys.executable),
            str(test_script_path),
            "--net-date", str(execute_date),
            "--train-id", str(train_id),
            "--model-prefix", str(best_model_base_name),
            "--rnn-path", os.path.join(base_path, f"{best_model_base_name}_RNN.pth")  # ä½¿ç”¨è®­ç»ƒåçš„RNN
        ]
        print("\nğŸš€ å¼€å§‹æ‰§è¡ŒJointNetæµ‹è¯•è„šæœ¬...")
        print(" ".join(test_cmd))
        subprocess.run(test_cmd, check=True)
    else:
        print(f"\nâš ï¸ æµ‹è¯•è„šæœ¬æœªæ‰¾åˆ°: {test_script_path}ï¼Œè·³è¿‡è‡ªåŠ¨æµ‹è¯•")

    print(f"\nğŸ‰ æ‰€æœ‰æµç¨‹å®Œæˆï¼æ–‡ä»¶ä¿å­˜è·¯å¾„: {base_path}")