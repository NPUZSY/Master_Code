import os
import time
import json
import subprocess
import sys
import argparse
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import torch

def setup_path():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(script_dir, '..', '..'))
    if project_root not in sys.path:
        sys.path.append(project_root)
    return project_root

project_root = setup_path()

# ----------------------------------------------------
# 1. ç¯å¢ƒä¸è·¯å¾„é…ç½®
# ----------------------------------------------------
from Scripts.Chapter3.MARL_Engine import (
    setup_project_root, device, 
    get_max_folder_name
)
project_root = setup_project_root()
from Scripts.Env import Envs
from Scripts.utils.global_utils import *

# ç¡®ä¿ Joint_Net å®šä¹‰æ­£ç¡®
from Scripts.Chapter4.Joint_Net import MultiTaskRNN, JointNet, JointDQN

font_get()

# ====================== å‘½ä»¤è¡Œå‚æ•°è§£æ ======================
def parse_args():
    parser = argparse.ArgumentParser(description='Joint_Model è€¦åˆæ¨¡å‹ç»§ç»­è®­ç»ƒè„šæœ¬')
    
    # è·¯å¾„å‚æ•°
    parser.add_argument('--pretrain-date', type=str, default="1219")
    parser.add_argument('--pretrain-train-id', type=str, default="1")
    parser.add_argument('--pretrain-prefix', type=str, default="Joint_Model")
    parser.add_argument('--rnn-path', type=str, 
                        default=os.path.join(project_root, "nets/Chap4/RNN_Reg_Opt_MultiTask/1216/17/rnn_classifier_multitask.pth"))

    # è®­ç»ƒè¶…å‚æ•°
    parser.add_argument('--batch-size', type=int, default=128) # å¾®è°ƒå»ºè®® batch å¤§ä¸€ç‚¹
    parser.add_argument('--lr', type=float, default=1e-6)
    parser.add_argument('--epsilon', type=float, default=0.9)
    parser.add_argument('--gamma', type=float, default=0.95)
    parser.add_argument('--pool-size', type=int, default=20) # å»ºè®®å¢å¤§æ± å­
    parser.add_argument('--episode', type=int, default=2000)
    parser.add_argument('--learn-frequency', type=int, default=5)
    
    return parser.parse_args()

args = parse_args()

# ====================== å…¨å±€é…ç½® ======================
try:
    env = Envs()
except Exception as e:
    print(f"âŒ ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
    sys.exit()

torch.set_default_dtype(torch.float32)
GLOBAL_SEED = 42
torch.manual_seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)

N_STATES = 7 
# ç¡®ä¿ MEMORY_CAPACITY è‡³å°‘å¤§äº batch_size
MEMORY_CAPACITY = max(env.step_length * args.pool_size, args.batch_size * 2)
execute_date = time.strftime("%m%d", time.localtime())

# ====================== è¾…åŠ©å‡½æ•°ï¼šåŠ è½½è€¦åˆæ¨¡å‹ ======================
def load_joint_agents(rnn_model, pretrain_date, pretrain_id, prefix):
    pretrain_dir = os.path.join(project_root, "nets", "Chap4", "Joint_Net", pretrain_date, pretrain_id)
    agents = {}
    names = ["FC", "BAT", "SC"]
    
    if not os.path.exists(pretrain_dir):
        raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {pretrain_dir}")

    for name in names:
        path = os.path.join(pretrain_dir, f"{prefix}_{name}.pth")
        if not os.path.exists(path):
            # é˜²å¾¡ï¼šå°è¯•åŠ è½½æ²¡æœ‰å‰ç¼€çš„é»˜è®¤æ–‡ä»¶
            path = os.path.join(pretrain_dir, f"Joint_Model_{name}.pth")
            if not os.path.exists(path):
                raise FileNotFoundError(f"æ— æ³•å®šä½æ¨¡å‹: {path}")
        
        ckpt = torch.load(path, map_location=device)
        # è·å–åŠ¨ä½œç»´åº¦
        try:
            n_act = ckpt['marl_part.output.weight'].shape[0]
        except KeyError:
            # é˜²å¾¡ï¼šå¦‚æœ state_dict ç»“æ„ä¸åŒï¼Œå°è¯•ä» marl_part æå–
            n_act = ckpt['output.weight'].shape[0]
            
        agent = JointDQN(name, rnn_model, n_act)
        agent.eval_net.load_state_dict(ckpt)
        agent.target_net.load_state_dict(ckpt)
        
        agent.setup_optimizer(args.lr, 0.5, 50)
        agents[name] = agent
        print(f"âœ… Loaded {name} Agent ({n_act} actions)")
        
    return agents["FC"], agents["BAT"], agents["SC"]

# ====================== Main è®­ç»ƒé€»è¾‘ ======================
if __name__ == '__main__':
    # 1. å‡†å¤‡ç›®å½•
    TARGET_BASE_DIR = os.path.join(project_root, "nets", "Chap4", "Joint_Net", execute_date)
    os.makedirs(TARGET_BASE_DIR, exist_ok=True)
    train_id = get_max_folder_name(TARGET_BASE_DIR)
    base_path = os.path.join(TARGET_BASE_DIR, str(train_id))
    os.makedirs(base_path, exist_ok=True)
    writer = SummaryWriter(log_dir=os.path.join(base_path, "logs"))

    # 2. åˆå§‹åŒ–åŸºç¡€ RNN
    rnn_model = MultiTaskRNN().to(device)

    # 3. åŠ è½½æ™ºèƒ½ä½“
    FC_Agent, Bat_Agent, SC_Agent = load_joint_agents(
        rnn_model, args.pretrain_date, args.pretrain_train_id, args.pretrain_prefix
    )
    all_agents = [FC_Agent, Bat_Agent, SC_Agent]

    # 4. å…±äº«å†…å­˜ (ğŸš¨ å…³é”®ä¿®å¤ç‚¹)
    MEMORY_WIDTH = N_STATES * 2 + 3 + 1 # s(7), a1, a2, a3, r(1), s_(7) = 18
    shared_memory = np.zeros((MEMORY_CAPACITY, MEMORY_WIDTH))
    memory_counter = [0]
    
    # é˜²å¾¡æ€§ï¼šæ˜¾å¼ç»‘å®šåˆ° MARL_Engine å†…éƒ¨ä½¿ç”¨çš„å˜é‡å
    for a in all_agents:
        a.shared_memory = shared_memory # ğŸš¨ å¿…é¡»ä½¿ç”¨ shared_memory åŒ¹é…çˆ¶ç±» learn æ–¹æ³•
        a.memory_counter = memory_counter

    # 5. è®­ç»ƒå¾ªç¯
    print(f'\nğŸš€ Joint Fine-tuning [ID: {train_id}] [Device: {device}]')
    reward_max = -float('inf')
    x_episodes, y_rewards, loss_records = [], [], []

    pbar = tqdm(range(args.episode), desc="Joint Training")
    for i_episode in pbar:
        s = env.reset()
        ep_r = 0
        step_loss = []

        while True:
            a_fc = FC_Agent.choose_action(s, train=True, epsilon=args.epsilon)
            a_bat = Bat_Agent.choose_action(s, train=True, epsilon=args.epsilon)
            a_sc = SC_Agent.choose_action(s, train=True, epsilon=args.epsilon)

            s_, r, done, _ = env.step([a_fc, a_bat, a_sc])

            # å­˜å‚¨è½¬æ¢æ•°æ® (æ˜¾å¼æ‹¼æ¥ç¡®ä¿ç»´åº¦æ­£ç¡®)
            transition = np.zeros(MEMORY_WIDTH)
            transition[0:7] = s
            transition[7:10] = [a_fc, a_bat, a_sc]
            transition[10] = r
            transition[11:18] = s_
            
            index = memory_counter[0] % MEMORY_CAPACITY
            shared_memory[index, :] = transition
            memory_counter[0] += 1

            ep_r += r

            # å­¦ä¹ è§¦å‘åˆ¤å®š
            # ğŸš¨ é˜²å¾¡ï¼šå¿…é¡»ä¿è¯æ± å­é‡Œçš„æ•°æ®é‡å¤§äº batch_size ä¸”æ± å­å·²å¼€å§‹æœ‰æœ‰æ•ˆè¦†ç›–
            if memory_counter[0] > args.batch_size and memory_counter[0] % args.learn_frequency == 0:
                try:
                    l1 = FC_Agent.learn(0, N_STATES, args.gamma, 100, args.batch_size) or 0
                    l2 = Bat_Agent.learn(1, N_STATES, args.gamma, 100, args.batch_size) or 0
                    l3 = SC_Agent.learn(2, N_STATES, args.gamma, 100, args.batch_size) or 0
                    step_loss.append((l1 + l2 + l3) / 3.0)
                except Exception as e:
                    # è®°å½•å­¦ä¹ è¿‡ç¨‹ä¸­çš„å¼‚å¸¸ä½†ä¸ä¸­æ–­è®­ç»ƒ
                    pass

            if done:
                avg_loss = np.mean(step_loss) if step_loss else 0
                writer.add_scalar("Reward/Episode", ep_r, i_episode)
                writer.add_scalar("Loss/Average", avg_loss, i_episode)
                
                loss_records.append(avg_loss)
                pbar.set_postfix({'Rew': f'{ep_r:.1f}', 'Mem': f'{min(memory_counter[0], MEMORY_CAPACITY)}'})
                break
            s = s_

        x_episodes.append(i_episode)
        y_rewards.append(ep_r)

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if ep_r > reward_max and memory_counter[0] > args.batch_size:
            reward_max = ep_r
            for a in all_agents:
                save_path = os.path.join(base_path, f"Joint_Model_{a.agent_name}.pth")
                torch.save(a.eval_net.state_dict(), save_path)

    # 6. ä¿å­˜è®­ç»ƒæ›²çº¿ä¸æœ€ç»ˆæ¨¡å‹
    for a in all_agents:
        torch.save(a.eval_net.state_dict(), os.path.join(base_path, f"FINAL_{a.agent_name}.pth"))

    plt.figure(figsize=(10, 5))
    plt.plot(x_episodes, y_rewards, label='Episode Reward')
    plt.axhline(y=reward_max, color='r', linestyle='--', label=f'Best: {reward_max:.1f}')
    plt.xlabel('Episode'); plt.ylabel('Reward'); plt.legend(); plt.grid(True)
    plt.savefig(os.path.join(base_path, "train_curve.png"))
    
    writer.close()
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼ç»“æœç›®å½•: {base_path}")