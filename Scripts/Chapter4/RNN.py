import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os

# å°†å¤šåˆ†ç±» RNN æ¨¡å‹ä¿®æ”¹ä¸ºè¿ç»­å›å½’æ¨¡å‹ï¼Œè¾“å‡ºèŒƒå›´ä¸º [0, 1]
class ActionValueNet(nn.Module):
    # æ ¸å¿ƒä¿®æ”¹ç‚¹ 1: output_dim é»˜è®¤ä¸º 1 (å¯¹åº”è¿ç»­çš„å›å½’å€¼)
    def __init__(self, input_dim=7, hidden_dim_rnn=128, hidden_dim_fc=64, output_dim=1):
        super(ActionValueNet, self).__init__()
        
        # --- éšè—å±‚ 1: GRU (7 -> 128) ---
        self.rnn = nn.GRU(
            input_size=input_dim, 
            hidden_size=hidden_dim_rnn, 
            num_layers=2, 
            batch_first=True
        )
        
        # --- éšè—å±‚ 2: å…¨è¿æ¥å±‚ (128 -> 64) ---
        self.fc_128_64 = nn.Linear(hidden_dim_rnn, hidden_dim_fc)
        # --- éšè—å±‚ 3: å…¨è¿æ¥å±‚ (64 -> 64) ---
        self.fc_64_64 = nn.Linear(hidden_dim_fc, hidden_dim_fc)
        
        # --- è¾“å‡ºå±‚: å›å½’è¾“å‡º (64 -> 1) ---
        # æ ¸å¿ƒä¿®æ”¹ç‚¹ 2: è¾“å‡ºç»´åº¦ä¸º output_dim (1)
        self.fc_64_out = nn.Linear(hidden_dim_fc, output_dim)
        self.fc_64_1 = self.fc_64_out 
        
        self.requires_grad_fc_64_1_only = True 

    def forward(self, x):
        x = x.unsqueeze(1) # (N, 1, 7)
        out_rnn, _ = self.rnn(x)
        out_rnn = out_rnn.squeeze(1) # (N, 128)
        
        # 4.1 éšè—å±‚ 2 (64ç»´ç‰¹å¾)
        feature_64 = F.relu(self.fc_128_64(out_rnn)) # (N, 64)
        # 4.2 éšè—å±‚ 3 (64ç»´ç‰¹å¾)
        feature_64 = F.relu(self.fc_64_64(feature_64)) # (64, 64)
        
        # 5. è¾“å‡ºå±‚ (1ç»´åŸå§‹è¾“å‡º)
        a_raw_out = self.fc_64_out(feature_64) # (N, 1)
        
        # ğŸ“Œ æ ¸å¿ƒä¿®æ”¹ 3: ä½¿ç”¨ Sigmoid æ¿€æ´»å‡½æ•°å°†è¾“å‡ºçº¦æŸåˆ° [0, 1] ä¹‹é—´
        a_out = torch.sigmoid(a_raw_out) # (N, 1)
        
        # è¿”å›çº¦æŸåœ¨ [0, 1] çš„å›å½’å€¼ï¼Œä»¥åŠç‰¹å¾
        return a_out, feature_64

    # è¾…åŠ©æ–¹æ³•: è®¾ç½®å¯è®­ç»ƒå±‚ (ä¿æŒä¸å˜ï¼Œä½†é’ˆå¯¹æ–°çš„ 1 ç»´è¾“å‡ºå±‚)
    def set_trainable_layers(self, trainable=True):
        if self.requires_grad_fc_64_1_only:
            for param in self.parameters():
                param.requires_grad = False
            for param in self.fc_64_out.parameters(): # fc_64_out æ˜¯æœ€ç»ˆè¾“å‡ºå±‚
                param.requires_grad = trainable
        else:
            for param in self.parameters():
                param.requires_grad = trainable