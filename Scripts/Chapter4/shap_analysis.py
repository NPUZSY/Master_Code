import matplotlib.pyplot as plt
import torch
import numpy as np
import os
import json
import argparse
import sys
from json import JSONEncoder
import torch.nn as nn
import torch.nn.functional as F
import shap
import pandas as pd
import re  # æ­£åˆ™è¡¨è¾¾å¼å¤„ç†æ•°å€¼


# ç¤ºä¾‹æŒ‡ä»¤
# python Scripts/Chapter4/shap_analysis.py --net-date 1222 --train-id 11 --n-samples 500


# ====================== 1. ç¯å¢ƒä¸è·¯å¾„é…ç½®ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰ ======================
def setup_path():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(script_dir, '..', '..'))
    if project_root not in sys.path:
        sys.path.append(project_root)
    return project_root

project_root = setup_path()

# å¯¼å…¥åŸæœ‰å¼•æ“ç»„ä»¶
from Scripts.Chapter3.MARL_Engine import Net, IndependentDQN, device
from Scripts.Env import Envs  # å¯¼å…¥ä½ æä¾›çš„çœŸå®ç¯å¢ƒ
from Scripts.utils.global_utils import font_get

# è·å–å­—ä½“è®¾ç½®
font_get()

# ====================== 2. JointNet ç›¸å…³ç±»å®šä¹‰ï¼ˆå®Œå…¨å¤ç”¨ï¼‰ ======================
class MultiTaskRNN(nn.Module):
    """é€‚é… 7 ç»´è¾“å…¥çš„å¤šä»»åŠ¡ RNN ç»“æ„"""
    def __init__(self, input_dim=7, hidden_dim_rnn=256, num_layers_rnn=2, hidden_dim_fc=64):
        super(MultiTaskRNN, self).__init__()
        self.rnn = nn.GRU(input_dim, hidden_dim_rnn, num_layers=num_layers_rnn, batch_first=True)
        self.fc_rnn_to_64 = nn.Linear(hidden_dim_rnn, hidden_dim_fc)
        self.reg_head = nn.Linear(hidden_dim_fc, 1)
        self.cls_head = nn.Linear(hidden_dim_fc, 4)

    def forward(self, x):
        if x.dim() == 2: x = x.unsqueeze(1)
        out_rnn, _ = self.rnn(x)
        out_rnn = out_rnn[:, -1, :]
        feature_64 = F.relu(self.fc_rnn_to_64(out_rnn))
        return self.reg_head(feature_64), self.cls_head(feature_64), feature_64

class JointNet(nn.Module):
    """æ‹¼æ¥ RNN ç‰¹å¾(64) + å›å½’å€¼(1) = 65ç»´è¾“å…¥ MARL Head"""
    def __init__(self, rnn_part, marl_head):
        super(JointNet, self).__init__()
        self.rnn_part = rnn_part
        self.marl_part = marl_head

    def forward(self, x):
        reg_out, _, feature_64 = self.rnn_part(x)
        joint_input = torch.cat([feature_64, reg_out], dim=1)
        return self.marl_part(joint_input)

class JointDQN(IndependentDQN):
    """æ”¯æŒ 7 ç»´è¾“å…¥å¹¶è‡ªåŠ¨æ‰§è¡Œå†…éƒ¨æ‹¼æ¥çš„æ™ºèƒ½ä½“"""
    def __init__(self, agent_name, rnn_model, n_actions):
        super(JointDQN, self).__init__(agent_name, 65, n_actions)
        self.n_actions = n_actions
        self.eval_net = JointNet(rnn_model, self.eval_net).to(device)
        self.target_net = JointNet(rnn_model, self.target_net).to(device)

    def predict_q_values(self, x):
        """é€‚é…SHAPçš„é¢„æµ‹å‡½æ•°ï¼šè¾“å…¥çŠ¶æ€çŸ©é˜µï¼Œè¾“å‡ºæ‰€æœ‰åŠ¨ä½œçš„Qå€¼"""
        x_tensor = torch.FloatTensor(x).to(device)
        with torch.no_grad():
            q_values = self.eval_net(x_tensor)
        return q_values.cpu().numpy()

    def predict_max_q(self, x):
        """é€‚é…SHAPçš„é¢„æµ‹å‡½æ•°ï¼šè¾“å…¥çŠ¶æ€çŸ©é˜µï¼Œè¾“å‡ºmax Qå€¼ï¼ˆå†³ç­–å¯¹åº”çš„Qå€¼ï¼‰"""
        q_values = self.predict_q_values(x)
        return np.max(q_values, axis=1)

# ====================== 3. å·¥å…·ç±»ä¸å‚æ•°è§£æ ======================
class NumpyEncoder(JSONEncoder):
    """è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œå¤„ç†numpyç±»å‹"""
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, torch.Tensor):
            return obj.cpu().numpy().tolist()
        elif isinstance(obj, (np.float32, np.float64, np.int32, np.int64)):
            return float(obj)
        return super(NumpyEncoder, self).default(obj)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='JointNetæ™ºèƒ½ä½“SHAPåˆ†æè„šæœ¬ï¼ˆå½’ä¸€åŒ–æ¨ªåæ ‡çš„æ•´åˆDependence Plotï¼‰')
    
    # æ ¸å¿ƒå‚æ•°
    parser.add_argument('--net-date', type=str, required=True,
                        help='æ¨¡å‹æ‰€åœ¨çš„æ—¥æœŸæ–‡ä»¶å¤¹ï¼ˆå¿…å¡«ï¼Œå¦‚ï¼š1213ï¼‰')
    parser.add_argument('--train-id', type=str, required=True,
                        help='æ¨¡å‹å¯¹åº”çš„è®­ç»ƒIDï¼ˆå¿…å¡«ï¼Œå¦‚ï¼š11ï¼‰')
    parser.add_argument('--rnn-path', type=str, 
                        default=os.path.join(project_root, "nets/Chap4/RNN_Reg_Opt_MultiTask/1216/17/rnn_classifier_multitask.pth"),
                        help='é¢„è®­ç»ƒRNNæ¨¡å‹è·¯å¾„')
    
    # å¯é€‰é…ç½®
    parser.add_argument('--model-prefix', type=str, default="Joint_Model", help='æ¨¡å‹å‰ç¼€')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--n-samples', type=int, default=500, 
                        help='SHAPåˆ†æçš„é‡‡æ ·æ•°é‡ï¼ˆé»˜è®¤500ï¼Œè¶Šå¤šè¶Šå‡†ç¡®ä½†è€—æ—¶æ›´é•¿ï¼‰')
    parser.add_argument('--show-plot', action='store_true', help='æ˜¯å¦æ˜¾ç¤ºSHAPå›¾ï¼ˆé»˜è®¤ä»…ä¿å­˜ï¼‰')
    
    return parser.parse_args()

# ====================== 4. æ•°æ®å½’ä¸€åŒ–å·¥å…·å‡½æ•° ======================
def min_max_normalize(data, min_val=None, max_val=None):
    """
    Min-Maxå½’ä¸€åŒ–ï¼šå°†æ•°æ®ç¼©æ”¾åˆ°[0,1]åŒºé—´
    å‚æ•°:
        data: å¾…å½’ä¸€åŒ–çš„æ•°ç»„
        min_val: æ‰‹åŠ¨æŒ‡å®šæœ€å°å€¼ï¼ˆNoneåˆ™è‡ªåŠ¨è®¡ç®—ï¼‰
        max_val: æ‰‹åŠ¨æŒ‡å®šæœ€å¤§å€¼ï¼ˆNoneåˆ™è‡ªåŠ¨è®¡ç®—ï¼‰
    è¿”å›:
        normalized_data: å½’ä¸€åŒ–åçš„æ•°æ®
        min_val: ä½¿ç”¨çš„æœ€å°å€¼
        max_val: ä½¿ç”¨çš„æœ€å¤§å€¼
    """
    if min_val is None:
        min_val = np.min(data)
    if max_val is None:
        max_val = np.max(data)
    
    # é¿å…é™¤ä»¥0
    if max_val - min_val < 1e-8:
        return np.zeros_like(data), min_val, max_val
    
    normalized_data = (data - min_val) / (max_val - min_val)
    return normalized_data, min_val, max_val

# ====================== 5. SHAPåˆ†ææ ¸å¿ƒå‡½æ•°ï¼ˆå½’ä¸€åŒ–æ¨ªåæ ‡ï¼‰ ======================
def generate_state_samples(env, n_samples=500):
    """
    ç”Ÿæˆè¦†ç›–æ‰€æœ‰çŠ¶æ€ç»´åº¦çš„é‡‡æ ·æ•°æ®é›†ï¼ˆä¸¥æ ¼åŒ¹é…ç¯å¢ƒçš„observation_spaceï¼‰
    çŠ¶æ€ç»´åº¦å®šä¹‰ï¼ˆæ¥è‡ªEnvsç±»çš„observation_spaceï¼‰ï¼š
    [0] P_load: è´Ÿè½½åŠŸç‡ (0 ~ 80000 W)
    [1] Temperature: ç¯å¢ƒæ¸©åº¦ (-100 ~ 200 Â°C)
    [2] P_fc: ç‡ƒæ–™ç”µæ± åŠŸç‡ (0 ~ 5000 W)
    [3] P_bat: ç”µæ± åŠŸç‡ (-5000 ~ 5000 W)
    [4] P_sc: è¶…çº§ç”µå®¹åŠŸç‡ (-2000 ~ 2000 W)
    [5] SOC_bat: ç”µæ± SOC (0 ~ 1)
    [6] SOC_sc: è¶…çº§ç”µå®¹SOC (0 ~ 1)
    """
    samples = []
    
    # è·å–ç¯å¢ƒçš„è§‚æµ‹ç©ºé—´ä¸Šä¸‹é™
    obs_low = env.observation_space.low
    obs_high = env.observation_space.high
    
    # è¦†ç›–çœŸå®å·¥å†µçš„é‡‡æ ·ï¼ˆåŸºäºç¯å¢ƒå®é™…å‚æ•°ï¼‰
    for _ in range(n_samples):
        # 1. è´Ÿè½½åŠŸç‡ï¼šä»çœŸå®loadsæ•°æ®ä¸­é‡‡æ ·ï¼Œå›é€€åˆ°å‡åŒ€åˆ†å¸ƒ
        if len(env.loads) > 0:
            p_load = float(np.random.choice(env.loads))
        else:
            p_load = np.random.uniform(obs_low[0], obs_high[0])
        
        # 2. ç¯å¢ƒæ¸©åº¦ï¼šä»çœŸå®temperatureæ•°æ®ä¸­é‡‡æ ·ï¼Œå›é€€åˆ°å‡åŒ€åˆ†å¸ƒ
        if len(env.temperature) > 0:
            temp = float(np.random.choice(env.temperature))
        else:
            temp = np.random.uniform(obs_low[1], obs_high[1])
        
        # 3. ç‡ƒæ–™ç”µæ± åŠŸç‡ï¼š0 ~ P_FC_MAX(5000W)
        p_fc = np.random.uniform(obs_low[2], env.P_FC_MAX)
        
        # 4. ç”µæ± åŠŸç‡ï¼š-P_BAT_MAX(5000W) ~ P_BAT_MAX(5000W)
        p_bat = np.random.uniform(-env.P_BAT_MAX, env.P_BAT_MAX)
        
        # 5. è¶…çº§ç”µå®¹åŠŸç‡ï¼š-P_SC_MAX(2000W) ~ P_SC_MAX(2000W)
        p_sc = np.random.uniform(-env.P_SC_MAX, env.P_SC_MAX)
        
        # 6. ç”µæ± SOCï¼š0.2 ~ 0.8ï¼ˆç¯å¢ƒä¸­æƒ©ç½šåŒºé—´å¤–çš„åˆç†èŒƒå›´ï¼‰
        soc_bat = np.random.uniform(0.2, 0.8)
        
        # 7. è¶…çº§ç”µå®¹SOCï¼š0 ~ 1
        soc_sc = np.random.uniform(obs_low[6], obs_high[6])
        
        # æ„é€ çŠ¶æ€å‘é‡
        state = np.array([
            p_load, temp, p_fc, p_bat, p_sc, soc_bat, soc_sc
        ], dtype=np.float32)
        
        # ç¡®ä¿çŠ¶æ€åœ¨è§‚æµ‹ç©ºé—´èŒƒå›´å†…
        state = np.clip(state, obs_low, obs_high)
        samples.append(state)
    
    return np.array(samples)

def plot_combined_dependence_normalized(shap_values, state_samples, feature_names, top_k=3, 
                                        agent_name="Agent", save_dir="./", show_plot=False):
    """
    ç»˜åˆ¶å½’ä¸€åŒ–æ¨ªåæ ‡çš„æ•´åˆDependence Plotï¼š
    - æ¯ä¸ªç‰¹å¾çš„æ¨ªåæ ‡å…ˆMin-Maxå½’ä¸€åŒ–åˆ°[0,1]
    - å›¾ä¾‹ä¸­æ ‡æ³¨åŸå§‹å–å€¼èŒƒå›´ï¼Œä¿è¯ç‰©ç†æ„ä¹‰
    """
    # å®šä¹‰é¢œè‰²å’Œæ ‡è®°ï¼ˆåŒºåˆ†ä¸åŒç‰¹å¾ï¼‰
    colors = ['#e74c3c', '#3498db', '#2ecc71']  # çº¢ã€è“ã€ç»¿
    markers = ['o', 's', '^']  # åœ†å½¢ã€æ–¹å½¢ã€ä¸‰è§’å½¢
    
    # åˆ›å»ºç”»å¸ƒ
    plt.figure(figsize=(12, 8))
    
    # éå†TOP Kç‰¹å¾
    for i in range(top_k):
        # æå–è¯¥ç‰¹å¾çš„åŸå§‹å–å€¼
        feature_vals_original = state_samples[:, i]
        # å¯¹ç‰¹å¾å–å€¼è¿›è¡ŒMin-Maxå½’ä¸€åŒ–
        feature_vals_norm, min_val, max_val = min_max_normalize(feature_vals_original)
        # æå–å¯¹åº”çš„SHAPå€¼
        shap_vals = shap_values[:, i]
        
        # æ„é€ å›¾ä¾‹æ ‡ç­¾ï¼ˆåŒ…å«åŸå§‹å–å€¼èŒƒå›´ï¼‰
        label = f"TOP{i+1}: {feature_names[i]}\n(åŸå§‹èŒƒå›´: {min_val:.1f} ~ {max_val:.1f})"
        
        # ç»˜åˆ¶å½’ä¸€åŒ–åçš„æ•£ç‚¹å›¾
        plt.scatter(
            feature_vals_norm, 
            shap_vals, 
            color=colors[i],
            marker=markers[i],
            alpha=0.6,
            s=30,
            label=label
        )
        
        # æ·»åŠ è¶‹åŠ¿çº¿ï¼ˆåŸºäºå½’ä¸€åŒ–åçš„æ¨ªåæ ‡ï¼‰
        try:
            from scipy import stats
            # è®¡ç®—çº¿æ€§å›å½’è¶‹åŠ¿
            slope, intercept, r_value, p_value, std_err = stats.linregress(feature_vals_norm, shap_vals)
            # ç”Ÿæˆè¶‹åŠ¿çº¿xå€¼ï¼ˆå½’ä¸€åŒ–å0~1ï¼‰
            x_trend = np.linspace(0, 1, 100)
            y_trend = slope * x_trend + intercept
            # ç»˜åˆ¶è¶‹åŠ¿çº¿
            plt.plot(x_trend, y_trend, color=colors[i], linewidth=2, alpha=0.8)
        except:
            # è‹¥çº¿æ€§å›å½’å¤±è´¥ï¼Œè·³è¿‡è¶‹åŠ¿çº¿
            pass
    
    # è®¾ç½®å›¾è¡¨æ ·å¼
    plt.xlabel("Feature Value (Normalized to [0,1])", fontsize=14)
    plt.ylabel("SHAP Value", fontsize=14)
    plt.title(f"Combined SHAP Dependence Plot (Normalized X-axis) - {agent_name}", fontsize=16, pad=20)
    # ä¼˜åŒ–å›¾ä¾‹ï¼ˆé¿å…é‡å ï¼‰
    plt.legend(fontsize=10, loc='best', bbox_to_anchor=(1, 1))
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tick_params(labelsize=12)
    # è®¾ç½®æ¨ªåæ ‡èŒƒå›´ä¸º0~1
    plt.xlim(-0.05, 1.05)
    
    # ä¿å­˜å›¾è¡¨ - åªä¿ç•™SVGæ ¼å¼ï¼Œæ³¨é‡Šæ‰PNG
    dep_svg = os.path.join(save_dir, f"{agent_name}_SHAP_Combined_Dependence_Normalized.svg")
    # dep_png = os.path.join(save_dir, f"{agent_name}_SHAP_Combined_Dependence_Normalized.png")
    plt.savefig(dep_svg, bbox_inches='tight', dpi=1200)
    # plt.savefig(dep_png, bbox_inches='tight', dpi=1200)
    print(f"âœ… {agent_name} å½’ä¸€åŒ–æ•´åˆDependence Plotå·²ä¿å­˜ï¼š{dep_svg}")
    
    if show_plot:
        plt.show()
    plt.close()

def shap_analysis_agent(agent, state_samples, feature_names, save_dir, agent_name, show_plot=False):
    """
    å¯¹å•ä¸ªæ™ºèƒ½ä½“è¿›è¡ŒSHAPåˆ†æå¹¶ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨ï¼ˆå½’ä¸€åŒ–æ¨ªåæ ‡çš„æ•´åˆDependence Plotï¼‰
    ä¼˜åŒ–ï¼šä»…ä¿å­˜ç»˜å›¾æ‰€éœ€çš„å…¨å±€ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸ä¿å­˜åŸå§‹å¤§æ•°ç»„
    """
    # 1. åˆå§‹åŒ–SHAPè§£é‡Šå™¨ï¼ˆä½¿ç”¨KernelExplainerï¼Œé€‚é…ä»»æ„æ¨¡å‹ï¼‰
    # é€‰æ‹©å‰100ä¸ªæ ·æœ¬ä½œä¸ºèƒŒæ™¯é›†ï¼ˆåŠ é€Ÿè®¡ç®—ï¼‰
    background_samples = state_samples[:100]
    explainer = shap.KernelExplainer(agent.predict_max_q, background_samples)
    
    # 2. è®¡ç®—SHAPå€¼ï¼ˆå¯¹æ‰€æœ‰é‡‡æ ·æ ·æœ¬ï¼‰
    print(f"\nğŸ“Š æ­£åœ¨è®¡ç®—{agent_name}çš„SHAPå€¼ï¼ˆå…±{len(state_samples)}ä¸ªæ ·æœ¬ï¼‰...")
    # ä¼˜åŒ–ï¼šnsamples=50å¹³è¡¡è®¡ç®—é€Ÿåº¦å’Œå‡†ç¡®æ€§
    shap_values = explainer.shap_values(state_samples, nsamples=50)
    
    # ========== ä¼˜åŒ–ï¼šä»…ä¿å­˜ç»˜å›¾æ‰€éœ€çš„å…¨å±€ç»Ÿè®¡ä¿¡æ¯ ==========
    # 1. çŠ¶æ€æ ·æœ¬çš„å…¨å±€ç»Ÿè®¡ï¼ˆå‡å€¼ã€æœ€å€¼ã€æ ‡å‡†å·®ï¼‰- è¡¨å¾æ•°æ®åˆ†å¸ƒ
    state_stats = {
        feature_names[i]: {
            "mean": float(np.mean(state_samples[:, i])),
            "min": float(np.min(state_samples[:, i])),
            "max": float(np.max(state_samples[:, i])),
            "std": float(np.std(state_samples[:, i]))
        } for i in range(len(feature_names))
    }
    
    # 2. SHAPå€¼çš„å…¨å±€ç»Ÿè®¡ï¼ˆå‡å€¼ã€æœ€å€¼ã€æ ‡å‡†å·®ï¼‰- è¡¨å¾ç‰¹å¾å½±å“åˆ†å¸ƒ
    shap_stats = {
        feature_names[i]: {
            "mean_shap_value": float(np.mean(shap_values[:, i])),
            "min_shap_value": float(np.min(shap_values[:, i])),
            "max_shap_value": float(np.max(shap_values[:, i])),
            "std_shap_value": float(np.std(shap_values[:, i])),
            "abs_mean_shap_value": float(np.mean(np.abs(shap_values[:, i])))  # ç‰¹å¾é‡è¦æ€§
        } for i in range(len(feature_names))
    }
    
    # 3. TOP3ç‰¹å¾çš„å…³é”®ä¿¡æ¯ï¼ˆç»˜å›¾æ ¸å¿ƒï¼‰
    feature_importance = np.abs(shap_values).mean(axis=0)
    top3_indices = np.argsort(-feature_importance)[:3]
    top3_features = []
    for idx in top3_indices:
        # è®¡ç®—è¯¥ç‰¹å¾çš„å½’ä¸€åŒ–å‚æ•°ï¼ˆç»˜å›¾ç”¨ï¼‰
        feature_vals_norm, min_val, max_val = min_max_normalize(state_samples[:, idx])
        # è®¡ç®—è¯¥ç‰¹å¾SHAPå€¼çš„çº¿æ€§å›å½’å‚æ•°ï¼ˆè¶‹åŠ¿çº¿ç”¨ï¼‰
        slope, intercept, r_value = np.nan, np.nan, np.nan
        try:
            from scipy import stats
            slope, intercept, r_value, _, _ = stats.linregress(feature_vals_norm, shap_values[:, idx])
        except:
            pass
        
        top3_features.append({
            "feature_name": feature_names[idx],
            "importance": float(feature_importance[idx]),
            "rank": int(np.where(top3_indices == idx)[0][0] + 1),
            "original_range": f"{min_val:.1f} ~ {max_val:.1f}",
            "regression_slope": float(slope),
            "regression_intercept": float(intercept),
            "r_squared": float(r_value**2) if not np.isnan(r_value) else np.nan
        })
    
    # 4. æ±‡æ€»æ™ºèƒ½ä½“çš„æ ¸å¿ƒSHAPæ•°æ®
    shap_core_data = {
        "agent_name": agent_name,
        "n_samples": len(state_samples),
        "expected_value": float(explainer.expected_value),  # SHAPåŸºå‡†å€¼
        "state_statistics": state_stats,  # çŠ¶æ€æ ·æœ¬ç»Ÿè®¡
        "shap_statistics": shap_stats,    # SHAPå€¼ç»Ÿè®¡
        "top3_features": top3_features,   # TOP3ç‰¹å¾ï¼ˆç»˜å›¾æ ¸å¿ƒï¼‰
        "feature_importance_ranking": [  # æ‰€æœ‰ç‰¹å¾é‡è¦æ€§æ’å
            {
                "feature_name": feature_names[i],
                "importance": float(feature_importance[i]),
                "rank": int(np.argsort(np.argsort(-feature_importance))[i] + 1)
            } for i in range(len(feature_names))
        ]
    }
    
    # ä¿å­˜å•ä¸ªæ™ºèƒ½ä½“çš„æ ¸å¿ƒæ•°æ®JSON
    shap_json_path = os.path.join(save_dir, f"{agent_name}_SHAP_Core_Data.json")
    with open(shap_json_path, 'w', encoding='utf-8') as f:
        json.dump(shap_core_data, f, cls=NumpyEncoder, indent=4, ensure_ascii=False)
    print(f"âœ… {agent_name} SHAPæ ¸å¿ƒæ•°æ®å·²ä¿å­˜ä¸ºJSONï¼š{shap_json_path}")
    
    # 3. ç»˜åˆ¶SHAP Summary Plotï¼ˆæ ¸å¿ƒï¼šæ‰€æœ‰ç‰¹å¾çš„å½±å“æ±‡æ€»ï¼‰
    plt.figure(figsize=(12, 8))
    shap.summary_plot(
        shap_values, 
        features=state_samples,
        feature_names=feature_names,
        plot_type="dot",
        show=False,
        cmap=plt.get_cmap("RdYlBu_r"),
        plot_size=(12, 8)
    )
    plt.title(f"SHAP Summary Plot - {agent_name}", fontsize=16, pad=20)
    # ä¿å­˜Summary Plot - åªä¿ç•™SVGæ ¼å¼ï¼Œæ³¨é‡Šæ‰PNG
    summary_svg = os.path.join(save_dir, f"{agent_name}_SHAP_Summary.svg")
    # summary_png = os.path.join(save_dir, f"{agent_name}_SHAP_Summary.png")
    plt.savefig(summary_svg, bbox_inches='tight', dpi=1200)
    # plt.savefig(summary_png, bbox_inches='tight', dpi=1200)
    print(f"âœ… {agent_name} SHAP Summary Plotå·²ä¿å­˜ï¼š{summary_svg}")
    if show_plot:
        plt.show()
    plt.close()
    
    # 4. ç»˜åˆ¶SHAP Force Plotï¼ˆå•ä¸ªæ ·æœ¬çš„è¯¦ç»†å½±å“ï¼Œæ•°å­—ä¿ç•™æ•´æ•°ï¼‰
    sample_idx = 0
    # ç”ŸæˆForce Plotï¼ˆå…ˆä¸æ˜¾ç¤ºï¼‰
    force_plot = shap.force_plot(
        explainer.expected_value,
        shap_values[sample_idx],
        features=state_samples[sample_idx],
        feature_names=feature_names,
        matplotlib=True,
        figsize=(15, 4),
        show=False  # å…³é”®ï¼šå…ˆä¸æ˜¾ç¤ºï¼Œä¿®æ”¹æ–‡æœ¬åå†ä¿å­˜
    )
    
    # æ ¸å¿ƒä¿®æ”¹ï¼šéå†æ‰€æœ‰æ–‡æœ¬å…ƒç´ ï¼Œå°†å°æ•°è½¬ä¸ºæ•´æ•°
    for text in plt.gca().texts:
        text_str = text.get_text()
        # æ­£åˆ™åŒ¹é…æ‰€æœ‰å¸¦å°æ•°ç‚¹çš„æ•°å­—ï¼ˆåŒ…æ‹¬æ­£è´Ÿï¼‰
        nums = re.findall(r'-?\d+\.\d+', text_str)
        for num in nums:
            # å››èˆäº”å…¥è½¬ä¸ºæ•´æ•°
            int_num = str(round(float(num)))
            # æ›¿æ¢åŸæ–‡æœ¬ä¸­çš„å°æ•°ä¸ºæ•´æ•°
            text_str = text_str.replace(num, int_num)
        # æ›´æ–°æ–‡æœ¬å†…å®¹
        text.set_text(text_str)
    
    plt.title(f"SHAP Force Plot - {agent_name} (Sample {sample_idx})", fontsize=14, pad=10)
    # ä¿å­˜Force Plot - åªä¿ç•™SVGæ ¼å¼ï¼Œæ³¨é‡Šæ‰PNG
    force_svg = os.path.join(save_dir, f"{agent_name}_SHAP_Force.svg")
    # force_png = os.path.join(save_dir, f"{agent_name}_SHAP_Force.png")
    plt.savefig(force_svg, bbox_inches='tight', dpi=1200)
    # plt.savefig(force_png, bbox_inches='tight', dpi=1200)
    print(f"âœ… {agent_name} SHAP Force Plotå·²ä¿å­˜ï¼š{force_svg}")
    if show_plot:
        plt.show()
    plt.close()
    
    # 5. ç»˜åˆ¶å½’ä¸€åŒ–æ¨ªåæ ‡çš„æ•´åˆDependence Plotï¼ˆTOP3ç‰¹å¾ï¼‰
    plot_combined_dependence_normalized(
        shap_values=shap_values,
        state_samples=state_samples,
        feature_names=feature_names,
        top_k=3,
        agent_name=agent_name,
        save_dir=save_dir,
        show_plot=show_plot
    )
    
    # 6. è®¡ç®—ç‰¹å¾é‡è¦æ€§å¹¶ä¿å­˜
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'SHAP_Importance': feature_importance,
        'Importance_Rank': np.argsort(np.argsort(-feature_importance)) + 1  # æ’å
    }).sort_values('SHAP_Importance', ascending=False)
    
    # ä¿å­˜ç‰¹å¾é‡è¦æ€§CSV
    importance_csv = os.path.join(save_dir, f"{agent_name}_SHAP_Importance.csv")
    importance_df.to_csv(importance_csv, index=False, encoding='utf-8')
    print(f"âœ… {agent_name} ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜ï¼š{importance_csv}")
    
    # è¿”å›æ ¸å¿ƒæ•°æ®ï¼Œç”¨äºæ±‡æ€»ä¸‰ä¸ªæ™ºèƒ½ä½“
    return importance_df, shap_core_data

# ====================== 6. ä¸»ç¨‹åº ======================
if __name__ == '__main__':
    args = parse_args()
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("=" * 80)
    print("                    æ™ºèƒ½ä½“SHAPåˆ†æé…ç½®ï¼ˆå½’ä¸€åŒ–æ¨ªåæ ‡çš„æ•´åˆDependence Plotï¼‰                  ")
    print("=" * 80)
    print(f"æ¨¡å‹è·¯å¾„é…ç½®:")
    print(f"  - æ—¥æœŸæ–‡ä»¶å¤¹: {args.net_date}")
    print(f"  - è®­ç»ƒID: {args.train_id}")
    print(f"  - æ¨¡å‹å‰ç¼€: {args.model_prefix}")
    print(f"  - RNNæ¨¡å‹è·¯å¾„: {args.rnn_path}")
    print(f"SHAPé…ç½®:")
    print(f"  - é‡‡æ ·æ•°é‡: {args.n_samples}")
    print(f"  - æ˜¾ç¤ºå›¾è¡¨: {'æ˜¯' if args.show_plot else 'å¦'}")
    print(f"  - Forceå›¾æ•°å­—æ ¼å¼: ä»…ä¿ç•™æ•´æ•°ï¼ˆå››èˆäº”å…¥ï¼‰")
    print(f"  - Dependence Plot: å½’ä¸€åŒ–æ¨ªåæ ‡(0~1)çš„æ•´åˆTOP3ç‰¹å¾å›¾")
    print(f"  - å›¾ç‰‡ä¿å­˜æ ¼å¼: ä»…SVGï¼ˆPNGå·²æ³¨é‡Šï¼‰")
    print(f"  - æ•°æ®ä¿å­˜ä¼˜åŒ–: ä»…ä¿å­˜å…¨å±€ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‡å€¼/æœ€å€¼/æ ‡å‡†å·®ï¼‰ï¼Œä¸ä¿å­˜åŸå§‹å¤§æ•°ç»„")
    print(f"  - æ•°æ®è¾“å‡º: å•ä¸ªæ™ºèƒ½ä½“JSON + ä¸‰ä¸ªæ™ºèƒ½ä½“æ±‡æ€»JSON")
    print("çŠ¶æ€ç»´åº¦å®šä¹‰ï¼ˆåŒ¹é…Envsç¯å¢ƒï¼‰:")
    feature_names = [
        'Load_Power (W)', 
        'Temperature (Â°C)', 
        'FC_Power (W)', 
        'Battery_Power (W)', 
        'SC_Power (W)', 
        'Battery_SOC', 
        'SC_SOC'
    ]
    for i, name in enumerate(feature_names):
        print(f"  - ç»´åº¦{i}: {name}")
    print("=" * 80 + "\n")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    # åˆå§‹åŒ–çœŸå®ç¯å¢ƒï¼ˆä½¿ç”¨ä½ æä¾›çš„Envsç±»ï¼‰
    env = Envs()
    print(f"âœ… ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼Œè§‚æµ‹ç©ºé—´èŒƒå›´:")
    print(f"  - ä¸‹é™: {env.observation_space.low}")
    print(f"  - ä¸Šé™: {env.observation_space.high}")
    
    # åŠ è½½RNNæ¨¡å‹
    try:
        rnn_model = MultiTaskRNN().to(device)
        rnn_model.load_state_dict(torch.load(args.rnn_path, map_location=device))
        rnn_model.eval()
        print(f"\nâœ… æˆåŠŸåŠ è½½RNNæ¨¡å‹: {args.rnn_path}")
    except FileNotFoundError as e:
        print(f"âŒ RNNæ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        raise
    except Exception as e:
        print(f"âŒ RNNæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

    # åˆå§‹åŒ–ä¸‰ä¸ªæ™ºèƒ½ä½“ï¼ˆåŒ¹é…ç¯å¢ƒçš„åŠ¨ä½œç©ºé—´ï¼‰
    N_FC_ACTIONS = env.N_FC_ACTIONS  # 32
    N_BAT_ACTIONS = env.N_BAT_ACTIONS  # 40
    N_SC_ACTIONS = env.N_SC_ACTIONS  # 2
    
    FC_Agent = JointDQN("FC_Agent", rnn_model, N_FC_ACTIONS)
    Bat_Agent = JointDQN("Bat_Agent", rnn_model, N_BAT_ACTIONS)
    SC_Agent = JointDQN("SC_Agent", rnn_model, N_SC_ACTIONS)

    # è·¯å¾„è®¾ç½®
    MODEL_BASE_DIR = os.path.join(project_root, "nets", "Chap4", "Joint_Net", args.net_date, args.train_id)
    SHAP_DIR = os.path.join(MODEL_BASE_DIR, "SHAP_Analysis")
    MODEL_FILE_PREFIX = os.path.join(MODEL_BASE_DIR, args.model_prefix)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(SHAP_DIR, exist_ok=True)

    # åŠ è½½æ™ºèƒ½ä½“æƒé‡
    try:
        FC_Agent.load_net(f"{MODEL_FILE_PREFIX}_FC.pth")
        Bat_Agent.load_net(f"{MODEL_FILE_PREFIX}_BAT.pth")
        SC_Agent.load_net(f"{MODEL_FILE_PREFIX}_SC.pth")
        print(f"\nâœ… æˆåŠŸåŠ è½½æ‰€æœ‰æ™ºèƒ½ä½“æ¨¡å‹:")
        print(f"   æ¨¡å‹è·¯å¾„: {MODEL_FILE_PREFIX}_*.pth")
    except FileNotFoundError as e:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        print(f"   æœŸæœ›è·¯å¾„: {MODEL_FILE_PREFIX}_*.pth")
        raise
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

    # è®¾ç½®æ™ºèƒ½ä½“ä¸ºè¯„ä¼°æ¨¡å¼
    FC_Agent.eval_net.eval()
    Bat_Agent.eval_net.eval()
    SC_Agent.eval_net.eval()

    # ç”ŸæˆçŠ¶æ€é‡‡æ ·æ•°æ®é›†ï¼ˆä¸¥æ ¼åŒ¹é…ç¯å¢ƒå‚æ•°ï¼‰
    print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆ{args.n_samples}ä¸ªçŠ¶æ€é‡‡æ ·æ ·æœ¬ï¼ˆåŒ¹é…çœŸå®ç¯å¢ƒï¼‰...")
    state_samples = generate_state_samples(env, n_samples=args.n_samples)
    print(f"âœ… çŠ¶æ€é‡‡æ ·å®Œæˆï¼Œæ ·æœ¬å½¢çŠ¶: {state_samples.shape}")

    # ====================== é€ä¸ªæ™ºèƒ½ä½“è¿›è¡ŒSHAPåˆ†æ ======================
    agents = [FC_Agent, Bat_Agent, SC_Agent]
    all_importance = []
    all_shap_core_data = {}  # å­˜å‚¨ä¸‰ä¸ªæ™ºèƒ½ä½“çš„æ ¸å¿ƒæ•°æ®ï¼Œç”¨äºæ±‡æ€»
    
    for agent in agents:
        print(f"\n{'='*60}")
        print(f"å¼€å§‹åˆ†æ {agent.agent_name}")
        print(f"{'='*60}")
        
        # å•ä¸ªæ™ºèƒ½ä½“SHAPåˆ†æï¼ˆè¿”å›æ ¸å¿ƒæ•°æ®ï¼‰
        importance_df, shap_core_data = shap_analysis_agent(
            agent=agent,
            state_samples=state_samples,
            feature_names=feature_names,
            save_dir=SHAP_DIR,
            agent_name=agent.agent_name,
            show_plot=args.show_plot
        )
        
        # æ”¶é›†æ•°æ®
        importance_df['Agent'] = agent.agent_name
        all_importance.append(importance_df)
        all_shap_core_data[agent.agent_name] = shap_core_data
    
    # ========== æ–°å¢ï¼šä¿å­˜ä¸‰ä¸ªæ™ºèƒ½ä½“çš„æ±‡æ€»JSONæ–‡ä»¶ ==========
    combined_shap_data = {
        "analysis_config": {
            "net_date": args.net_date,
            "train_id": args.train_id,
            "n_samples": args.n_samples,
            "seed": args.seed,
            "feature_names": feature_names,
            "analysis_time": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
        },
        "agents": all_shap_core_data,
        "cross_agent_summary": {
            # è·¨æ™ºèƒ½ä½“çš„ç‰¹å¾é‡è¦æ€§å¯¹æ¯”ï¼ˆæ¯ä¸ªç‰¹å¾åœ¨ä¸åŒæ™ºèƒ½ä½“ä¸­çš„å¹³å‡é‡è¦æ€§ï¼‰
            "feature_importance_cross_agent": [
                {
                    "feature_name": fname,
                    "FC_Agent_importance": all_shap_core_data["FC_Agent"]["shap_statistics"][fname]["abs_mean_shap_value"],
                    "Bat_Agent_importance": all_shap_core_data["Bat_Agent"]["shap_statistics"][fname]["abs_mean_shap_value"],
                    "SC_Agent_importance": all_shap_core_data["SC_Agent"]["shap_statistics"][fname]["abs_mean_shap_value"],
                    "average_importance": float(np.mean([
                        all_shap_core_data["FC_Agent"]["shap_statistics"][fname]["abs_mean_shap_value"],
                        all_shap_core_data["Bat_Agent"]["shap_statistics"][fname]["abs_mean_shap_value"],
                        all_shap_core_data["SC_Agent"]["shap_statistics"][fname]["abs_mean_shap_value"]
                    ]))
                } for fname in feature_names
            ],
            # å„æ™ºèƒ½ä½“TOP1ç‰¹å¾æ±‡æ€»
            "top1_features_summary": [
                {
                    "agent_name": agent_name,
                    "top1_feature": all_shap_core_data[agent_name]["top3_features"][0]["feature_name"],
                    "top1_importance": all_shap_core_data[agent_name]["top3_features"][0]["importance"],
                    "top1_r_squared": all_shap_core_data[agent_name]["top3_features"][0]["r_squared"]
                } for agent_name in ["FC_Agent", "Bat_Agent", "SC_Agent"]
            ]
        }
    }
    
    # ä¿å­˜æ±‡æ€»JSONæ–‡ä»¶
    combined_json_path = os.path.join(SHAP_DIR, "All_Agents_SHAP_Core_Data.json")
    with open(combined_json_path, 'w', encoding='utf-8') as f:
        json.dump(combined_shap_data, f, cls=NumpyEncoder, indent=4, ensure_ascii=False)
    print(f"\nâœ… ä¸‰ä¸ªæ™ºèƒ½ä½“SHAPæ±‡æ€»æ•°æ®å·²ä¿å­˜ï¼š{combined_json_path}")
    
    # åˆå¹¶æ‰€æœ‰æ™ºèƒ½ä½“çš„ç‰¹å¾é‡è¦æ€§å¹¶ä¿å­˜
    combined_importance = pd.concat(all_importance, ignore_index=True)
    combined_csv = os.path.join(SHAP_DIR, "All_Agents_SHAP_Importance.csv")
    combined_importance.to_csv(combined_csv, index=False, encoding='utf-8')
    
    # ====================== å®Œæˆæç¤º ======================
    print("\n" + "="*80)
    print("ğŸ‰ æ‰€æœ‰æ™ºèƒ½ä½“SHAPåˆ†æå®Œæˆï¼")
    print(f"ğŸ“ åˆ†æç»“æœä¿å­˜ç›®å½•: {SHAP_DIR}")
    print(f"ğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶ç±»å‹:")
    print(f"   1. SHAP_Summary.svg (ç‰¹å¾å½±å“æ±‡æ€»å›¾ï¼Œä»…SVGæ ¼å¼)")
    print(f"   2. SHAP_Force.svg (å•ä¸ªæ ·æœ¬è¯¦ç»†å½±å“å›¾ï¼Œæ•°å­—ä»…ä¿ç•™æ•´æ•°ï¼Œä»…SVGæ ¼å¼)")
    print(f"   3. SHAP_Combined_Dependence_Normalized.svg (å½’ä¸€åŒ–æ¨ªåæ ‡çš„æ•´åˆä¾èµ–å›¾ï¼Œä»…SVGæ ¼å¼)")
    print(f"   4. *_SHAP_Importance.csv (ç‰¹å¾é‡è¦æ€§é‡åŒ–è¡¨)")
    print(f"   5. All_Agents_SHAP_Importance.csv (æ‰€æœ‰æ™ºèƒ½ä½“ç‰¹å¾é‡è¦æ€§æ±‡æ€»)")
    print(f"   6. *_SHAP_Core_Data.json (å•ä¸ªæ™ºèƒ½ä½“æ ¸å¿ƒæ•°æ®ï¼Œä»…å«å…¨å±€ç»Ÿè®¡ä¿¡æ¯)")
    print(f"   7. All_Agents_SHAP_Core_Data.json (ä¸‰ä¸ªæ™ºèƒ½ä½“æ±‡æ€»æ•°æ®ï¼Œå«è·¨æ™ºèƒ½ä½“å¯¹æ¯”)")
    print("="*80)
    
    # æ‰“å°ç‰¹å¾é‡è¦æ€§æ±‡æ€»
    print("\nğŸ“Š å„æ™ºèƒ½ä½“TOP3é‡è¦ç‰¹å¾:")
    for agent_name in ["FC_Agent", "Bat_Agent", "SC_Agent"]:
        agent_importance = combined_importance[combined_importance['Agent'] == agent_name].head(3)
        print(f"\n{agent_name}:")
        for _, row in agent_importance.iterrows():
            print(f"  - {row['Feature']} (é‡è¦æ€§: {row['SHAP_Importance']:.4f}, æ’å: {row['Importance_Rank']})")